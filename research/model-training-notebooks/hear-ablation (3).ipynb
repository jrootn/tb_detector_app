{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":7869025,"datasetId":4617096,"databundleVersionId":7974418}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (LEAK-FREE & PRODUCTION-READY)\n# Audio (HeAR + LogReg) + Metadata (LightGBM)\n# ============================================================================\n\n# ── CELL 1: Imports & Seeds ───────────────────────────────────────────────────\nimport os, sys, json, warnings, random, hashlib, zipfile, shutil\nimport numpy as np\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nprint(f\"Python    : {sys.version}\")\nprint(f\"sklearn   : {sklearn.__version__}\")\nprint(f\"librosa   : {librosa.__version__}\")\nprint(f\"numpy     : {np.__version__}\")\nprint(f\"pandas    : {pd.__version__}\")\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\n    print(f\"lightgbm  : {lgb.__version__}\")\nexcept ImportError:\n    HAS_LGB = False\n    print(\"lightgbm  : NOT FOUND — using GradientBoostingClassifier\")\n\ntry:\n    import tensorflow as tf\n    tf.random.set_seed(SEED)\n    print(f\"tensorflow: {tf.__version__}\")\nexcept ImportError:\n    print(\"tensorflow: NOT FOUND\")\n\nfrom sklearn.model_selection import StratifiedGroupKFold\n\n# ── CELL 2: Configuration ─────────────────────────────────────────────────────\nBASE   = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA   = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"   \n\n# Metadata files\nCLINICAL_CSV   = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV  = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"   \n\n# ---- Audio ----\nSR          = 16_000\nWIN_SECS    = 2.0\nWIN_SAMPLES = int(SR * WIN_SECS)\nENERGY_THRESH_S = 2.2       \n\n# ---- Training ----\nN_SPLITS     = 5             # Replacing predefined folds with 5 dynamic folds\nTARGET_SENS  = [0.85, 0.90, 0.95]\nMISS_AUG_P   = 0.20          \nCALIBRATE    = True\nLGB_N_ITER   = 500\nLGB_LR       = 0.05\nINNER_VAL_FRAC = 0.20        \n\n# ---- Output ----\nOUT_ROOT  = \"/kaggle/working/outputs\"\nAUDIO_OUT = os.path.join(OUT_ROOT, \"audio_model\")\nMETA_OUT  = os.path.join(OUT_ROOT, \"metadata_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [AUDIO_OUT, META_OUT, CACHE_DIR,\n          f\"{AUDIO_OUT}/plots\", f\"{META_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_embeddings.parquet\")\nprint(f\"Output root: {OUT_ROOT}\")\n\n# ── CELL 3: Build the master cough-level manifest ─────────────────────────────\ndef harmonise_fold_df(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\",\"gold_standard\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try:    return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndef resolve_audio_paths(filenames, audio_dir=AUDIO_BASE):\n    lookup = {}\n    for dirpath, _, fns in os.walk(audio_dir):\n        for fn in fns:\n            if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n                full = os.path.join(dirpath, fn)\n                lookup[fn] = full\n                lookup[os.path.splitext(fn)[0]] = full\n    def _resolve(fn):\n        if pd.isna(fn): return np.nan\n        fn = str(fn)\n        if fn in lookup: return lookup[fn]\n        stem = os.path.splitext(fn)[0]\n        if stem in lookup: return lookup[stem]\n        if os.path.isfile(fn): return fn\n        return np.nan\n    return filenames.apply(_resolve), lookup\n\nprint(\"Loading raw solicited data manifest …\")\nraw_audio_df = pd.read_csv(SOLICITED_CSV)\nraw_audio_df = harmonise_fold_df(raw_audio_df)\n\n# If label isn't in audio manifest, we will merge it from clinical\nif \"label_raw\" not in raw_audio_df.columns:\n    print(\"Label not in audio manifest, will extract from clinical data.\")\n\n# ── CELL 4: Join clinical metadata ────────────────────────────────────────────\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"drug_\",\n                 \"microscopy\",\"molecular\",\"confirmatory\",\"reference_test\",\n                 \"gold_standard\",\"diagnosis\",\"tb_status\",\"tb_result\",\n                 \"label\",\"label_raw\",\"_fold\",\"_split\",\"filename\",\"audio_path\"]\n\nprint(\"\\nLoading clinical metadata …\")\nclinical_df = pd.read_csv(CLINICAL_CSV)\nclinical_df = harmonise_fold_df(clinical_df)\n\n# Ensure raw_audio_df gets labels if missing\nif \"label_raw\" not in raw_audio_df.columns and \"label_raw\" in clinical_df.columns:\n    raw_audio_df = raw_audio_df.merge(clinical_df[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\nraw_audio_df[\"label\"] = binarise_label(raw_audio_df[\"label_raw\"])\nraw_audio_df = raw_audio_df.dropna(subset=[\"label\"]).reset_index(drop=True)\nraw_audio_df[\"label\"] = raw_audio_df[\"label\"].astype(int)\n\ndef get_meta_cols(df):\n    skip = set(POST_DIAG_KW) | {\"participant_id\"}\n    num_cols, cat_cols = [], []\n    for c in df.columns:\n        if any(kw in c.lower() for kw in POST_DIAG_KW): continue\n        if c in skip: continue\n        if df[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n        else: cat_cols.append(c)\n    return num_cols, cat_cols\n\nclinical_num, clinical_cat = get_meta_cols(clinical_df)\n\ncough_df = raw_audio_df.merge(\n    clinical_df[[\"participant_id\"] + clinical_num + clinical_cat],\n    on=\"participant_id\", how=\"left\"\n)\n\nprint(\"\\nResolving audio file paths …\")\ncough_df[\"audio_path\"], audio_lookup = resolve_audio_paths(cough_df[\"filename\"])\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── CELL 5: Sanity assertions ─────────────────────────────────────────────────\nprint(\"\\n── Sanity checks ──\")\nn_pos = cough_df[\"label\"].sum()\nn_neg = (cough_df[\"label\"] == 0).sum()\nprev = n_pos / len(cough_df)\n\nprint(f\"  ✓ Total valid cough rows : {len(cough_df)}\")\nprint(f\"  ✓ Participants         : {cough_df['participant_id'].nunique()}\")\nprint(f\"  ✓ TB+ coughs           : {n_pos} ({100*prev:.1f}%)\")\nprint(f\"  ✓ TB- coughs           : {n_neg}\")\n\n# ── CELL 6: Fold assignment (LEAK-FREE DYNAMIC SPLIT) ─────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds (Leak-Free) ...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    y_te = cough_df.loc[te_idx, \"label\"].values\n    n_tr_parts = cough_df.loc[tr_idx, \"participant_id\"].nunique()\n    n_te_parts = cough_df.loc[te_idx, \"participant_id\"].nunique()\n    print(f\"  Fold {fold_i}: train={len(tr_idx)} rows/{n_tr_parts} subjects  \"\n          f\"test={len(te_idx)} rows/{n_te_parts} subjects  \"\n          f\"TB+_test={int(y_te.sum())}/{len(y_te)}\")\n\n# ── CELL 7: Audio loading & window selection ──────────────────────────────────\ndef compute_audio_quality(audio, sr=SR):\n    duration    = len(audio) / sr\n    clip_ratio  = float(np.mean(np.abs(audio) > 0.99))\n    frame_len   = 400; hop = 160\n    frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n    rms    = np.sqrt(np.mean(frames**2, axis=0)) + 1e-9\n    snr_db = float(20 * np.log10(rms.max() / rms.min()))\n    return {\"duration_s\": round(duration, 3), \"clip_ratio\": round(clip_ratio, 4), \"snr_proxy_db\": round(snr_db, 2)}\n\ndef select_best_window(audio, sr=SR):\n    \"\"\"Deterministically select the highest-energy 2-second segment.\"\"\"\n    n = len(audio)\n    if n == 0:\n        return np.zeros(WIN_SAMPLES, np.float32)\n    peak = np.max(np.abs(audio))\n    if peak > 0: audio = audio / peak\n\n    if n / sr <= ENERGY_THRESH_S:\n        if n < WIN_SAMPLES: audio = np.pad(audio, (0, WIN_SAMPLES - n))\n        return audio[:WIN_SAMPLES].astype(np.float32)\n\n    frame_len = 400; hop = 160\n    frames     = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n    rms        = np.sqrt(np.mean(frames**2, axis=0))\n    smooth_n   = max(1, int(0.2 * sr / hop))\n    rms_smooth = np.convolve(rms, np.ones(smooth_n)/smooth_n, mode=\"same\")\n    peak_fr    = int(np.argmax(rms_smooth))\n    center     = peak_fr * hop + frame_len // 2\n    start      = max(0, center - WIN_SAMPLES // 2)\n    end        = start + WIN_SAMPLES\n    if end > n:\n        end = n; start = max(0, n - WIN_SAMPLES)\n    seg = audio[start:end]\n    if len(seg) < WIN_SAMPLES: seg = np.pad(seg, (0, WIN_SAMPLES - len(seg)))\n    return seg[:WIN_SAMPLES].astype(np.float32)\n\ndef load_and_select(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        qual = compute_audio_quality(audio)\n        seg  = select_best_window(audio)\n        return seg, qual\n    except Exception as e:\n        return None, None\n\n# ── CELL 8: HeAR model + disk cache ──────────────────────────────────────────\nprint(\"\\nLoading HeAR model …\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL   = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    EMBED_DIM    = 512\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"  ⚠ HeAR load failed: {e}\")\n    HEAR_MODEL = HEAR_SERVING = None\n    EMBED_DIM  = 512\n\ndef _path_key(path):\n    return hashlib.md5(f\"{HEAR_VERSION}::{path}\".encode()).hexdigest()\n\ndef _load_cache():\n    if os.path.isfile(EMBED_CACHE):\n        try:\n            df = pd.read_parquet(EMBED_CACHE)\n            return df.set_index(\"key\") if \"key\" in df.columns else df\n        except: pass\n    return pd.DataFrame(columns=[\"key\",\"embedding\"]).set_index(\"key\")\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef get_embeddings(df_rows, batch_size=64, desc=\"\"):\n    from tqdm.auto import tqdm\n    cache = _load_cache()\n    N = len(df_rows)\n    embeddings  = np.zeros((N, EMBED_DIM), np.float32)\n    \n    keys = [_path_key(str(r.audio_path)) if pd.notna(r.audio_path) else None for _, r in df_rows.iterrows()]\n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] is not None and keys[i] not in cache.index]\n\n    buf_segs, buf_keys = [], []\n    def flush():\n        if not buf_segs: return\n        embs = _infer_batch(buf_segs)\n        new_rows = [{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)]\n        nonlocal cache\n        cache = pd.concat([cache, pd.DataFrame(new_rows).set_index(\"key\")])\n        buf_segs.clear(); buf_keys.clear()\n\n    for i, row in tqdm(need, desc=f\"HeAR [{desc}]\", leave=False):\n        seg, _ = load_and_select(row.audio_path)\n        if seg is not None:\n            buf_segs.append(seg); buf_keys.append(keys[i])\n        if len(buf_segs) >= batch_size: flush()\n    flush()\n    \n    cache.reset_index().to_parquet(EMBED_CACHE, index=False)\n    for i, (_, row) in enumerate(df_rows.iterrows()):\n        k = keys[i]\n        if k in cache.index:\n            val = cache.loc[k, \"embedding\"]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── CELL 9: Metadata preprocessing ───────────────────────────────────────────\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nMETA_COLS_NUM = clinical_num\nMETA_COLS_CAT = clinical_cat\nALL_META_COLS = META_COLS_NUM + META_COLS_CAT\n\nclass MissingnessAugmenter(BaseEstimator, TransformerMixin):\n    def __init__(self, p=MISS_AUG_P, seed=SEED):\n        self.p = p; self.seed = seed\n    def fit(self, X, y=None): return self\n    def fit_transform(self, X, y=None, **kw):\n        rng   = np.random.RandomState(self.seed)\n        X_out = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n        mask  = rng.random(X_out.shape) < self.p\n        X_out[mask] = np.nan\n        return X_out\n    def transform(self, X): return X\n\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")), \n                                              (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef preprocess_meta_fold(df_train, df_val, df_test, num_cols, cat_cols):\n    all_cols = num_cols + cat_cols\n    def add_miss_indicators(df):\n        d = df[all_cols].copy()\n        for c in all_cols: d[f\"__miss_{c}\"] = d[c].isna().astype(np.float32)\n        return d\n        \n    X_tr_raw  = add_miss_indicators(df_train)\n    X_val_raw = add_miss_indicators(df_val)\n    X_te_raw  = add_miss_indicators(df_test)\n    ind_cols = [f\"__miss_{c}\" for c in all_cols]\n\n    aug = MissingnessAugmenter(p=MISS_AUG_P, seed=SEED)\n    X_tr_feat = aug.fit_transform(df_train[all_cols])\n    \n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    prep.fit(X_tr_feat)\n\n    def transform_and_stack(feat_df, ind_df):\n        transformed = prep.transform(feat_df)\n        indicators  = ind_df[ind_cols].values.astype(np.float32)\n        return np.hstack([transformed, indicators])\n\n    return transform_and_stack(X_tr_feat, X_tr_raw), transform_and_stack(df_val[all_cols], X_val_raw), transform_and_stack(df_test[all_cols], X_te_raw), prep\n\n# ── CELL 10: Model builders ───────────────────────────────────────────────────\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\n\ndef build_audio_clf():\n    return Pipeline([\n        (\"sc\",  StandardScaler()),\n        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=5000, C=1.0, solver=\"lbfgs\", random_state=SEED)),\n    ])\n\ndef build_meta_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=LGB_N_ITER, learning_rate=LGB_LR,\n            num_leaves=15, max_depth=4,         # Prevent Overfitting\n            subsample=0.8, colsample_bytree=0.8,\n            min_child_samples=10,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1,\n        )\n    from sklearn.ensemble import GradientBoostingClassifier\n    return GradientBoostingClassifier(n_estimators=200, learning_rate=LGB_LR, max_depth=4, subsample=0.8, random_state=SEED)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\") # Changed to sigmoid for stability on small sets\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── CELL 11 & 12: Evaluation & Plotting ───────────────────────────────────────\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score, f1_score, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve)\nfrom scipy.stats import spearmanr\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n\nC_POS, C_NEG = \"#e63946\", \"#457b9d\"\n\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)),\n            \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9),\n            \"precision\": tp/(tp+fp+1e-9), \"npv\": tn/(tn+fn+1e-9), \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob, val_true=None, val_prob=None):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {}\n    m[\"roc_auc\"] = float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"pr_auc\"]  = float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"brier\"]   = float(brier_score_loss(y_true, y_prob))\n    m[\"spearman_rho\"]= float(spearmanr(y_prob, y_true).statistic)\n    m.update(metrics_at_thresh(y_true, y_prob, 0.5))\n    \n    tune_t = val_true if val_true is not None else y_true\n    tune_p = val_prob if val_prob is not None else y_prob\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(tune_t, tune_p, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef _save(fig, path):\n    fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n\ndef save_all_plots(y_true, y_prob, plot_dir, prefix, best_t=0.5):\n    if len(np.unique(y_true)) < 2: return\n    y_true=np.array(y_true); y_prob=np.array(y_prob)\n    \n    # ROC\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=C_POS, lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{prefix} ROC\"); ax.legend(); _save(fig, f\"{plot_dir}/{prefix}_roc.png\")\n    \n    # PR\n    p, r, _ = precision_recall_curve(y_true, y_prob); ap = average_precision_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(r, p, color=C_POS, lw=2, label=f\"AP={ap:.3f}\")\n    ax.set(title=f\"{prefix} PR\"); ax.legend(); _save(fig, f\"{plot_dir}/{prefix}_pr.png\")\n\n# ── CELL 13: Inner-fold val split helper ─────────────────────────────────────\ndef inner_val_split(df_sub, val_frac=INNER_VAL_FRAC):\n    n_splits = max(2, int(round(1/val_frac)))\n    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    splits = list(sgkf.split(df_sub, df_sub[\"label\"], df_sub[\"participant_id\"]))\n    tr_idx, val_idx = splits[0]\n    return df_sub.iloc[tr_idx].reset_index(drop=True), df_sub.iloc[val_idx].reset_index(drop=True)\n\n# ── CELL 14 & 15: AUDIO MODEL CV & OOF ────────────────────────────────────────\nprint(\"\\n\" + \"=\"*70)\nprint(\"AUDIO MODEL  —  HeAR + LogisticRegression\")\nprint(\"=\"*70)\n\naudio_oof_rows, fold_metrics_aud = [], []\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n── Audio Fold {fold_i+1} ──\")\n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    df_tr, df_val = inner_val_split(df_tr_full)\n\n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n\n    X_tr_emb  = get_embeddings(df_tr,  desc=f\"F{fold_i}_tr\")\n    X_val_emb = get_embeddings(df_val, desc=f\"F{fold_i}_val\")\n    X_te_emb  = get_embeddings(df_te,  desc=f\"F{fold_i}_te\")\n\n    clf_a = build_audio_clf().fit(X_tr_emb, y_tr)\n\n    if CALIBRATE and len(np.unique(y_val)) >= 2:\n        inner_lr = clf_a.named_steps[\"clf\"]\n        cal_a    = calibrate(inner_lr, clf_a.named_steps[\"sc\"].transform(X_val_emb), y_val)\n        _proba_a = lambda X: cal_a.predict_proba(clf_a.named_steps[\"sc\"].transform(X))[:,1]\n    else:\n        cal_a = None\n        _proba_a = lambda X: clf_a.predict_proba(X)[:,1]\n\n    val_prob, te_prob = _proba_a(X_val_emb), _proba_a(X_te_emb)\n    \n    fm = full_eval(y_te, te_prob, val_true=y_val, val_prob=val_prob)\n    fold_metrics_aud.append(fm)\n    print(f\"  AUC={fm['roc_auc']:.3f}  PR-AUC={fm['pr_auc']:.3f}\")\n\n    for pid, p, lbl in zip(df_te[\"participant_id\"], te_prob, y_te):\n        audio_oof_rows.append({\"participant_id\":pid,\"fold\":fold_i,\"prob\":p,\"label\":lbl})\n\naud_oof = pd.DataFrame(audio_oof_rows)\nyt_a, yp_a = aud_oof[\"label\"].values, aud_oof[\"prob\"].values\nm_aud_cough = full_eval(yt_a, yp_a)\npart_a = aud_oof.groupby(\"participant_id\").agg(prob=(\"prob\",\"max\"), label=(\"label\",\"first\")).reset_index()\nm_aud_part = full_eval(part_a[\"label\"].values, part_a[\"prob\"].values)\nsave_all_plots(yt_a, yp_a, f\"{AUDIO_OUT}/plots\", \"audio\", best_t=find_thresh_for_sens(yt_a, yp_a, 0.90))\n\n# ── CELL 16 & 17: METADATA MODEL CV & OOF ──────────────────────────────────────\nprint(\"\\n\" + \"=\"*70)\nprint(\"METADATA MODEL  —  LightGBM + missingness augmentation\")\nprint(\"=\"*70)\n\nmeta_oof_rows, fold_metrics_meta = [], []\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n── Meta Fold {fold_i+1} ──\")\n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    df_tr, df_val = inner_val_split(df_tr_full)\n\n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n\n    X_tr, X_val_m, X_te_m, prep = preprocess_meta_fold(df_tr, df_val, df_te, META_COLS_NUM, META_COLS_CAT)\n\n    clf_m = build_meta_clf(int(y_tr.sum()), int((y_tr==0).sum()))\n    if HAS_LGB and len(np.unique(y_val)) >= 2:\n        clf_m.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr),(X_val_m, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n    else:\n        clf_m.fit(X_tr, y_tr)\n\n    if CALIBRATE and len(np.unique(y_val)) >= 2:\n        cal_m    = calibrate(clf_m, X_val_m, y_val)\n        val_prob = cal_m.predict_proba(X_val_m)[:,1]\n        te_prob  = cal_m.predict_proba(X_te_m)[:,1]\n    else:\n        val_prob = clf_m.predict_proba(X_val_m)[:,1]\n        te_prob  = clf_m.predict_proba(X_te_m)[:,1]\n\n    fm = full_eval(y_te, te_prob, val_true=y_val, val_prob=val_prob)\n    fold_metrics_meta.append(fm)\n    print(f\"  AUC={fm['roc_auc']:.3f}  PR-AUC={fm['pr_auc']:.3f}\")\n\n    for pid, p, lbl in zip(df_te[\"participant_id\"], te_prob, y_te):\n        meta_oof_rows.append({\"participant_id\":pid,\"fold\":fold_i,\"prob\":p,\"label\":lbl})\n\nmeta_oof = pd.DataFrame(meta_oof_rows)\nyt_m, yp_m = meta_oof[\"label\"].values, meta_oof[\"prob\"].values\nm_meta_cough = full_eval(yt_m, yp_m)\npart_m = meta_oof.groupby(\"participant_id\").agg(prob=(\"prob\",\"max\"), label=(\"label\",\"first\")).reset_index()\nm_meta_part = full_eval(part_m[\"label\"].values, part_m[\"prob\"].values)\nsave_all_plots(yt_m, yp_m, f\"{META_OUT}/plots\", \"meta\", best_t=find_thresh_for_sens(yt_m, yp_m, 0.90))\n\n# ── CELL 18-20: Reporting & Zipping ──────────────────────────────────────────\nif m_aud_cough and m_meta_cough:\n    fig, ax = plt.subplots(figsize=(6,5))\n    fpr, tpr, _ = roc_curve(yt_a, yp_a); ax.plot(fpr, tpr, lw=2, color=C_POS, label=f\"Audio AUC={roc_auc_score(yt_a,yp_a):.3f}\")\n    fpr, tpr, _ = roc_curve(yt_m, yp_m); ax.plot(fpr, tpr, lw=2, color=C_NEG, label=f\"Meta AUC={roc_auc_score(yt_m,yp_m):.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=\"ROC Comparison\"); ax.legend(); _save(fig, f\"{OUT_ROOT}/roc_comparison.png\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"Audio (HeAR+LR)\", m_aud_cough, m_aud_part), \n                           make_row(\"Metadata (LightGBM)\", m_meta_cough, m_meta_part)])\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"REPORT-READY SUMMARY (LEAK-FREE)\")\nprint(\"=\"*100)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-19T23:44:42.480965Z","iopub.execute_input":"2026-02-19T23:44:42.481222Z","iopub.status.idle":"2026-02-19T23:53:38.663300Z","shell.execute_reply.started":"2026-02-19T23:44:42.481200Z","shell.execute_reply":"2026-02-19T23:53:38.662487Z"}},"outputs":[{"name":"stdout","text":"Python    : 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\nsklearn   : 1.6.1\nlibrosa   : 0.11.0\nnumpy     : 2.0.2\npandas    : 2.2.2\nlightgbm  : 4.6.0\n","output_type":"stream"},{"name":"stderr","text":"2026-02-19 23:44:50.697731: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771544690.905738      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771544690.962956      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771544691.465121      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771544691.465167      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771544691.465170      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771544691.465173      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"tensorflow: 2.19.0\nOutput root: /kaggle/working/outputs\nLoading raw solicited data manifest …\nLabel not in audio manifest, will extract from clinical data.\n\nLoading clinical metadata …\n\nResolving audio file paths …\n\n── Sanity checks ──\n  ✓ Total valid cough rows : 9772\n  ✓ Participants         : 1082\n  ✓ TB+ coughs           : 2930 (30.0%)\n  ✓ TB- coughs           : 6842\n\nBuilding Custom Stratified Group K-Folds (Leak-Free) ...\n  Fold 0: train=7821 rows/866 subjects  test=1951 rows/216 subjects  TB+_test=599/1951\n  Fold 1: train=7812 rows/866 subjects  test=1960 rows/216 subjects  TB+_test=626/1960\n  Fold 2: train=7787 rows/865 subjects  test=1985 rows/217 subjects  TB+_test=577/1985\n  Fold 3: train=7846 rows/865 subjects  test=1926 rows/217 subjects  TB+_test=592/1926\n  Fold 4: train=7822 rows/866 subjects  test=1950 rows/216 subjects  TB+_test=536/1950\n\nLoading HeAR model …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecfa0c71b8c431990a6662873440fb4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e3ba12170e1470e9e986b2c4740e1ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9661bb549555487fad423adb686025b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b64ee56876534a448b0e0d3c8e71b6a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/fing(…):   0%|          | 0.00/79.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d58aa2f80848f7903ced01d4c99f58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/kera(…):   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf94974c18f0477e8216e7b2ef1cc3ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/save(…):   0%|          | 0.00/4.89M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1df5047650144bf09a76b7b956c35424"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40bfc19d8f7409494e809c8c42e4d48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/vari(…):   0%|          | 0.00/12.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c17a9c0f94d4da9bab5f85d6f3bcada"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/5.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a77661c48c74400c830bd1ab08aac29d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/kera(…):   0%|          | 0.00/644k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebac88b4e7f14111b12d0ec97eb528ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/fing(…):   0%|          | 0.00/76.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcc62a7954594de0b90613d26c33e587"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/save(…):   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62bafe692e5f44e4885e29104de33554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/vari(…):   0%|          | 0.00/3.95M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05a00f1056d94c77aff4b962860d2a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/4.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d5794a03754c7d9119af7bbdd808f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/spectrogram_frontend/fing(…):   0%|          | 0.00/55.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a07de2a0404be09619881c86715252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.data-00000-of-00001:   0%|          | 0.00/24.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84d8c573547641a2ac2587612b87608b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/spectrogram_frontend/kera(…):   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f39dcefaf83e445b957b0317743f3352"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/spectrogram_frontend/save(…):   0%|          | 0.00/340k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bac6b8c180eb41d9950a00b68f1cc65c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fingerprint.pb:   0%|          | 0.00/78.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"653285c29a1d45cbaede653a5548ff91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ad46285c294436eaadfbd9930370397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gitattributes:   0%|          | 0.00/1.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"611c12ebedee46e683ac3c9968b1bfa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables/variables.data-00000-of-00001:   0%|          | 0.00/1.21G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9393184b8c84d769a704169293baaf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"saved_model.pb:   0%|          | 0.00/3.98M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a87665bca945ffab0793009ff703b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/6.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6040bfc862a545199886b88a1f82bf9a"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1771544732.699097      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13757 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1771544732.705145      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13757 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"✓ HeAR loaded\n\n======================================================================\nAUDIO MODEL  —  HeAR + LogisticRegression\n======================================================================\n\n── Audio Fold 1 ──\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HeAR [F0_tr]:   0%|          | 0/6222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1771544762.170483     167 service.cc:152] XLA service 0x7c8f6ad9daf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1771544762.170524     167 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1771544762.170530     167 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1771544762.531763     167 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1771544765.931556     167 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HeAR [F0_val]:   0%|          | 0/1599 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F0_te]:   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  AUC=0.695  PR-AUC=0.469\n\n── Audio Fold 2 ──\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HeAR [F1_tr]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F1_val]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F1_te]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  AUC=0.641  PR-AUC=0.433\n\n── Audio Fold 3 ──\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HeAR [F2_tr]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F2_val]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F2_te]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  AUC=0.656  PR-AUC=0.430\n\n── Audio Fold 4 ──\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HeAR [F3_tr]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F3_val]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F3_te]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"  AUC=0.646  PR-AUC=0.414\n\n── Audio Fold 5 ──\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HeAR [F4_tr]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F4_val]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HeAR [F4_te]: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c674ecda42714564ab3032c8037c8500"}},"metadata":{}},{"name":"stdout","text":"  AUC=0.679  PR-AUC=0.449\n\n======================================================================\nMETADATA MODEL  —  LightGBM + missingness augmentation\n======================================================================\n\n── Meta Fold 1 ──\n  AUC=0.781  PR-AUC=0.609\n\n── Meta Fold 2 ──\n  AUC=0.814  PR-AUC=0.664\n\n── Meta Fold 3 ──\n  AUC=0.772  PR-AUC=0.612\n\n── Meta Fold 4 ──\n  AUC=0.851  PR-AUC=0.726\n\n── Meta Fold 5 ──\n  AUC=0.817  PR-AUC=0.693\n\n====================================================================================================\nREPORT-READY SUMMARY (LEAK-FREE)\n====================================================================================================\n              Model ROC-AUC (cough) ROC-AUC (participant) Sens@90% Spec@90%\n    Audio (HeAR+LR)           0.656                 0.668    0.900    0.228\nMetadata (LightGBM)           0.799                 0.786    0.903    0.465\n\n✅ Zipped to: /kaggle/working/outputs.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 2 - SCIENTIFIC BEST PRACTICE)\n# Leak-Free CV + Mean-Pooled HeAR + MNAR-Aware LightGBM + Ensemble\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve)\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V2)\nOUT_ROOT = \"/kaggle/working/outputs_v2\"\nAUDIO_OUT = os.path.join(OUT_ROOT, \"audio_model\")\nMETA_OUT  = os.path.join(OUT_ROOT, \"metadata_model\")\nENS_OUT   = os.path.join(OUT_ROOT, \"ensemble_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [AUDIO_OUT, META_OUT, ENS_OUT, CACHE_DIR, \n          f\"{AUDIO_OUT}/plots\", f\"{META_OUT}/plots\", f\"{ENS_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\n# Identify safe metadata columns (excluding post-diag leaks)\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\n# Map physical audio files\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"Total valid audio files: {len(cough_df)} | Participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD (LEAK-FREE SPLITS) ───────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\nfor i, (tr, te) in enumerate(folds):\n    tr_p = set(cough_df.loc[tr, \"participant_id\"])\n    te_p = set(cough_df.loc[te, \"participant_id\"])\n    assert len(tr_p & te_p) == 0, f\"LEAK DETECTED IN FOLD {i}!\"\n\n# ── 4. AUDIO FEATURE EXTRACTION (MEAN-POOLING) ──────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    \"\"\"Slices entire audio into 2-second chunks for mean pooling.\"\"\"\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        \n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        \n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n            \n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    \"\"\"Safe caching without Pandas index type confusion.\"\"\"\n    if os.path.exists(EMBED_CACHE):\n        try:\n            cache = pd.read_parquet(EMBED_CACHE)\n        except:\n            cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else:\n        cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    \n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio (Mean Pooled)\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)  # FORCE pyarrow string type\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    # Dictionary lookup for maximum speed and safety\n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n        \n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            \n    return embeddings\n\n# ── 5. MNAR-AWARE PREPROCESSING & MODEL BUILDERS ────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), \n            (\"sc\", StandardScaler())\n        ]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), \n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n        ]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_audio_clf():\n    return Pipeline([\n        (\"sc\", StandardScaler()),\n        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000, C=0.1, random_state=SEED))\n    ])\n\ndef build_meta_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=300, learning_rate=0.03,\n            num_leaves=15, max_depth=4,         \n            subsample=0.8, colsample_bytree=0.8,\n            min_child_samples=15,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression()\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)),\n            \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9),\n            \"precision\": tp/(tp+fp+1e-9), \"npv\": tn/(tn+fn+1e-9), \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {}\n    m[\"roc_auc\"] = float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"pr_auc\"]  = float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V2 CV Pipeline (Audio + Meta + Ensemble)...\")\noof_aud, oof_meta, oof_ens = np.zeros(len(cough_df)), np.zeros(len(cough_df)), np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # ── AUDIO PATH ──\n    X_tr_emb  = get_mean_embeddings(df_tr)\n    X_val_emb = get_mean_embeddings(df_val)\n    X_te_emb  = get_mean_embeddings(df_te)\n    \n    clf_a = build_audio_clf().fit(X_tr_emb, y_tr)\n    cal_a = calibrate(clf_a.named_steps[\"clf\"], clf_a.named_steps[\"sc\"].transform(X_val_emb), y_val)\n    \n    te_prob_a = cal_a.predict_proba(clf_a.named_steps[\"sc\"].transform(X_te_emb))[:,1]\n    oof_aud[te_idx] = te_prob_a\n    \n    # ── META PATH ──\n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = prep.fit_transform(df_tr)\n    X_val_m = prep.transform(df_val)\n    X_te_m  = prep.transform(df_te)\n    \n    clf_m = build_meta_clf(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    cal_m = calibrate(clf_m, X_val_m, y_val)\n    \n    te_prob_m = cal_m.predict_proba(X_te_m)[:,1]\n    oof_meta[te_idx] = te_prob_m\n\n    # ── ENSEMBLE PATH ──\n    te_prob_ens = (te_prob_a + te_prob_m) / 2.0\n    oof_ens[te_idx] = te_prob_ens\n    \n    print(f\" Fold {fold_i+1} ROC-AUC | Audio: {roc_auc_score(y_te, te_prob_a):.3f} | Meta: {roc_auc_score(y_te, te_prob_m):.3f} | Ens: {roc_auc_score(y_te, te_prob_ens):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_aud\"] = oof_aud\ncough_df[\"pred_meta\"] = oof_meta\ncough_df[\"pred_ens\"] = oof_ens\n\n# Participant-Level Aggregation\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"),\n    prob_aud=(\"pred_aud\", \"max\"),\n    prob_meta=(\"pred_meta\", \"max\"),\n    prob_ens=(\"pred_ens\", \"max\")\n).reset_index()\n\nm_aud  = full_eval(cough_df['label'], oof_aud)\nm_meta = full_eval(cough_df['label'], oof_meta)\nm_ens  = full_eval(cough_df['label'], oof_ens)\n\np_aud  = full_eval(part_df['label'], part_df['prob_aud'])\np_meta = full_eval(part_df['label'], part_df['prob_meta'])\np_ens  = full_eval(part_df['label'], part_df['prob_ens'])\n\n# Plotting\nplot_curves(cough_df['label'], oof_aud, f\"{AUDIO_OUT}/plots/audio\", \"Audio\")\nplot_curves(cough_df['label'], oof_meta, f\"{META_OUT}/plots/meta\", \"Metadata\")\nplot_curves(cough_df['label'], oof_ens, f\"{ENS_OUT}/plots/ensemble\", \"Ensemble\")\n\n# Generate Summary Table\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([\n    make_row(\"Audio (HeAR Mean-Pool)\", m_aud, p_aud), \n    make_row(\"Metadata (MNAR LightGBM)\", m_meta, p_meta),\n    make_row(\"Fusion Ensemble (V2)\", m_ens, p_ens)\n])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REPORT-READY SUMMARY (VERSION 2 - BEST PRACTICES)\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False))\n\n# Zipping Outputs\nzip_path = \"/kaggle/working/outputs_v2.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V2 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T00:43:17.842179Z","iopub.execute_input":"2026-02-20T00:43:17.843043Z","iopub.status.idle":"2026-02-20T00:48:00.237481Z","shell.execute_reply.started":"2026-02-20T00:43:17.843014Z","shell.execute_reply":"2026-02-20T00:48:00.236777Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nTotal valid audio files: 9772 | Participants: 1082\n\nBuilding Custom Stratified Group K-Folds...\n\nLoading HeAR Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70a344b8d54415cb79054103a611a21"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"✓ HeAR loaded\n\nStarting V2 CV Pipeline (Audio + Meta + Ensemble)...\n\n--- FOLD 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled):   0%|          | 0/1565 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled):   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 1 ROC-AUC | Audio: 0.681 | Meta: 0.780 | Ens: 0.771\n\n--- FOLD 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 2 ROC-AUC | Audio: 0.638 | Meta: 0.779 | Ens: 0.761\n\n--- FOLD 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 3 ROC-AUC | Audio: 0.660 | Meta: 0.725 | Ens: 0.739\n\n--- FOLD 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 4 ROC-AUC | Audio: 0.661 | Meta: 0.841 | Ens: 0.813\n\n--- FOLD 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mean Pooled): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 5 ROC-AUC | Audio: 0.678 | Meta: 0.801 | Ens: 0.801\n\n================================================================================\nREPORT-READY SUMMARY (VERSION 2 - BEST PRACTICES)\n================================================================================\n                   Model ROC-AUC (cough) ROC-AUC (participant) Sens@90% Spec@90%\n  Audio (HeAR Mean-Pool)           0.663                 0.685    0.901    0.245\nMetadata (MNAR LightGBM)           0.779                 0.776    0.901    0.383\n    Fusion Ensemble (V2)           0.775                 0.771    0.900    0.380\n\n✅ All V2 Results Zipped to: /kaggle/working/outputs_v2.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 3 - THE DeepGB-TB ADAPTATION)\n# Leak-Free CV + Early Fusion + Cross-Modal LightGBM Feature Bottlenecking\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve)\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V3)\nOUT_ROOT = \"/kaggle/working/outputs_v3\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"early_fusion_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. STRATIFIED GROUP K-FOLD (LEAK-FREE SPLITS) ───────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION ─────────────────────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── 5. MNAR PREPROCESSING & FUSION MODEL BUILDER ────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_early_fusion_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=400, learning_rate=0.02,\n            num_leaves=31, max_depth=5,         \n            subsample=0.8, \n            colsample_bytree=0.15, # <--- THE DeepGB-TB SECRET: Forces mixing of Audio & Meta\n            min_child_samples=15,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V3 CV Pipeline (Early Fusion Cross-Attention Model)...\")\noof_fusion = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # Extract Embeddings (Audio)\n    X_tr_emb  = get_mean_embeddings(df_tr)\n    X_val_emb = get_mean_embeddings(df_val)\n    X_te_emb  = get_mean_embeddings(df_te)\n    \n    # Preprocess Metadata (Tabular)\n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = prep.fit_transform(df_tr)\n    X_val_m = prep.transform(df_val)\n    X_te_m  = prep.transform(df_te)\n    \n    # --- EARLY FUSION: Combine Audio & Metadata into one massive array ---\n    X_tr_fusion  = np.hstack([X_tr_emb, X_tr_m])\n    X_val_fusion = np.hstack([X_val_emb, X_val_m])\n    X_te_fusion  = np.hstack([X_te_emb, X_te_m])\n    \n    # Train Cross-Modal LightGBM\n    clf_fusion = build_early_fusion_clf(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_fusion, y_tr)\n    cal_fusion = calibrate(clf_fusion, X_val_fusion, y_val)\n    \n    te_prob_fusion = cal_fusion.predict_proba(X_te_fusion)[:,1]\n    oof_fusion[te_idx] = te_prob_fusion\n    \n    print(f\" Fold {fold_i+1} Early Fusion ROC-AUC: {roc_auc_score(y_te, te_prob_fusion):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_fusion\"] = oof_fusion\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_fusion=(\"pred_fusion\", \"max\")\n).reset_index()\n\nm_fusion = full_eval(cough_df['label'], oof_fusion)\np_fusion = full_eval(part_df['label'], part_df['prob_fusion'])\n\nplot_curves(cough_df['label'], oof_fusion, f\"{FUSION_OUT}/plots/early_fusion\", \"Early Fusion\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"Early Fusion (V3 - DeepGB-TB Inspired)\", m_fusion, p_fusion)])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REPORT-READY SUMMARY (VERSION 3 - EARLY FUSION)\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v3.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V3 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T01:11:36.521299Z","iopub.execute_input":"2026-02-20T01:11:36.521880Z","iopub.status.idle":"2026-02-20T01:19:03.272801Z","shell.execute_reply.started":"2026-02-20T01:11:36.521849Z","shell.execute_reply":"2026-02-20T01:19:03.272191Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n\nBuilding Custom Stratified Group K-Folds...\n\nLoading HeAR Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33adaeae77644c9681bacbe5fd3c5442"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"✓ HeAR loaded\n\nStarting V3 CV Pipeline (Early Fusion Cross-Attention Model)...\n\n--- FOLD 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/6256 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/1565 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 1 Early Fusion ROC-AUC: 0.805\n\n--- FOLD 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 2 Early Fusion ROC-AUC: 0.810\n\n--- FOLD 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 3 Early Fusion ROC-AUC: 0.758\n\n--- FOLD 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 4 Early Fusion ROC-AUC: 0.844\n\n--- FOLD 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 5 Early Fusion ROC-AUC: 0.843\n\n================================================================================\nREPORT-READY SUMMARY (VERSION 3 - EARLY FUSION)\n================================================================================\n                                 Model ROC-AUC (cough) ROC-AUC (participant) Sens@90% Spec@90%\nEarly Fusion (V3 - DeepGB-TB Inspired)           0.809                 0.804    0.900    0.495\n\n✅ All V3 Results Zipped to: /kaggle/working/outputs_v3.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 4 - THE ACOUSTIC BOTTLENECK)\n# Early Fusion + PCA Dimensionality Reduction + LightGBM\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\nPCA_COMPONENTS = 32  # THE BOTTLENECK\n\n# Output Directories (V4)\nOUT_ROOT = \"/kaggle/working/outputs_v4\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"pca_fusion_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION ─────────────────────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── 5. PREPROCESSING & BOTTLENECK BUILDERS ──────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# Feature Compression Pipeline\ndef build_audio_pca_preprocessor():\n    return Pipeline([\n        (\"sc\", StandardScaler()),\n        (\"pca\", PCA(n_components=PCA_COMPONENTS, random_state=SEED))\n    ])\n\ndef build_early_fusion_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=500, learning_rate=0.015,  # Slowed down learning rate for stability\n            num_leaves=31, max_depth=5,         \n            subsample=0.8, \n            colsample_bytree=0.6,  # We can increase this now because dimensions are balanced!\n            min_child_samples=15,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V4 CV Pipeline (PCA Bottleneck + Early Fusion)...\")\noof_fusion = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # 1. Extract Raw Embeddings\n    X_tr_emb_raw  = get_mean_embeddings(df_tr)\n    X_val_emb_raw = get_mean_embeddings(df_val)\n    X_te_emb_raw  = get_mean_embeddings(df_te)\n    \n    # 2. Apply PCA Bottleneck (Reduces 512-dim to 32-dim)\n    pca_prep = build_audio_pca_preprocessor()\n    X_tr_emb  = pca_prep.fit_transform(X_tr_emb_raw)\n    X_val_emb = pca_prep.transform(X_val_emb_raw)\n    X_te_emb  = pca_prep.transform(X_te_emb_raw)\n    \n    # 3. Preprocess Metadata (Tabular)\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m  = meta_prep.transform(df_te)\n    \n    # 4. EARLY FUSION: Combine Compressed Audio (32) & Metadata (~15)\n    X_tr_fusion  = np.hstack([X_tr_emb, X_tr_m])\n    X_val_fusion = np.hstack([X_val_emb, X_val_m])\n    X_te_fusion  = np.hstack([X_te_emb, X_te_m])\n    \n    # 5. Train Cross-Modal LightGBM\n    clf_fusion = build_early_fusion_clf(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_fusion, y_tr)\n    cal_fusion = calibrate(clf_fusion, X_val_fusion, y_val)\n    \n    te_prob_fusion = cal_fusion.predict_proba(X_te_fusion)[:,1]\n    oof_fusion[te_idx] = te_prob_fusion\n    \n    print(f\" Fold {fold_i+1} PCA-Fusion ROC-AUC: {roc_auc_score(y_te, te_prob_fusion):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_fusion\"] = oof_fusion\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_fusion=(\"pred_fusion\", \"max\")\n).reset_index()\n\nm_fusion = full_eval(cough_df['label'], oof_fusion)\np_fusion = full_eval(part_df['label'], part_df['prob_fusion'])\n\nplot_curves(cough_df['label'], oof_fusion, f\"{FUSION_OUT}/plots/pca_fusion\", \"PCA Early Fusion\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"PCA-Bottlenecked Fusion (V4)\", m_fusion, p_fusion)])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REPORT-READY SUMMARY (VERSION 4 - PCA BOTTLENECK)\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v4.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V4 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T01:21:56.297260Z","iopub.execute_input":"2026-02-20T01:21:56.298001Z","iopub.status.idle":"2026-02-20T01:28:45.075144Z","shell.execute_reply.started":"2026-02-20T01:21:56.297971Z","shell.execute_reply":"2026-02-20T01:28:45.074519Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n\nBuilding Custom Stratified Group K-Folds...\n\nLoading HeAR Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f0b50a3ca35484e995d0cc2c77afdda"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"✓ HeAR loaded\n\nStarting V4 CV Pipeline (PCA Bottleneck + Early Fusion)...\n\n--- FOLD 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/6256 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/1565 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 1 PCA-Fusion ROC-AUC: 0.781\n\n--- FOLD 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 2 PCA-Fusion ROC-AUC: 0.795\n\n--- FOLD 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 3 PCA-Fusion ROC-AUC: 0.745\n\n--- FOLD 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 4 PCA-Fusion ROC-AUC: 0.841\n\n--- FOLD 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 5 PCA-Fusion ROC-AUC: 0.818\n\n================================================================================\nREPORT-READY SUMMARY (VERSION 4 - PCA BOTTLENECK)\n================================================================================\n                       Model ROC-AUC (cough) ROC-AUC (participant) Sens@90% Spec@90%\nPCA-Bottlenecked Fusion (V4)           0.791                 0.793    0.900    0.427\n\n✅ All V4 Results Zipped to: /kaggle/working/outputs_v4.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 5 - STATE OF THE ART)\n# Asymmetric Blending (Stacking) + SMOTE + Clinical Attention Mechanism\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\ntry:\n    from imblearn.over_sampling import SMOTE\n    HAS_SMOTE = True\n    print(\"✓ imbalanced-learn (SMOTE) found.\")\nexcept ImportError:\n    HAS_SMOTE = False\n    print(\"⚠ imbalanced-learn not found. SMOTE disabled.\")\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V5)\nOUT_ROOT = \"/kaggle/working/outputs_v5\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"sota_stacking_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION ─────────────────────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── 5. PREPROCESSING & STACKING BUILDERS ────────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_audio_expert():\n    return Pipeline([\n        (\"sc\", StandardScaler()),\n        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000, C=0.1, random_state=SEED))\n    ])\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=300, learning_rate=0.03, num_leaves=15, max_depth=4,         \n            subsample=0.8, colsample_bytree=0.8, min_child_samples=15,\n            scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\")\n\ndef build_supervisor_clf():\n    # The Supervisor must be shallow to prevent overfitting the Expert's predictions\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100, learning_rate=0.05,\n            num_leaves=7, max_depth=3,  \n            min_child_samples=10,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", C=0.1)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V5 CV Pipeline (Asymmetric Stacking + SMOTE)...\")\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    # SPLIT: 70% for Level-1 Experts, 30% for Level-2 Supervisor\n    l1_split = int(len(df_tr_full) * 0.7)\n    df_l1 = df_tr_full.iloc[:l1_split]\n    df_l2 = df_tr_full.iloc[l1_split:]\n    \n    y_l1, y_l2, y_te = df_l1[\"label\"].values, df_l2[\"label\"].values, df_te[\"label\"].values\n    \n    # ── LEVEL 1: AUDIO EXPERT ──\n    X_l1_emb = get_mean_embeddings(df_l1)\n    X_l2_emb = get_mean_embeddings(df_l2)\n    X_te_emb = get_mean_embeddings(df_te)\n    \n    clf_aud = build_audio_expert().fit(X_l1_emb, y_l1)\n    \n    prob_l2_aud = clf_aud.predict_proba(X_l2_emb)[:,1]\n    prob_te_aud = clf_aud.predict_proba(X_te_emb)[:,1]\n    \n    # ── LEVEL 1: CLINICAL EXPERT ──\n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_l1_m = prep.fit_transform(df_l1)\n    X_l2_m = prep.transform(df_l2)\n    X_te_m = prep.transform(df_te)\n    \n    clf_meta = build_clinical_expert(int(y_l1.sum()), int((y_l1==0).sum())).fit(X_l1_m, y_l1)\n    \n    prob_l2_meta = clf_meta.predict_proba(X_l2_m)[:,1]\n    prob_te_meta = clf_meta.predict_proba(X_te_m)[:,1]\n    \n    # ── LEVEL 2: THE SUPERVISOR ──\n    # Concatenate the Expert probabilities WITH the raw clinical features\n    X_l2_stack = np.column_stack([prob_l2_aud, prob_l2_meta, X_l2_m])\n    X_te_stack = np.column_stack([prob_te_aud, prob_te_meta, X_te_m])\n    \n    # Apply SMOTE to the Supervisor's training data to learn the TB boundary flawlessly\n    if HAS_SMOTE:\n        smote = SMOTE(random_state=SEED)\n        X_l2_resampled, y_l2_resampled = smote.fit_resample(X_l2_stack, y_l2)\n    else:\n        X_l2_resampled, y_l2_resampled = X_l2_stack, y_l2\n        \n    # Train Supervisor & Calibrate internally via CV to ensure perfect probabilities\n    base_supervisor = build_supervisor_clf()\n    cal_supervisor = CalibratedClassifierCV(base_supervisor, cv=3, method=\"sigmoid\")\n    cal_supervisor.fit(X_l2_resampled, y_l2_resampled)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    print(f\" Fold {fold_i+1} Supervisor ROC-AUC: {roc_auc_score(y_te, te_prob_stack):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_stack=(\"pred_stack\", \"max\")\n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"Supervisor Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"Asymmetric Supervisor + SMOTE (V5)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"REPORT-READY SUMMARY (VERSION 5 - SOTA STACKING)\")\nprint(\"=\"*90)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v5.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V5 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T01:31:58.007848Z","iopub.execute_input":"2026-02-20T01:31:58.008520Z","iopub.status.idle":"2026-02-20T01:38:42.748177Z","shell.execute_reply.started":"2026-02-20T01:31:58.008492Z","shell.execute_reply":"2026-02-20T01:38:42.747558Z"}},"outputs":[{"name":"stdout","text":"✓ imbalanced-learn (SMOTE) found.\nLoading data...\n\nBuilding Custom Stratified Group K-Folds...\n\nLoading HeAR Model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"545ecdede6964f698e0ed6fb0766d1f0"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"✓ HeAR loaded\n\nStarting V5 CV Pipeline (Asymmetric Stacking + SMOTE)...\n\n--- FOLD 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/5474 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/2347 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio:   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 1 Supervisor ROC-AUC: 0.766\n\n--- FOLD 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 2 Supervisor ROC-AUC: 0.784\n\n--- FOLD 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 3 Supervisor ROC-AUC: 0.767\n\n--- FOLD 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 4 Supervisor ROC-AUC: 0.840\n\n--- FOLD 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":" Fold 5 Supervisor ROC-AUC: 0.816\n\n==========================================================================================\nREPORT-READY SUMMARY (VERSION 5 - SOTA STACKING)\n==========================================================================================\n                             Model ROC-AUC (cough) ROC-AUC (participant) Sens@90% Spec@90%\nAsymmetric Supervisor + SMOTE (V5)           0.790                 0.796    0.909    0.477\n\n✅ All V5 Results Zipped to: /kaggle/working/outputs_v5.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 6 - EXACT 2-SECOND SOTA)\n# PCA Bottleneck + Early Fusion + TRBL + Heavy Debugging\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # EXACTLY 2 SECONDS\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\nPCA_COMPONENTS = 32  \n\n# Output Directories (V6)\nOUT_ROOT = \"/kaggle/working/outputs_v6\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"debug_fusion_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_exact2s_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\nprint(f\"[*] Identified Numeric Clinical Features: {num_cols}\")\nprint(f\"[*] Identified Categorical Clinical Features: {cat_cols}\")\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION (EXACTLY 2 SECONDS) ─────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_exact_audio(path):\n    \"\"\"Loads audio and forces EXACTLY 32,000 samples (2 seconds)\"\"\"\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        if len(audio) < WIN_SAMPLES:\n            audio = np.pad(audio, (0, WIN_SAMPLES - len(audio)))\n        else:\n            audio = audio[:WIN_SAMPLES]\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef get_exact_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys = [], []\n    new_entries = []\n    \n    for i, row in tqdm(need, desc=\"Extracting Audio (2-Sec Strict)\", leave=False):\n        seg = load_exact_audio(row.audio_path)\n        buf_segs.append(seg)\n        buf_keys.append(keys[i])\n        \n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            buf_segs, buf_keys = [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            \n    return embeddings\n\n# ── 5. PREPROCESSING & RISK-BALANCED LIGHTGBM BUILDERS ──────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_audio_pca_preprocessor():\n    return Pipeline([(\"sc\", StandardScaler()), (\"pca\", PCA(n_components=PCA_COMPONENTS, random_state=SEED))])\n\ndef build_risk_balanced_clf(n_pos, n_neg):\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=400, learning_rate=0.02,\n            num_leaves=31, max_depth=5,         \n            subsample=0.8, colsample_bytree=0.6,\n            min_child_samples=15,\n            scale_pos_weight=trbl_scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V6 TRAINING (PCA FUSION + TRBL)\")\nprint(\"=\"*60)\noof_fusion = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    print(f\"[*] Fold Balances:\")\n    print(f\"    Train: {len(y_tr)} (TB+: {y_tr.sum()}) | Val: {len(y_val)} (TB+: {y_val.sum()}) | Test: {len(y_te)} (TB+: {y_te.sum()})\")\n    \n    # Extract Strict 2-sec Embeddings\n    X_tr_emb_raw  = get_exact_embeddings(df_tr)\n    X_val_emb_raw = get_exact_embeddings(df_val)\n    X_te_emb_raw  = get_exact_embeddings(df_te)\n    \n    # PCA Bottleneck\n    pca_prep = build_audio_pca_preprocessor()\n    X_tr_emb  = pca_prep.fit_transform(X_tr_emb_raw)\n    X_val_emb = pca_prep.transform(X_val_emb_raw)\n    X_te_emb  = pca_prep.transform(X_te_emb_raw)\n    \n    print(f\"[*] Audio Features reduced from {X_tr_emb_raw.shape[1]} to {X_tr_emb.shape[1]} via PCA\")\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m  = meta_prep.transform(df_te)\n    \n    print(f\"[*] Clinical Metadata Features generated (MNAR Aware): {X_tr_m.shape[1]}\")\n    \n    # EARLY FUSION\n    X_tr_fusion  = np.hstack([X_tr_emb, X_tr_m])\n    X_val_fusion = np.hstack([X_val_emb, X_val_m])\n    X_te_fusion  = np.hstack([X_te_emb, X_te_m])\n    \n    print(f\"[*] Final Fused Training Matrix Shape: {X_tr_fusion.shape}\")\n    \n    # Train Risk-Balanced LightGBM\n    clf_fusion = build_risk_balanced_clf(int(y_tr.sum()), int((y_tr==0).sum()))\n    clf_fusion.fit(X_tr_fusion, y_tr)\n    cal_fusion = calibrate(clf_fusion, X_val_fusion, y_val)\n    \n    te_prob_fusion = cal_fusion.predict_proba(X_te_fusion)[:,1]\n    oof_fusion[te_idx] = te_prob_fusion\n    \n    fold_auc = roc_auc_score(y_te, te_prob_fusion)\n    print(f\"[*] Fold {fold_i+1} ROC-AUC: {fold_auc:.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_fusion\"] = oof_fusion\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_fusion=(\"pred_fusion\", \"max\")\n).reset_index()\n\nm_fusion = full_eval(cough_df['label'], oof_fusion)\np_fusion = full_eval(part_df['label'], part_df['prob_fusion'])\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V6 (Exact 2s + PCA + TRBL)\", m_fusion, p_fusion)])\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"REPORT-READY SUMMARY (VERSION 6)\")\nprint(\"=\"*85)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v6.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V6 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T02:01:41.620650Z","iopub.execute_input":"2026-02-20T02:01:41.621405Z","iopub.status.idle":"2026-02-20T02:05:58.006437Z","shell.execute_reply.started":"2026-02-20T02:01:41.621377Z","shell.execute_reply":"2026-02-20T02:05:58.005613Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n1. LOADING & HARMONISING DATA\n============================================================\n[*] Identified Numeric Clinical Features: ['age', 'height', 'weight', 'reported_cough_dur', 'heart_rate', 'temperature']\n[*] Identified Categorical Clinical Features: ['sex', 'tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul', 'tb_prior_Unknown', 'hemoptysis', 'weight_loss', 'smoke_lweek', 'fever', 'night_sweats']\n[*] Total valid audio files mapped: 9772\n[*] Total unique participants: 1082\n\n[*] Building Custom Stratified Group K-Folds...\n\n============================================================\n2. LOADING GOOGLE HeAR MODEL\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d1c96f29398400a8fdfe7210af8ad65"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"[*] ✓ HeAR loaded successfully\n\n============================================================\n3. STARTING V6 TRAINING (PCA FUSION + TRBL)\n============================================================\n\n--- FOLD 1/5 ---\n[*] Fold Balances:\n    Train: 6256 (TB+: 1931) | Val: 1565 (TB+: 400) | Test: 1951 (TB+: 599)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict):   0%|          | 0/6256 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict):   0%|          | 0/1565 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict):   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Audio Features reduced from 512 to 32 via PCA\n[*] Clinical Metadata Features generated (MNAR Aware): 27\n[*] Final Fused Training Matrix Shape: (6256, 59)\n[*] Fold 1 ROC-AUC: 0.7734\n\n--- FOLD 2/5 ---\n[*] Fold Balances:\n    Train: 6249 (TB+: 1883) | Val: 1563 (TB+: 421) | Test: 1960 (TB+: 626)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Audio Features reduced from 512 to 32 via PCA\n[*] Clinical Metadata Features generated (MNAR Aware): 27\n[*] Final Fused Training Matrix Shape: (6249, 59)\n[*] Fold 2 ROC-AUC: 0.7922\n\n--- FOLD 3/5 ---\n[*] Fold Balances:\n    Train: 6229 (TB+: 1981) | Val: 1558 (TB+: 372) | Test: 1985 (TB+: 577)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Audio Features reduced from 512 to 32 via PCA\n[*] Clinical Metadata Features generated (MNAR Aware): 27\n[*] Final Fused Training Matrix Shape: (6229, 59)\n[*] Fold 3 ROC-AUC: 0.7516\n\n--- FOLD 4/5 ---\n[*] Fold Balances:\n    Train: 6276 (TB+: 1937) | Val: 1570 (TB+: 401) | Test: 1926 (TB+: 592)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Audio Features reduced from 512 to 32 via PCA\n[*] Clinical Metadata Features generated (MNAR Aware): 27\n[*] Final Fused Training Matrix Shape: (6276, 59)\n[*] Fold 4 ROC-AUC: 0.8289\n\n--- FOLD 5/5 ---\n[*] Fold Balances:\n    Train: 6257 (TB+: 1963) | Val: 1565 (TB+: 431) | Test: 1950 (TB+: 536)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Audio Features reduced from 512 to 32 via PCA\n[*] Clinical Metadata Features generated (MNAR Aware): 27\n[*] Final Fused Training Matrix Shape: (6257, 59)\n[*] Fold 5 ROC-AUC: 0.8280\n\n=====================================================================================\nREPORT-READY SUMMARY (VERSION 6)\n=====================================================================================\n                     Model ROC-AUC (recording) ROC-AUC (participant) Sens@90% Spec@90%\nV6 (Exact 2s + PCA + TRBL)              0.7874                0.7882   0.9003   0.4186\n\n✅ All V6 Results Zipped to: /kaggle/working/outputs_v6.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 7 - THE SOTA LIMIT BREAKER)\n# OOF CVPEM Stacking + Unshackled 512-dim HeAR + TRBL Meta-Learner\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # EXACTLY 2 SECONDS (Based on original notebook constraints)\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V7)\nOUT_ROOT = \"/kaggle/working/outputs_v7\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"oof_stacking_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_exact2s_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION (EXACTLY 2 SECONDS) ─────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_exact_audio(path):\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        if len(audio) < WIN_SAMPLES:\n            audio = np.pad(audio, (0, WIN_SAMPLES - len(audio)))\n        else:\n            audio = audio[:WIN_SAMPLES]\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef get_exact_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys = [], []\n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio (2-Sec Strict)\", leave=False):\n        buf_segs.append(load_exact_audio(row.audio_path))\n        buf_keys.append(keys[i])\n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            buf_segs, buf_keys = [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            \n    return embeddings\n\n# ── 5. PREPROCESSING & OOF STACKING BUILDERS ────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# LEVEL 1 EXPERTS (No PCA, Audio gets full 512 dimensions!)\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, num_leaves=31, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\n# LEVEL 2 SUPERVISOR (With TRBL Loss)\ndef build_supervisor(n_pos, n_neg):\n    # TRBL: Tuberculosis Risk-Balanced Loss (1.5x penalty for False Negatives)\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    return LogisticRegression(class_weight={0: 1.0, 1: trbl_scale}, max_iter=2000, random_state=SEED)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V7 TRAINING (OOF STACKING + UNSHACKLED HeAR)\")\nprint(\"=\"*60)\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    print(f\"[*] Fold Balances:\")\n    print(f\"    Train: {len(y_tr)} (TB+: {y_tr.sum()}) | Val: {len(y_val)} (TB+: {y_val.sum()}) | Test: {len(y_te)} (TB+: {y_te.sum()})\")\n    \n    # Extract Strict 2-sec Embeddings (All 512 dimensions)\n    X_tr_emb = get_exact_embeddings(df_tr)\n    X_val_emb = get_exact_embeddings(df_val)\n    X_te_emb = get_exact_embeddings(df_te)\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m = meta_prep.transform(df_te)\n    \n    # ── LEVEL 1: GENERATE OUT-OF-FOLD (OOF) PROBABILITIES FOR TRAIN ──\n    print(\"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\")\n    cv_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=SEED)\n    inner_folds = list(cv_inner.split(df_tr, y_tr, df_tr[\"participant_id\"]))\n    \n    tr_oof_a = np.zeros(len(y_tr))\n    tr_oof_m = np.zeros(len(y_tr))\n    \n    for i_tr, i_val in inner_folds:\n        # Inner Audio Expert\n        clf_a_inner = build_audio_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum()))\n        clf_a_inner.fit(X_tr_emb[i_tr], y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_inner.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        # Inner Clinical Expert\n        clf_m_inner = build_clinical_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum()))\n        clf_m_inner.fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_inner.predict_proba(X_tr_m[i_val])[:, 1]\n        \n    # ── LEVEL 1: TRAIN EXPERTS ON FULL TRAIN SET & PREDICT VAL/TEST ──\n    print(\"[*] Training Level-1 Experts on Full Train Set\")\n    clf_a = build_audio_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_emb, y_tr)\n    val_prob_a = clf_a.predict_proba(X_val_emb)[:,1]\n    te_prob_a = clf_a.predict_proba(X_te_emb)[:,1]\n    \n    clf_m = build_clinical_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    val_prob_m = clf_m.predict_proba(X_val_m)[:,1]\n    te_prob_m = clf_m.predict_proba(X_te_m)[:,1]\n\n    # ── LEVEL 2: THE SUPERVISOR (TRBL META-LEARNER) ──\n    print(\"[*] Training Level-2 Supervisor on OOF Features\")\n    X_tr_stack = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_val_stack = np.column_stack([val_prob_a, val_prob_m, X_val_m])\n    X_te_stack = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n    \n    supervisor = build_supervisor(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_val_stack, y_val)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    fold_auc = roc_auc_score(y_te, te_prob_stack)\n    print(f\"[*] Fold {fold_i+1} Supervisor ROC-AUC: {fold_auc:.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_stack=(\"pred_stack\", \"max\")\n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"V7 Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V7 (OOF CVPEM Stacking + TRBL)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"REPORT-READY SUMMARY (VERSION 7 - SOTA LIMIT BREAKER)\")\nprint(\"=\"*85)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v7.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V7 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T02:13:38.173105Z","iopub.execute_input":"2026-02-20T02:13:38.173434Z","iopub.status.idle":"2026-02-20T02:21:33.437098Z","shell.execute_reply.started":"2026-02-20T02:13:38.173404Z","shell.execute_reply":"2026-02-20T02:21:33.436365Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n1. LOADING & HARMONISING DATA\n============================================================\n[*] Total valid audio files mapped: 9772\n[*] Total unique participants: 1082\n\n[*] Building Custom Stratified Group K-Folds...\n\n============================================================\n2. LOADING GOOGLE HeAR MODEL\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7b983da64f247d3bb6cd66eeb0f2a24"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"[*] ✓ HeAR loaded successfully\n\n============================================================\n3. STARTING V7 TRAINING (OOF STACKING + UNSHACKLED HeAR)\n============================================================\n\n--- FOLD 1/5 ---\n[*] Fold Balances:\n    Train: 6256 (TB+: 1931) | Val: 1565 (TB+: 400) | Test: 1951 (TB+: 599)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict):   0%|          | 0/6256 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict):   0%|          | 0/1565 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict):   0%|          | 0/1951 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\n[*] Training Level-1 Experts on Full Train Set\n[*] Training Level-2 Supervisor on OOF Features\n[*] Fold 1 Supervisor ROC-AUC: 0.8078\n\n--- FOLD 2/5 ---\n[*] Fold Balances:\n    Train: 6249 (TB+: 1883) | Val: 1563 (TB+: 421) | Test: 1960 (TB+: 626)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\n[*] Training Level-1 Experts on Full Train Set\n[*] Training Level-2 Supervisor on OOF Features\n[*] Fold 2 Supervisor ROC-AUC: 0.8410\n\n--- FOLD 3/5 ---\n[*] Fold Balances:\n    Train: 6229 (TB+: 1981) | Val: 1558 (TB+: 372) | Test: 1985 (TB+: 577)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\n[*] Training Level-1 Experts on Full Train Set\n[*] Training Level-2 Supervisor on OOF Features\n[*] Fold 3 Supervisor ROC-AUC: 0.7726\n\n--- FOLD 4/5 ---\n[*] Fold Balances:\n    Train: 6276 (TB+: 1937) | Val: 1570 (TB+: 401) | Test: 1926 (TB+: 592)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\n[*] Training Level-1 Experts on Full Train Set\n[*] Training Level-2 Supervisor on OOF Features\n[*] Fold 4 Supervisor ROC-AUC: 0.8460\n\n--- FOLD 5/5 ---\n[*] Fold Balances:\n    Train: 6257 (TB+: 1963) | Val: 1565 (TB+: 431) | Test: 1950 (TB+: 536)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (2-Sec Strict): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\n[*] Training Level-1 Experts on Full Train Set\n[*] Training Level-2 Supervisor on OOF Features\n[*] Fold 5 Supervisor ROC-AUC: 0.8143\n\n=====================================================================================\nREPORT-READY SUMMARY (VERSION 7 - SOTA LIMIT BREAKER)\n=====================================================================================\n                         Model ROC-AUC (recording) ROC-AUC (participant) Sens@90% Spec@90%\nV7 (OOF CVPEM Stacking + TRBL)              0.8153                0.8115   0.9003   0.5658\n\n✅ All V7 Results Zipped to: /kaggle/working/outputs_v7.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import os\nimport librosa\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# Root directory from your screenshot\nAUDIO_ROOT = \"/kaggle/input/tb-audio/Tuberculosis/raw_data\"\n\nfolder_stats = []\n\n# Iterate through subfolders (longitudinal_data, solicited_data)\nfor folder_name in ['longitudinal_data', 'solicited_data']:\n    folder_path = os.path.join(AUDIO_ROOT, folder_name)\n    if not os.path.exists(folder_path):\n        continue\n        \n    print(f\"Processing folder: {folder_name}...\")\n    \n    # Find all unique audio files in this specific folder\n    files_in_folder = []\n    for root, _, filenames in os.walk(folder_path):\n        for fn in filenames:\n            if fn.lower().endswith((\".wav\", \".ogg\", \".flac\", \".mp3\")):\n                files_in_folder.append(os.path.join(root, fn))\n    \n    unique_files = list(set(files_in_folder))\n    \n    durations = []\n    sizes = []\n    \n    for path in tqdm(unique_files, desc=f\"Analyzing {folder_name}\", leave=False):\n        try:\n            # Fast duration check without loading the full waveform\n            duration = librosa.get_duration(path=path)\n            durations.append(duration)\n            sizes.append(os.path.getsize(path) / (1024 * 1024)) # MB\n        except Exception:\n            continue\n\n    if durations:\n        folder_stats.append({\n            \"Folder\": folder_name,\n            \"Unique Files\": len(durations),\n            \"Total Minutes\": sum(durations) / 60,\n            \"Avg Sec\": sum(durations) / len(durations),\n            \"Total Size (MB)\": sum(sizes)\n        })\n\n# Display the final breakdown\nstats_df = pd.DataFrame(folder_stats)\nprint(\"\\n\" + \"=\"*60)\nprint(\"AUDIO DATASET BREAKDOWN BY FOLDER\")\nprint(\"=\"*60)\nprint(stats_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T02:33:58.638594Z","iopub.execute_input":"2026-02-20T02:33:58.639322Z","iopub.status.idle":"2026-02-20T02:37:16.135607Z","shell.execute_reply.started":"2026-02-20T02:33:58.639293Z","shell.execute_reply":"2026-02-20T02:37:16.134706Z"}},"outputs":[{"name":"stdout","text":"Processing folder: longitudinal_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing longitudinal_data:   0%|          | 0/19996 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Processing folder: solicited_data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Analyzing solicited_data:   0%|          | 0/9772 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nAUDIO DATASET BREAKDOWN BY FOLDER\n============================================================\n           Folder  Unique Files  Total Minutes  Avg Sec  Total Size (MB)\nlongitudinal_data         19996     166.507850 0.499623       841.826656\n   solicited_data          9772      81.241302 0.498821       410.739120\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 8.1 - SOTA LIMIT BREAKER)\n# Acoustic Tiling (Zero-Silence) + OOF Stacking + Non-Linear Supervisor\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # Google HeAR HARDCODED constraint (2.0s)\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V8.1)\nOUT_ROOT = \"/kaggle/working/outputs_v8_1\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"tiled_oof_stacking\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_tiled2s_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. ACOUSTIC TILING (ZERO-SILENCE) ───────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL & EXTRACTING AUDIO\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_tiled_audio(path):\n    \"\"\"If audio is < 2s, TILES (repeats) the audio instead of padding with silence.\"\"\"\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        dur = len(audio) / sr\n        \n        if len(audio) == 0:\n            return np.zeros(WIN_SAMPLES, np.float32), 0.0\n            \n        if len(audio) < WIN_SAMPLES:\n            # TILING THE SIGNAL: Repeats 0.5s audio 4x to fill 2.0s\n            repeats = int(np.ceil(WIN_SAMPLES / len(audio)))\n            audio = np.tile(audio, repeats)[:WIN_SAMPLES]\n        else:\n            # Smart Energy Peak Detection for files > 2s\n            frame_len = 400; hop = 160\n            frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n            rms = np.sqrt(np.mean(frames**2, axis=0))\n            smooth_n = max(1, int(0.2 * sr / hop))\n            rms_smooth = np.convolve(rms, np.ones(smooth_n)/smooth_n, mode=\"same\")\n            peak_fr = int(np.argmax(rms_smooth))\n            center = peak_fr * hop + frame_len // 2\n            \n            start = max(0, center - WIN_SAMPLES // 2)\n            end = start + WIN_SAMPLES\n            if end > len(audio):\n                end = len(audio); start = max(0, len(audio) - WIN_SAMPLES)\n            audio = audio[start:end]\n            \n        return audio, dur\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32), 0.0\n\ndef get_tiled_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    durations = np.zeros(N, np.float32)\n    \n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys, buf_durs = [], [], []\n    new_entries = []\n    \n    for i, row in tqdm(need, desc=\"Extracting Audio (Acoustic Tiling)\", leave=False):\n        seg, dur = load_tiled_audio(row.audio_path)\n        buf_segs.append(seg)\n        buf_keys.append(keys[i])\n        buf_durs.append(dur)\n        \n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            buf_segs, buf_keys, buf_durs = [], [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], zip(cache[\"embedding\"], cache[\"duration\"])))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            emb_val, dur_val = cache_dict[k]\n            embeddings[i] = np.array(emb_val, np.float32) if not isinstance(emb_val, np.ndarray) else emb_val\n            durations[i]  = float(dur_val)\n            \n    return embeddings, durations\n\n# ── 5. PREPROCESSING & OOF STACKING BUILDERS ────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# LEVEL 1 EXPERTS\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, num_leaves=31, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\n# LEVEL 2 SUPERVISOR (Non-Linear LightGBM with TRBL)\ndef build_supervisor(n_pos, n_neg):\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100, learning_rate=0.03,\n            num_leaves=7, max_depth=3,  # Very shallow to avoid overfitting meta-features\n            min_child_samples=10,\n            scale_pos_weight=trbl_scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight={0: 1.0, 1: trbl_scale}, max_iter=2000, random_state=SEED)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V8.1 TRAINING (ACOUSTIC TILING + STACKING)\")\nprint(\"=\"*60)\n\n# Pre-fetch all embeddings once to print duration stats\nprint(\"[*] Pre-fetching audio to audit durations & tile...\")\nall_embs, all_durs = get_tiled_embeddings(cough_df)\nprint(f\"    -> Audio Durations (seconds) | Min: {np.min(all_durs):.2f}s | Max: {np.max(all_durs):.2f}s | Mean: {np.mean(all_durs):.2f}s\")\n\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # Extract Smart Embeddings\n    X_tr_emb, _ = get_tiled_embeddings(df_tr)\n    X_val_emb, _ = get_tiled_embeddings(df_val)\n    X_te_emb, _ = get_tiled_embeddings(df_te)\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m = meta_prep.transform(df_te)\n    \n    # ── LEVEL 1: GENERATE OUT-OF-FOLD (OOF) PROBABILITIES FOR TRAIN ──\n    print(\"[*] Generating Out-Of-Fold Probabilities...\")\n    cv_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=SEED)\n    inner_folds = list(cv_inner.split(df_tr, y_tr, df_tr[\"participant_id\"]))\n    \n    tr_oof_a = np.zeros(len(y_tr))\n    tr_oof_m = np.zeros(len(y_tr))\n    \n    for i_tr, i_val in inner_folds:\n        clf_a_inner = build_audio_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_emb[i_tr], y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_inner.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        clf_m_inner = build_clinical_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_inner.predict_proba(X_tr_m[i_val])[:, 1]\n        \n    # ── LEVEL 1: TRAIN EXPERTS ON FULL TRAIN SET ──\n    clf_a = build_audio_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_emb, y_tr)\n    val_prob_a = clf_a.predict_proba(X_val_emb)[:,1]\n    te_prob_a = clf_a.predict_proba(X_te_emb)[:,1]\n    \n    clf_m = build_clinical_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    val_prob_m = clf_m.predict_proba(X_val_m)[:,1]\n    te_prob_m = clf_m.predict_proba(X_te_m)[:,1]\n\n    # ── LEVEL 2: THE NON-LINEAR SUPERVISOR ──\n    X_tr_stack = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_val_stack = np.column_stack([val_prob_a, val_prob_m, X_val_m])\n    X_te_stack = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n    \n    supervisor = build_supervisor(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_val_stack, y_val)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    print(f\"[*] Fold {fold_i+1} Supervisor ROC-AUC: {roc_auc_score(y_te, te_prob_stack):.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_stack=(\"pred_stack\", \"max\")\n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"V8.1 Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V8.1 (Acoustic Tiling + LGBM Meta)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"REPORT-READY SUMMARY (VERSION 8.1 - SOTA LIMIT BREAKER)\")\nprint(\"=\"*85)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v8_1.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V8.1 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T02:59:06.327445Z","iopub.execute_input":"2026-02-20T02:59:06.328130Z","iopub.status.idle":"2026-02-20T03:07:20.936067Z","shell.execute_reply.started":"2026-02-20T02:59:06.328100Z","shell.execute_reply":"2026-02-20T03:07:20.935259Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n1. LOADING & HARMONISING DATA\n============================================================\n[*] Total valid audio files mapped: 9772\n[*] Total unique participants: 1082\n\n[*] Building Custom Stratified Group K-Folds...\n\n============================================================\n2. LOADING GOOGLE HeAR MODEL & EXTRACTING AUDIO\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd280e2ab504c0ba69f5588165c2930"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"[*] ✓ HeAR loaded successfully\n\n============================================================\n3. STARTING V8.1 TRAINING (ACOUSTIC TILING + STACKING)\n============================================================\n[*] Pre-fetching audio to audit durations & tile...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling):   0%|          | 0/9772 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"    -> Audio Durations (seconds) | Min: 0.34s | Max: 0.50s | Mean: 0.50s\n\n--- FOLD 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 1 Supervisor ROC-AUC: 0.8078\n\n--- FOLD 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 2 Supervisor ROC-AUC: 0.8031\n\n--- FOLD 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 3 Supervisor ROC-AUC: 0.7525\n\n--- FOLD 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 4 Supervisor ROC-AUC: 0.8412\n\n--- FOLD 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Acoustic Tiling): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 5 Supervisor ROC-AUC: 0.8332\n\n=====================================================================================\nREPORT-READY SUMMARY (VERSION 8.1 - SOTA LIMIT BREAKER)\n=====================================================================================\n                             Model ROC-AUC (recording) ROC-AUC (participant) Sens@90% Spec@90%\nV8.1 (Acoustic Tiling + LGBM Meta)              0.8023                0.7977   0.9010   0.5294\n\n✅ All V8.1 Results Zipped to: /kaggle/working/outputs_v8_1.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 9 - THE DEPLOYMENT CANDIDATE)\n# Mirrored Tiling (No Artifacts) + OOF Stacking + Mean Probability Voting\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # Google HeAR HARDCODED constraint (2.0s)\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V9)\nOUT_ROOT = \"/kaggle/working/outputs_v9\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"mirrored_oof_stacking\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mirrored_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. ACOUSTIC REFLECTION (ZERO-ARTIFACT TILING) ───────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL & EXTRACTING AUDIO\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_reflected_audio(path):\n    \"\"\"Uses Mirrored Tiling to extend 0.5s audio to 2.0s without seam artifacts.\"\"\"\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        dur = len(audio) / sr\n        \n        if len(audio) == 0:\n            return np.zeros(WIN_SAMPLES, np.float32), 0.0\n            \n        if len(audio) < WIN_SAMPLES:\n            # MIRRORED TILING (Acoustic Reflection):\n            # Normal tiling [1, 2, 3] -> [1, 2, 3 | 1, 2, 3] creates a harsh \"click\" at 3->1.\n            # Mirrored tiling [1, 2, 3] -> [1, 2, 3, 3, 2, 1 | 1, 2, 3] ensures perfect amplitude continuity!\n            audio_mirrored = np.concatenate((audio, audio[::-1]))\n            repeats = int(np.ceil(WIN_SAMPLES / len(audio_mirrored)))\n            audio = np.tile(audio_mirrored, repeats)[:WIN_SAMPLES]\n        else:\n            # Smart Energy Peak Detection for files > 2s\n            frame_len = 400; hop = 160\n            frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n            rms = np.sqrt(np.mean(frames**2, axis=0))\n            smooth_n = max(1, int(0.2 * sr / hop))\n            rms_smooth = np.convolve(rms, np.ones(smooth_n)/smooth_n, mode=\"same\")\n            peak_fr = int(np.argmax(rms_smooth))\n            center = peak_fr * hop + frame_len // 2\n            \n            start = max(0, center - WIN_SAMPLES // 2)\n            end = start + WIN_SAMPLES\n            if end > len(audio):\n                end = len(audio); start = max(0, len(audio) - WIN_SAMPLES)\n            audio = audio[start:end]\n            \n        return audio, dur\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32), 0.0\n\ndef get_reflected_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    durations = np.zeros(N, np.float32)\n    \n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys, buf_durs = [], [], []\n    new_entries = []\n    \n    for i, row in tqdm(need, desc=\"Extracting Audio (Mirrored)\", leave=False):\n        seg, dur = load_reflected_audio(row.audio_path)\n        buf_segs.append(seg)\n        buf_keys.append(keys[i])\n        buf_durs.append(dur)\n        \n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            buf_segs, buf_keys, buf_durs = [], [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], zip(cache[\"embedding\"], cache[\"duration\"])))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            emb_val, dur_val = cache_dict[k]\n            embeddings[i] = np.array(emb_val, np.float32) if not isinstance(emb_val, np.ndarray) else emb_val\n            durations[i]  = float(dur_val)\n            \n    return embeddings, durations\n\n# ── 5. PREPROCESSING & OOF STACKING BUILDERS ────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# LEVEL 1 EXPERTS\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        # Added colsample_bytree to force LightGBM to generalize across the 512 dimensions\n        return lgb.LGBMClassifier(\n            n_estimators=300, learning_rate=0.03, num_leaves=31, \n            colsample_bytree=0.3, scale_pos_weight=scale, \n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\n# LEVEL 2 SUPERVISOR (Non-Linear LightGBM with TRBL)\ndef build_supervisor(n_pos, n_neg):\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100, learning_rate=0.03,\n            num_leaves=7, max_depth=3,\n            min_child_samples=10,\n            scale_pos_weight=trbl_scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight={0: 1.0, 1: trbl_scale}, max_iter=2000, random_state=SEED)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V9 TRAINING (MIRRORED AUDIO + MEAN POOLING)\")\nprint(\"=\"*60)\n\nprint(\"[*] Pre-fetching audio to audit durations & apply acoustic reflection...\")\nall_embs, all_durs = get_reflected_embeddings(cough_df)\n\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # Extract Mirrored Embeddings\n    X_tr_emb, _ = get_reflected_embeddings(df_tr)\n    X_val_emb, _ = get_reflected_embeddings(df_val)\n    X_te_emb, _ = get_reflected_embeddings(df_te)\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m = meta_prep.transform(df_te)\n    \n    # ── LEVEL 1: GENERATE OUT-OF-FOLD (OOF) PROBABILITIES FOR TRAIN ──\n    print(\"[*] Generating Out-Of-Fold Probabilities...\")\n    cv_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=SEED)\n    inner_folds = list(cv_inner.split(df_tr, y_tr, df_tr[\"participant_id\"]))\n    \n    tr_oof_a = np.zeros(len(y_tr))\n    tr_oof_m = np.zeros(len(y_tr))\n    \n    for i_tr, i_val in inner_folds:\n        clf_a_inner = build_audio_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_emb[i_tr], y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_inner.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        clf_m_inner = build_clinical_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_inner.predict_proba(X_tr_m[i_val])[:, 1]\n        \n    # ── LEVEL 1: TRAIN EXPERTS ON FULL TRAIN SET ──\n    clf_a = build_audio_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_emb, y_tr)\n    val_prob_a = clf_a.predict_proba(X_val_emb)[:,1]\n    te_prob_a = clf_a.predict_proba(X_te_emb)[:,1]\n    \n    clf_m = build_clinical_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    val_prob_m = clf_m.predict_proba(X_val_m)[:,1]\n    te_prob_m = clf_m.predict_proba(X_te_m)[:,1]\n\n    # ── LEVEL 2: THE NON-LINEAR SUPERVISOR ──\n    X_tr_stack = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_val_stack = np.column_stack([val_prob_a, val_prob_m, X_val_m])\n    X_te_stack = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n    \n    supervisor = build_supervisor(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_val_stack, y_val)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    print(f\"[*] Fold {fold_i+1} Supervisor ROC-AUC: {roc_auc_score(y_te, te_prob_stack):.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\n# ----------------------------------------------------------------------------\n# THE SPECIFICITY FIX: Use 'mean' instead of 'max' to prevent noisy outliers \n# from destroying healthy patient predictions!\n# ----------------------------------------------------------------------------\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), \n    prob_stack=(\"pred_stack\", \"mean\")  \n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"V9 Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V9 (Mirrored Audio + Mean Voting)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"REPORT-READY SUMMARY (VERSION 9 - DEPLOYMENT CANDIDATE)\")\nprint(\"=\"*90)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v9.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V9 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T03:14:02.347610Z","iopub.execute_input":"2026-02-20T03:14:02.347959Z","iopub.status.idle":"2026-02-20T03:19:28.311023Z","shell.execute_reply.started":"2026-02-20T03:14:02.347931Z","shell.execute_reply":"2026-02-20T03:19:28.310346Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n1. LOADING & HARMONISING DATA\n============================================================\n[*] Total valid audio files mapped: 9772\n[*] Total unique participants: 1082\n\n[*] Building Custom Stratified Group K-Folds...\n\n============================================================\n2. LOADING GOOGLE HeAR MODEL & EXTRACTING AUDIO\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b94c7a636c944c99e97073dfaa0b092"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"[*] ✓ HeAR loaded successfully\n\n============================================================\n3. STARTING V9 TRAINING (MIRRORED AUDIO + MEAN POOLING)\n============================================================\n[*] Pre-fetching audio to audit durations & apply acoustic reflection...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored):   0%|          | 0/9772 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n--- FOLD 1/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 1 Supervisor ROC-AUC: 0.8031\n\n--- FOLD 2/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 2 Supervisor ROC-AUC: 0.8098\n\n--- FOLD 3/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 3 Supervisor ROC-AUC: 0.7511\n\n--- FOLD 4/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 4 Supervisor ROC-AUC: 0.8440\n\n--- FOLD 5/5 ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting Audio (Mirrored): 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Generating Out-Of-Fold Probabilities...\n[*] Fold 5 Supervisor ROC-AUC: 0.8302\n\n==========================================================================================\nREPORT-READY SUMMARY (VERSION 9 - DEPLOYMENT CANDIDATE)\n==========================================================================================\n                            Model ROC-AUC (recording) ROC-AUC (participant) Sens@90% Spec@90%\nV9 (Mirrored Audio + Mean Voting)              0.8016                0.7958   0.9010   0.5338\n\n✅ All V9 Results Zipped to: /kaggle/working/outputs_v9.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 10 - RESEARCH CANDIDATE)\n# ============================================================================\n# KEY IMPROVEMENTS OVER V9:\n#\n#  1. MULTI-WINDOW EMBEDDING: Every cough recording is sliced into ALL possible\n#     2-second windows (with 50% overlap), each embedded by HeAR, then\n#     aggregated (mean + std + [25,50,75] percentiles = 5x512 = 2560-dim feature).\n#     This replaces the single-window \"energy peak\" approach that discards signal.\n#\n#  2. PARTICIPANT-LEVEL CLASSIFICATION (no late-fusion): All cough embeddings\n#     from a participant are aggregated BEFORE classification, not after.\n#     The model sees a 2560-dim \"acoustic fingerprint\" per participant,\n#     combined with clinical meta. This avoids noisy recording-level predictions.\n#\n#  3. PARTIAL AUC (pAUC) AS OBJECTIVE: The CODA challenge metric is specificity\n#     at ≥90% sensitivity. We add a pAUC metric (TPR in [0.85, 1.0]) and tune\n#     all thresholds against it.\n#\n#  4. COUNTRY AS EXPLICIT FEATURE: AUROC varies dramatically by country in this\n#     dataset (0.63-0.81 in the challenge). Country is now a hard-coded feature.\n#\n#  5. EMBEDDING NOISE AUGMENTATION: Gaussian noise injection on HeAR embeddings\n#     during training (σ=0.01) provides regularization without touching audio.\n#\n#  6. FIXED CALIBRATION SPLIT: Uses a proper StratifiedGroupKFold inner split\n#     for calibration data, not a naive sequential iloc slice.\n#\n#  7. COUGH COUNT AS FEATURE: Number of valid cough segments per participant\n#     is included as an explicit clinical feature.\n#\n#  8. CLEANER CACHE: Cache key now includes a content hash of the audio path\n#     only (no version string that can silently go stale).\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom scipy import stats as sp_stats\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR          = 16_000\nWIN_SAMPLES = 32_000   # 2s @ 16kHz — HeAR hard constraint\nHOP_SAMPLES = 16_000   # 50% overlap for multi-window extraction\nEMBED_DIM   = 512\nN_SPLITS    = 5\nTARGET_SENS = [0.85, 0.90, 0.95]\nPAUC_LOW    = 0.85     # pAUC window: specificity-at-sensitivity >= PAUC_LOW\n\n# Aggregation stats per participant (mean + std + 3 percentiles = 5 vectors)\nAGG_FUNCS   = [\"mean\", \"std\", \"p25\", \"p50\", \"p75\"]\nAGG_DIM     = EMBED_DIM * len(AGG_FUNCS)  # 2560\n\n# Embedding augmentation noise (Gaussian, applied only during training)\nEMB_NOISE_STD = 0.01\n\n# Output Directories\nOUT_ROOT   = \"/kaggle/working/outputs_v10\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"multiwindow_participant\")\nCACHE_DIR  = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nEMBED_CACHE = os.path.join(CACHE_DIR, \"hear_multiwindow_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio    = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\n# ── Clinical feature selection (exclude post-diagnostic leakage) ─────────────\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\n                \"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\n# Ensure country column is included as categorical (high-value feature)\ncountry_col = None\nfor hint in [\"country\", \"site\", \"country_id\", \"collection_country\"]:\n    matches = [c for c in df_clinical.columns if hint in c.lower()]\n    if matches:\n        country_col = matches[0]\n        if country_col not in cat_cols:\n            cat_cols.append(country_col)\n        break\n\nif country_col:\n    print(f\"[*] Country feature found: '{country_col}'\")\nelse:\n    print(\"[!] WARNING: No country column found. Country is a top-3 feature for TB!\")\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols],\n                          on=\"participant_id\", how=\"left\")\n\n# ── Audio file mapping ────────────────────────────────────────────────────────\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(\n    lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\nprint(f\"[*] Num clinical features — numerical: {len(num_cols)}, categorical: {len(cat_cols)}\")\n\n# ── 3. HeAR MODEL LOADING ────────────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL   = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments: list) -> np.ndarray:\n    \"\"\"Run HeAR on a batch of 2s segments. Returns (N, 512) float32.\"\"\"\n    if HEAR_SERVING is None:\n        return np.zeros((len(segments), EMBED_DIM), np.float32)\n    import tensorflow as tf\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\n# ── 4. MULTI-WINDOW AUDIO LOADING ────────────────────────────────────────────\ndef load_audio(path: str):\n    \"\"\"Load audio, return float32 waveform at SR.\"\"\"\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef extract_windows(audio: np.ndarray) -> list:\n    \"\"\"\n    Slice audio into all WIN_SAMPLES windows with HOP_SAMPLES stride.\n    Short clips: mirrored-tile to WIN_SAMPLES, yielding exactly 1 window.\n    Long clips: sliding window with 50% overlap — captures all cough events.\n    \"\"\"\n    if len(audio) == 0:\n        return [np.zeros(WIN_SAMPLES, np.float32)]\n\n    if len(audio) < WIN_SAMPLES:\n        # Mirrored tiling (no click artifact at boundaries)\n        audio_mir = np.concatenate((audio, audio[::-1]))\n        repeats   = int(np.ceil(WIN_SAMPLES / len(audio_mir)))\n        audio     = np.tile(audio_mir, repeats)[:WIN_SAMPLES]\n        return [audio.astype(np.float32)]\n\n    windows = []\n    start   = 0\n    while start + WIN_SAMPLES <= len(audio):\n        windows.append(audio[start:start + WIN_SAMPLES].astype(np.float32))\n        start += HOP_SAMPLES\n\n    # Always include the tail window to capture the end of long coughs\n    if start < len(audio):\n        tail_start = len(audio) - WIN_SAMPLES\n        windows.append(audio[tail_start:].astype(np.float32))\n\n    return windows if windows else [audio[:WIN_SAMPLES].astype(np.float32)]\n\ndef aggregate_embeddings(emb_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Aggregate (N_windows, 512) embeddings into a single (2560,) vector.\n    Stats: mean, std, 25th, 50th, 75th percentile across windows.\n    This is much richer than a single-window embedding.\n    \"\"\"\n    if emb_matrix.shape[0] == 1:\n        # Single window: pad std/percentiles with zeros (no variance info)\n        e = emb_matrix[0]\n        return np.concatenate([e, np.zeros(EMBED_DIM * 4, np.float32)])\n    m   = emb_matrix.mean(axis=0)\n    s   = emb_matrix.std(axis=0)\n    p25 = np.percentile(emb_matrix, 25, axis=0)\n    p50 = np.percentile(emb_matrix, 50, axis=0)\n    p75 = np.percentile(emb_matrix, 75, axis=0)\n    return np.concatenate([m, s, p25, p50, p75]).astype(np.float32)\n\n# ── 5. EMBEDDING EXTRACTION WITH CACHING ─────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. EXTRACTING MULTI-WINDOW HeAR EMBEDDINGS\")\nprint(\"=\"*60)\n\ndef get_multiwindow_embeddings(df_rows: pd.DataFrame):\n    \"\"\"\n    Returns:\n      agg_embeddings: (N_rows, AGG_DIM=2560)  — per-recording aggregated embedding\n      n_windows_arr:  (N_rows,)               — number of windows extracted per recording\n    \"\"\"\n    if os.path.exists(EMBED_CACHE):\n        try:    cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n    else:\n        cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n\n    N   = len(df_rows)\n    agg = np.zeros((N, AGG_DIM), np.float32)\n    nw  = np.zeros(N, np.int32)\n\n    # Use path-only hash (no version string that can silently go stale)\n    keys         = [hashlib.md5(str(r.audio_path).encode()).hexdigest()\n                    for _, r in df_rows.iterrows()]\n    cached_keys  = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    need         = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows())\n                    if keys[i] not in cached_keys]\n\n    # Process in batches across all windows from multiple files\n    BATCH = 64\n    buf_segs, buf_meta = [], []  # meta = (row_idx, key, window_idx_for_file, total_for_file)\n\n    new_entries = {}  # key -> {\"segments_emb\": list_of_embs, \"n_windows\": int}\n\n    for i, row in tqdm(need, desc=\"Loading audio\", leave=False):\n        audio   = load_audio(row.audio_path)\n        windows = extract_windows(audio)\n        k       = keys[i]\n        new_entries[k] = {\"n_windows\": len(windows), \"embs\": []}\n\n        for seg in windows:\n            buf_segs.append(seg)\n            buf_meta.append(k)\n\n            if len(buf_segs) >= BATCH:\n                batch_embs = _infer_batch(buf_segs)\n                for idx, (bk, be) in enumerate(zip(buf_meta, batch_embs)):\n                    new_entries[bk][\"embs\"].append(be)\n                buf_segs, buf_meta = [], []\n\n    if buf_segs:\n        batch_embs = _infer_batch(buf_segs)\n        for bk, be in zip(buf_meta, batch_embs):\n            new_entries[bk][\"embs\"].append(be)\n\n    # Aggregate and save new entries\n    new_rows = []\n    for k, v in new_entries.items():\n        emb_mat = np.stack(v[\"embs\"])           # (n_windows, 512)\n        agg_emb = aggregate_embeddings(emb_mat) # (2560,)\n        new_rows.append({\"key\": k,\n                         \"agg_embedding\": agg_emb.tolist(),\n                         \"n_windows\": v[\"n_windows\"]})\n\n    if new_rows:\n        cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n        cache.to_parquet(EMBED_CACHE, index=False)\n\n    cache_dict = {row[\"key\"]: row for _, row in cache.iterrows()}\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            r       = cache_dict[k]\n            agg_val = r[\"agg_embedding\"]\n            agg[i]  = np.array(agg_val, np.float32) if not isinstance(agg_val, np.ndarray) else agg_val\n            nw[i]   = int(r[\"n_windows\"])\n\n    return agg, nw\n\n# Extract all embeddings upfront\nprint(\"[*] Pre-fetching all audio embeddings (multi-window)...\")\nall_agg_embs, all_n_windows = get_multiwindow_embeddings(cough_df)\ncough_df[\"n_cough_windows\"] = all_n_windows\n\nprint(f\"[*] Embedding shape per recording: ({AGG_DIM},)\")\nprint(f\"[*] Avg windows per recording: {all_n_windows.mean():.1f}  \"\n      f\"(max={all_n_windows.max()}, min={all_n_windows.min()})\")\n\n# ── 6. PARTICIPANT-LEVEL AGGREGATION ─────────────────────────────────────────\n# Pool all per-recording embeddings into one fingerprint per participant.\n# This is the key architectural insight: the model should reason about\n# a participant, not individual recordings.\nprint(\"\\n[*] Aggregating to participant level...\")\n\nparticipant_ids  = cough_df[\"participant_id\"].values\nunique_pids      = cough_df[\"participant_id\"].unique()\n\n# Build a participant-level DataFrame\npid_records = []\nfor pid in unique_pids:\n    mask  = participant_ids == pid\n    p_embs = all_agg_embs[mask]   # (n_recordings, 2560)\n    label  = cough_df.loc[mask, \"label\"].values[0]\n    n_recs = mask.sum()\n\n    # Average across recordings for this participant\n    p_mean_emb = p_embs.mean(axis=0)  # (2560,)\n\n    # Get clinical features from first row (they're participant-level)\n    first_row = cough_df.loc[mask].iloc[0]\n    rec = {\"participant_id\": pid, \"label\": label, \"n_recordings\": n_recs}\n    for col in num_cols + cat_cols:\n        rec[col] = first_row.get(col, np.nan)\n    rec[\"n_cough_windows_total\"] = int(cough_df.loc[mask, \"n_cough_windows\"].sum())\n    pid_records.append((rec, p_mean_emb))\n\nparticipant_df  = pd.DataFrame([r for r, _ in pid_records]).reset_index(drop=True)\nparticipant_embs = np.stack([e for _, e in pid_records])  # (N_participants, 2560)\n\nprint(f\"[*] Participant-level dataset: {len(participant_df)} participants, \"\n      f\"{participant_df['label'].sum()} TB+, {(participant_df['label']==0).sum()} TB-\")\n\n# Add n_recordings and n_cough_windows_total to numerical features\nextra_num = [\"n_recordings\", \"n_cough_windows_total\"]\nnum_cols_p = num_cols + extra_num\n\n# ── 7. CROSS-VALIDATION SETUP ────────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. STARTING V10 TRAINING (PARTICIPANT-LEVEL + MULTI-WINDOW)\")\nprint(\"=\"*60)\n\n# Folds are at participant level (no group constraint needed — already deduplicated)\nsgkf  = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(participant_df, participant_df[\"label\"]))\n\n# ── 8. PREPROCESSING & MODEL BUILDERS ────────────────────────────────────────\ndef build_meta_preprocessor(num_c, cat_c):\n    transformers = []\n    if num_c:\n        transformers.append((\"num\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n            (\"sc\",  StandardScaler())\n        ]), num_c))\n    if cat_c:\n        transformers.append((\"cat\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n        ]), cat_c))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef add_embedding_noise(X: np.ndarray, noise_std: float = EMB_NOISE_STD) -> np.ndarray:\n    \"\"\"Gaussian noise injection for embedding augmentation (training only).\"\"\"\n    return X + np.random.normal(0, noise_std, X.shape).astype(np.float32)\n\ndef build_audio_expert(n_pos, n_neg):\n    \"\"\"LightGBM over aggregated multi-window HeAR embeddings (2560-dim).\"\"\"\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=400,\n            learning_rate=0.02,\n            num_leaves=31,\n            colsample_bytree=0.4,    # important: 2560 features, need regularization\n            subsample=0.8,\n            min_child_samples=5,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=3000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    \"\"\"LightGBM over clinical + demographic features.\"\"\"\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=200,\n            learning_rate=0.02,\n            num_leaves=15,\n            max_depth=4,\n            min_child_samples=5,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\")\n\ndef build_supervisor(n_pos, n_neg):\n    \"\"\"\n    Shallow meta-learner over stacked [audio_prob, clinical_prob, clinical_features].\n    Deliberately constrained to prevent overfitting on small N.\n    \"\"\"\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100,\n            learning_rate=0.02,\n            num_leaves=7,\n            max_depth=3,\n            min_child_samples=8,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight={0: 1.0, 1: scale}, max_iter=2000, random_state=SEED)\n\n# ── 9. EVALUATION HELPERS ────────────────────────────────────────────────────\ndef partial_auc(y_true, y_prob, low_tpr=PAUC_LOW):\n    \"\"\"\n    Partial AUC: area under ROC curve where TPR (sensitivity) >= low_tpr.\n    Normalized to [0, 1] by dividing by the max possible area (1 - low_tpr).\n    This is the primary metric aligned with the CODA challenge objective.\n    \"\"\"\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    # We want the region where TPR >= low_tpr\n    mask  = tpr >= low_tpr\n    if mask.sum() < 2:\n        return 0.0\n    # Interpolate the FPR at exactly tpr=low_tpr\n    sub_fpr = fpr[mask]\n    sub_tpr = tpr[mask]\n    area    = float(np.trapz(sub_tpr, sub_fpr))\n    # Area is negative because fpr is typically increasing -> abs\n    area    = abs(area)\n    max_area = (1.0 - low_tpr)\n    return area / max_area if max_area > 0 else 0.0\n\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\n        \"threshold\":   float(t),\n        \"accuracy\":    float(accuracy_score(y_true, y_pred)),\n        \"sensitivity\": tp / (tp + fn + 1e-9),\n        \"specificity\": tn / (tn + fp + 1e-9),\n        \"f1\":          float(f1_score(y_true, y_pred, zero_division=0))\n    }\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\n        \"roc_auc\":  float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else np.nan,\n        \"pauc_85\":  partial_auc(y_true, y_prob, low_tpr=0.85),\n        \"pauc_90\":  partial_auc(y_true, y_prob, low_tpr=0.90),\n    }\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\n            \"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)\n        }\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    auc = roc_auc_score(y_true, y_prob)\n    fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n\n    # Full ROC\n    ax = axes[0]\n    ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1)\n    ax.set(title=f\"{title_prefix} — Full ROC\", xlabel=\"FPR\", ylabel=\"TPR\")\n    ax.legend()\n\n    # Partial AUC zoom (high sensitivity region)\n    ax = axes[1]\n    mask = tpr >= 0.85\n    ax.fill_between(fpr[mask], tpr[mask], 0.85, alpha=0.25, color=\"#457b9d\",\n                    label=f\"pAUC@85%={partial_auc(y_true,y_prob,0.85):.3f}\")\n    ax.plot(fpr, tpr, color=\"#e63946\", lw=2)\n    ax.set_xlim([0, 1]); ax.set_ylim([0.8, 1.0])\n    ax.set(title=\"Partial AUC (TPR≥85%)\", xlabel=\"FPR\", ylabel=\"TPR\")\n    ax.legend(fontsize=8)\n\n    fig.tight_layout()\n    fig.savefig(f\"{path_prefix}_roc.png\", dpi=150)\n    plt.close(fig)\n\n# ── 10. MAIN TRAINING LOOP ───────────────────────────────────────────────────\noof_stack = np.zeros(len(participant_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n\n    df_tr_full   = participant_df.iloc[tr_idx].reset_index(drop=True)\n    df_te        = participant_df.iloc[te_idx].reset_index(drop=True)\n    emb_tr_full  = participant_embs[tr_idx]\n    emb_te       = participant_embs[te_idx]\n\n    y_tr_full    = df_tr_full[\"label\"].values\n    y_te         = df_te[\"label\"].values\n\n    # ── Calibration split: 20% of train, stratified ──────────────────────────\n    cal_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n    tr_sub_idx, cal_idx = next(cal_fold.split(df_tr_full, y_tr_full))\n\n    df_tr     = df_tr_full.iloc[tr_sub_idx].reset_index(drop=True)\n    df_cal    = df_tr_full.iloc[cal_idx].reset_index(drop=True)\n    emb_tr    = emb_tr_full[tr_sub_idx]\n    emb_cal   = emb_tr_full[cal_idx]\n    y_tr      = df_tr[\"label\"].values\n    y_cal     = df_cal[\"label\"].values\n\n    print(f\"[*] Train: {len(y_tr)} ({y_tr.sum()} TB+)  \"\n          f\"| Cal: {len(y_cal)} ({y_cal.sum()} TB+)  \"\n          f\"| Test: {len(y_te)} ({y_te.sum()} TB+)\")\n\n    # ── Preprocess clinical features ──────────────────────────────────────────\n    meta_prep  = build_meta_preprocessor(num_cols_p, cat_cols)\n    X_tr_m     = meta_prep.fit_transform(df_tr)\n    X_cal_m    = meta_prep.transform(df_cal)\n    X_te_m     = meta_prep.transform(df_te)\n\n    # ── Embedding noise augmentation (training only) ──────────────────────────\n    X_tr_emb   = add_embedding_noise(emb_tr)\n    X_cal_emb  = emb_cal   # no augmentation at calibration/test time\n    X_te_emb   = emb_te\n\n    # ── LEVEL 1: Inner OOF for stacking meta-features ────────────────────────\n    print(\"[*]  Generating inner OOF probabilities...\")\n    inner_cv   = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n    tr_oof_a   = np.zeros(len(y_tr))\n    tr_oof_m   = np.zeros(len(y_tr))\n\n    for i_tr, i_val in inner_cv.split(X_tr_emb, y_tr):\n        n_p  = int(y_tr[i_tr].sum()); n_n = int((y_tr[i_tr]==0).sum())\n\n        # Audio expert (inner)\n        clf_a_i = build_audio_expert(n_p, n_n)\n        X_aug   = add_embedding_noise(X_tr_emb[i_tr])   # augment inner too\n        clf_a_i.fit(X_aug, y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_i.predict_proba(X_tr_emb[i_val])[:, 1]\n\n        # Clinical expert (inner)\n        clf_m_i = build_clinical_expert(n_p, n_n)\n        clf_m_i.fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_i.predict_proba(X_tr_m[i_val])[:, 1]\n\n    # ── LEVEL 1: Full experts on all train data ───────────────────────────────\n    n_pos = int(y_tr.sum()); n_neg = int((y_tr==0).sum())\n\n    clf_a = build_audio_expert(n_pos, n_neg)\n    clf_a.fit(X_tr_emb, y_tr)\n    cal_prob_a = clf_a.predict_proba(X_cal_emb)[:, 1]\n    te_prob_a  = clf_a.predict_proba(X_te_emb)[:, 1]\n\n    clf_m = build_clinical_expert(n_pos, n_neg)\n    clf_m.fit(X_tr_m, y_tr)\n    cal_prob_m = clf_m.predict_proba(X_cal_m)[:, 1]\n    te_prob_m  = clf_m.predict_proba(X_te_m)[:, 1]\n\n    # ── LEVEL 2: Supervisor ───────────────────────────────────────────────────\n    X_tr_stack  = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_cal_stack = np.column_stack([cal_prob_a, cal_prob_m, X_cal_m])\n    X_te_stack  = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n\n    supervisor  = build_supervisor(n_pos, n_neg)\n    supervisor.fit(X_tr_stack, y_tr)\n\n    # Platt scaling calibration on the held-out calibration fold\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_cal_stack, y_cal)\n\n    te_prob_stack       = cal_supervisor.predict_proba(X_te_stack)[:, 1]\n    oof_stack[te_idx]   = te_prob_stack\n\n    fold_auc  = roc_auc_score(y_te, te_prob_stack)\n    fold_pauc = partial_auc(y_te, te_prob_stack, low_tpr=0.90)\n    print(f\"[*] Fold {fold_i+1} | AUC={fold_auc:.4f} | pAUC@90%={fold_pauc:.4f}\")\n\n# ── 11. FINAL REPORTING ──────────────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*90)\nprint(\"5. FINAL EVALUATION (V10 — PARTICIPANT-LEVEL)\")\nprint(\"=\"*90)\n\nparticipant_df[\"pred_stack\"] = oof_stack\n\nm_part = full_eval(participant_df[\"label\"], participant_df[\"pred_stack\"])\n\nplot_curves(participant_df[\"label\"], participant_df[\"pred_stack\"],\n            f\"{FUSION_OUT}/plots/v10_participant\", \"V10 Participant-Level\")\n\ndef make_row(name, pm):\n    tt = pm.get(\"tuned_thresholds\", {})\n    return {\n        \"Model\":        name,\n        \"ROC-AUC\":      f\"{pm.get('roc_auc', 0):.4f}\",\n        \"pAUC@85%\":     f\"{pm.get('pauc_85', 0):.4f}\",\n        \"pAUC@90%\":     f\"{pm.get('pauc_90', 0):.4f}\",\n        \"Spec@Sens=85%\": f\"{tt.get('sens_85',{}).get('specificity',0):.4f}\",\n        \"Spec@Sens=90%\": f\"{tt.get('sens_90',{}).get('specificity',0):.4f}\",\n        \"Spec@Sens=95%\": f\"{tt.get('sens_95',{}).get('specificity',0):.4f}\",\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V10 (Multi-Window + Participant-Level)\", m_part)])\n\nprint(summary_df.to_string(index=False))\nprint()\n\n# Threshold summary\nfor sk, sv in m_part.get(\"tuned_thresholds\", {}).items():\n    print(f\"  [{sk}] threshold={sv['threshold']:.4f}  \"\n          f\"sens={sv['sensitivity']:.3f}  spec={sv['specificity']:.3f}  \"\n          f\"f1={sv['f1']:.3f}\")\n\n# Save results\nsummary_df.to_csv(f\"{FUSION_OUT}/v10_summary.csv\", index=False)\nparticipant_df.to_csv(f\"{FUSION_OUT}/v10_oof_predictions.csv\", index=False)\n\nzip_path = \"/kaggle/working/outputs_v10.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root, fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\n\nprint(f\"\\n✅ All V10 Results → {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T03:26:07.582070Z","iopub.execute_input":"2026-02-20T03:26:07.582400Z","iopub.status.idle":"2026-02-20T03:37:14.268632Z","shell.execute_reply.started":"2026-02-20T03:26:07.582373Z","shell.execute_reply":"2026-02-20T03:37:14.267975Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n1. LOADING & HARMONISING DATA\n============================================================\n[!] WARNING: No country column found. Country is a top-3 feature for TB!\n[*] Total valid audio files: 9772\n[*] Total unique participants: 1082\n[*] Num clinical features — numerical: 6, categorical: 10\n\n============================================================\n2. LOADING GOOGLE HeAR MODEL\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a60d231934584d42a87d645646f64166"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"[*] ✓ HeAR loaded successfully\n\n============================================================\n3. EXTRACTING MULTI-WINDOW HeAR EMBEDDINGS\n============================================================\n[*] Pre-fetching all audio embeddings (multi-window)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading audio:   0%|          | 0/9772 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[*] Embedding shape per recording: (2560,)\n[*] Avg windows per recording: 1.0  (max=1, min=1)\n\n[*] Aggregating to participant level...\n[*] Participant-level dataset: 1082 participants, 291 TB+, 791 TB-\n\n============================================================\n4. STARTING V10 TRAINING (PARTICIPANT-LEVEL + MULTI-WINDOW)\n============================================================\n\n--- FOLD 1/5 ---\n[*] Train: 692 (187 TB+)  | Cal: 173 (46 TB+)  | Test: 217 (58 TB+)\n[*]  Generating inner OOF probabilities...\n[*] Fold 1 | AUC=0.8482 | pAUC@90%=6.0930\n\n--- FOLD 2/5 ---\n[*] Train: 692 (185 TB+)  | Cal: 173 (47 TB+)  | Test: 217 (59 TB+)\n[*]  Generating inner OOF probabilities...\n[*] Fold 2 | AUC=0.8497 | pAUC@90%=6.3999\n\n--- FOLD 3/5 ---\n[*] Train: 692 (186 TB+)  | Cal: 174 (47 TB+)  | Test: 216 (58 TB+)\n[*]  Generating inner OOF probabilities...\n[*] Fold 3 | AUC=0.8008 | pAUC@90%=3.9573\n\n--- FOLD 4/5 ---\n[*] Train: 692 (186 TB+)  | Cal: 174 (47 TB+)  | Test: 216 (58 TB+)\n[*]  Generating inner OOF probabilities...\n[*] Fold 4 | AUC=0.7644 | pAUC@90%=3.5388\n\n--- FOLD 5/5 ---\n[*] Train: 692 (186 TB+)  | Cal: 174 (47 TB+)  | Test: 216 (58 TB+)\n[*]  Generating inner OOF probabilities...\n[*] Fold 5 | AUC=0.7346 | pAUC@90%=2.4116\n\n==========================================================================================\n5. FINAL EVALUATION (V10 — PARTICIPANT-LEVEL)\n==========================================================================================\n                                 Model ROC-AUC pAUC@85% pAUC@90% Spec@Sens=85% Spec@Sens=90% Spec@Sens=95%\nV10 (Multi-Window + Participant-Level)  0.7965   3.7788   4.4164        0.6056        0.4627        0.3059\n\n  [sens_85] threshold=0.1736  sens=0.852  spec=0.606  f1=0.583\n  [sens_90] threshold=0.1193  sens=0.900  spec=0.463  f1=0.536\n  [sens_95] threshold=0.0947  sens=0.952  spec=0.306  f1=0.496\n\n✅ All V10 Results → /kaggle/working/outputs_v10.zip\nPIPELINE COMPLETE\n","output_type":"stream"}],"execution_count":14}]}