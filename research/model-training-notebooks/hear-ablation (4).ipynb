{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":14911137,"datasetId":9541114,"databundleVersionId":15776836},{"sourceType":"datasetVersion","sourceId":7869025,"datasetId":4617096,"databundleVersionId":7974418},{"sourceType":"modelInstanceVersion","sourceId":720208,"databundleVersionId":15326991,"modelInstanceId":547778,"modelId":480416}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (LEAK-FREE & PRODUCTION-READY)\n# Audio (HeAR + LogReg) + Metadata (LightGBM)\n# ============================================================================\n\n# ── CELL 1: Imports & Seeds ───────────────────────────────────────────────────\nimport os, sys, json, warnings, random, hashlib, zipfile, shutil\nimport numpy as np\nimport pandas as pd\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nprint(f\"Python    : {sys.version}\")\nprint(f\"sklearn   : {sklearn.__version__}\")\nprint(f\"librosa   : {librosa.__version__}\")\nprint(f\"numpy     : {np.__version__}\")\nprint(f\"pandas    : {pd.__version__}\")\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\n    print(f\"lightgbm  : {lgb.__version__}\")\nexcept ImportError:\n    HAS_LGB = False\n    print(\"lightgbm  : NOT FOUND — using GradientBoostingClassifier\")\n\ntry:\n    import tensorflow as tf\n    tf.random.set_seed(SEED)\n    print(f\"tensorflow: {tf.__version__}\")\nexcept ImportError:\n    print(\"tensorflow: NOT FOUND\")\n\nfrom sklearn.model_selection import StratifiedGroupKFold\n\n# ── CELL 2: Configuration ─────────────────────────────────────────────────────\nBASE   = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA   = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"   \n\n# Metadata files\nCLINICAL_CSV   = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV  = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"   \n\n# ---- Audio ----\nSR          = 16_000\nWIN_SECS    = 2.0\nWIN_SAMPLES = int(SR * WIN_SECS)\nENERGY_THRESH_S = 2.2       \n\n# ---- Training ----\nN_SPLITS     = 5             # Replacing predefined folds with 5 dynamic folds\nTARGET_SENS  = [0.85, 0.90, 0.95]\nMISS_AUG_P   = 0.20          \nCALIBRATE    = True\nLGB_N_ITER   = 500\nLGB_LR       = 0.05\nINNER_VAL_FRAC = 0.20        \n\n# ---- Output ----\nOUT_ROOT  = \"/kaggle/working/outputs\"\nAUDIO_OUT = os.path.join(OUT_ROOT, \"audio_model\")\nMETA_OUT  = os.path.join(OUT_ROOT, \"metadata_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [AUDIO_OUT, META_OUT, CACHE_DIR,\n          f\"{AUDIO_OUT}/plots\", f\"{META_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_embeddings.parquet\")\nprint(f\"Output root: {OUT_ROOT}\")\n\n# ── CELL 3: Build the master cough-level manifest ─────────────────────────────\ndef harmonise_fold_df(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\",\"gold_standard\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try:    return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndef resolve_audio_paths(filenames, audio_dir=AUDIO_BASE):\n    lookup = {}\n    for dirpath, _, fns in os.walk(audio_dir):\n        for fn in fns:\n            if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n                full = os.path.join(dirpath, fn)\n                lookup[fn] = full\n                lookup[os.path.splitext(fn)[0]] = full\n    def _resolve(fn):\n        if pd.isna(fn): return np.nan\n        fn = str(fn)\n        if fn in lookup: return lookup[fn]\n        stem = os.path.splitext(fn)[0]\n        if stem in lookup: return lookup[stem]\n        if os.path.isfile(fn): return fn\n        return np.nan\n    return filenames.apply(_resolve), lookup\n\nprint(\"Loading raw solicited data manifest …\")\nraw_audio_df = pd.read_csv(SOLICITED_CSV)\nraw_audio_df = harmonise_fold_df(raw_audio_df)\n\n# If label isn't in audio manifest, we will merge it from clinical\nif \"label_raw\" not in raw_audio_df.columns:\n    print(\"Label not in audio manifest, will extract from clinical data.\")\n\n# ── CELL 4: Join clinical metadata ────────────────────────────────────────────\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"drug_\",\n                 \"microscopy\",\"molecular\",\"confirmatory\",\"reference_test\",\n                 \"gold_standard\",\"diagnosis\",\"tb_status\",\"tb_result\",\n                 \"label\",\"label_raw\",\"_fold\",\"_split\",\"filename\",\"audio_path\"]\n\nprint(\"\\nLoading clinical metadata …\")\nclinical_df = pd.read_csv(CLINICAL_CSV)\nclinical_df = harmonise_fold_df(clinical_df)\n\n# Ensure raw_audio_df gets labels if missing\nif \"label_raw\" not in raw_audio_df.columns and \"label_raw\" in clinical_df.columns:\n    raw_audio_df = raw_audio_df.merge(clinical_df[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\nraw_audio_df[\"label\"] = binarise_label(raw_audio_df[\"label_raw\"])\nraw_audio_df = raw_audio_df.dropna(subset=[\"label\"]).reset_index(drop=True)\nraw_audio_df[\"label\"] = raw_audio_df[\"label\"].astype(int)\n\ndef get_meta_cols(df):\n    skip = set(POST_DIAG_KW) | {\"participant_id\"}\n    num_cols, cat_cols = [], []\n    for c in df.columns:\n        if any(kw in c.lower() for kw in POST_DIAG_KW): continue\n        if c in skip: continue\n        if df[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n        else: cat_cols.append(c)\n    return num_cols, cat_cols\n\nclinical_num, clinical_cat = get_meta_cols(clinical_df)\n\ncough_df = raw_audio_df.merge(\n    clinical_df[[\"participant_id\"] + clinical_num + clinical_cat],\n    on=\"participant_id\", how=\"left\"\n)\n\nprint(\"\\nResolving audio file paths …\")\ncough_df[\"audio_path\"], audio_lookup = resolve_audio_paths(cough_df[\"filename\"])\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── CELL 5: Sanity assertions ─────────────────────────────────────────────────\nprint(\"\\n── Sanity checks ──\")\nn_pos = cough_df[\"label\"].sum()\nn_neg = (cough_df[\"label\"] == 0).sum()\nprev = n_pos / len(cough_df)\n\nprint(f\"  ✓ Total valid cough rows : {len(cough_df)}\")\nprint(f\"  ✓ Participants         : {cough_df['participant_id'].nunique()}\")\nprint(f\"  ✓ TB+ coughs           : {n_pos} ({100*prev:.1f}%)\")\nprint(f\"  ✓ TB- coughs           : {n_neg}\")\n\n# ── CELL 6: Fold assignment (LEAK-FREE DYNAMIC SPLIT) ─────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds (Leak-Free) ...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    y_te = cough_df.loc[te_idx, \"label\"].values\n    n_tr_parts = cough_df.loc[tr_idx, \"participant_id\"].nunique()\n    n_te_parts = cough_df.loc[te_idx, \"participant_id\"].nunique()\n    print(f\"  Fold {fold_i}: train={len(tr_idx)} rows/{n_tr_parts} subjects  \"\n          f\"test={len(te_idx)} rows/{n_te_parts} subjects  \"\n          f\"TB+_test={int(y_te.sum())}/{len(y_te)}\")\n\n# ── CELL 7: Audio loading & window selection ──────────────────────────────────\ndef compute_audio_quality(audio, sr=SR):\n    duration    = len(audio) / sr\n    clip_ratio  = float(np.mean(np.abs(audio) > 0.99))\n    frame_len   = 400; hop = 160\n    frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n    rms    = np.sqrt(np.mean(frames**2, axis=0)) + 1e-9\n    snr_db = float(20 * np.log10(rms.max() / rms.min()))\n    return {\"duration_s\": round(duration, 3), \"clip_ratio\": round(clip_ratio, 4), \"snr_proxy_db\": round(snr_db, 2)}\n\ndef select_best_window(audio, sr=SR):\n    \"\"\"Deterministically select the highest-energy 2-second segment.\"\"\"\n    n = len(audio)\n    if n == 0:\n        return np.zeros(WIN_SAMPLES, np.float32)\n    peak = np.max(np.abs(audio))\n    if peak > 0: audio = audio / peak\n\n    if n / sr <= ENERGY_THRESH_S:\n        if n < WIN_SAMPLES: audio = np.pad(audio, (0, WIN_SAMPLES - n))\n        return audio[:WIN_SAMPLES].astype(np.float32)\n\n    frame_len = 400; hop = 160\n    frames     = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n    rms        = np.sqrt(np.mean(frames**2, axis=0))\n    smooth_n   = max(1, int(0.2 * sr / hop))\n    rms_smooth = np.convolve(rms, np.ones(smooth_n)/smooth_n, mode=\"same\")\n    peak_fr    = int(np.argmax(rms_smooth))\n    center     = peak_fr * hop + frame_len // 2\n    start      = max(0, center - WIN_SAMPLES // 2)\n    end        = start + WIN_SAMPLES\n    if end > n:\n        end = n; start = max(0, n - WIN_SAMPLES)\n    seg = audio[start:end]\n    if len(seg) < WIN_SAMPLES: seg = np.pad(seg, (0, WIN_SAMPLES - len(seg)))\n    return seg[:WIN_SAMPLES].astype(np.float32)\n\ndef load_and_select(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        qual = compute_audio_quality(audio)\n        seg  = select_best_window(audio)\n        return seg, qual\n    except Exception as e:\n        return None, None\n\n# ── CELL 8: HeAR model + disk cache ──────────────────────────────────────────\nprint(\"\\nLoading HeAR model …\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL   = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    EMBED_DIM    = 512\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"  ⚠ HeAR load failed: {e}\")\n    HEAR_MODEL = HEAR_SERVING = None\n    EMBED_DIM  = 512\n\ndef _path_key(path):\n    return hashlib.md5(f\"{HEAR_VERSION}::{path}\".encode()).hexdigest()\n\ndef _load_cache():\n    if os.path.isfile(EMBED_CACHE):\n        try:\n            df = pd.read_parquet(EMBED_CACHE)\n            return df.set_index(\"key\") if \"key\" in df.columns else df\n        except: pass\n    return pd.DataFrame(columns=[\"key\",\"embedding\"]).set_index(\"key\")\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef get_embeddings(df_rows, batch_size=64, desc=\"\"):\n    from tqdm.auto import tqdm\n    cache = _load_cache()\n    N = len(df_rows)\n    embeddings  = np.zeros((N, EMBED_DIM), np.float32)\n    \n    keys = [_path_key(str(r.audio_path)) if pd.notna(r.audio_path) else None for _, r in df_rows.iterrows()]\n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] is not None and keys[i] not in cache.index]\n\n    buf_segs, buf_keys = [], []\n    def flush():\n        if not buf_segs: return\n        embs = _infer_batch(buf_segs)\n        new_rows = [{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)]\n        nonlocal cache\n        cache = pd.concat([cache, pd.DataFrame(new_rows).set_index(\"key\")])\n        buf_segs.clear(); buf_keys.clear()\n\n    for i, row in tqdm(need, desc=f\"HeAR [{desc}]\", leave=False):\n        seg, _ = load_and_select(row.audio_path)\n        if seg is not None:\n            buf_segs.append(seg); buf_keys.append(keys[i])\n        if len(buf_segs) >= batch_size: flush()\n    flush()\n    \n    cache.reset_index().to_parquet(EMBED_CACHE, index=False)\n    for i, (_, row) in enumerate(df_rows.iterrows()):\n        k = keys[i]\n        if k in cache.index:\n            val = cache.loc[k, \"embedding\"]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── CELL 9: Metadata preprocessing ───────────────────────────────────────────\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nMETA_COLS_NUM = clinical_num\nMETA_COLS_CAT = clinical_cat\nALL_META_COLS = META_COLS_NUM + META_COLS_CAT\n\nclass MissingnessAugmenter(BaseEstimator, TransformerMixin):\n    def __init__(self, p=MISS_AUG_P, seed=SEED):\n        self.p = p; self.seed = seed\n    def fit(self, X, y=None): return self\n    def fit_transform(self, X, y=None, **kw):\n        rng   = np.random.RandomState(self.seed)\n        X_out = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n        mask  = rng.random(X_out.shape) < self.p\n        X_out[mask] = np.nan\n        return X_out\n    def transform(self, X): return X\n\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")), \n                                              (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef preprocess_meta_fold(df_train, df_val, df_test, num_cols, cat_cols):\n    all_cols = num_cols + cat_cols\n    def add_miss_indicators(df):\n        d = df[all_cols].copy()\n        for c in all_cols: d[f\"__miss_{c}\"] = d[c].isna().astype(np.float32)\n        return d\n        \n    X_tr_raw  = add_miss_indicators(df_train)\n    X_val_raw = add_miss_indicators(df_val)\n    X_te_raw  = add_miss_indicators(df_test)\n    ind_cols = [f\"__miss_{c}\" for c in all_cols]\n\n    aug = MissingnessAugmenter(p=MISS_AUG_P, seed=SEED)\n    X_tr_feat = aug.fit_transform(df_train[all_cols])\n    \n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    prep.fit(X_tr_feat)\n\n    def transform_and_stack(feat_df, ind_df):\n        transformed = prep.transform(feat_df)\n        indicators  = ind_df[ind_cols].values.astype(np.float32)\n        return np.hstack([transformed, indicators])\n\n    return transform_and_stack(X_tr_feat, X_tr_raw), transform_and_stack(df_val[all_cols], X_val_raw), transform_and_stack(df_test[all_cols], X_te_raw), prep\n\n# ── CELL 10: Model builders ───────────────────────────────────────────────────\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\n\ndef build_audio_clf():\n    return Pipeline([\n        (\"sc\",  StandardScaler()),\n        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=5000, C=1.0, solver=\"lbfgs\", random_state=SEED)),\n    ])\n\ndef build_meta_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=LGB_N_ITER, learning_rate=LGB_LR,\n            num_leaves=15, max_depth=4,         # Prevent Overfitting\n            subsample=0.8, colsample_bytree=0.8,\n            min_child_samples=10,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1,\n        )\n    from sklearn.ensemble import GradientBoostingClassifier\n    return GradientBoostingClassifier(n_estimators=200, learning_rate=LGB_LR, max_depth=4, subsample=0.8, random_state=SEED)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\") # Changed to sigmoid for stability on small sets\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── CELL 11 & 12: Evaluation & Plotting ───────────────────────────────────────\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score, f1_score, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve)\nfrom scipy.stats import spearmanr\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom sklearn.calibration import calibration_curve\n\nC_POS, C_NEG = \"#e63946\", \"#457b9d\"\n\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)),\n            \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9),\n            \"precision\": tp/(tp+fp+1e-9), \"npv\": tn/(tn+fn+1e-9), \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob, val_true=None, val_prob=None):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {}\n    m[\"roc_auc\"] = float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"pr_auc\"]  = float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"brier\"]   = float(brier_score_loss(y_true, y_prob))\n    m[\"spearman_rho\"]= float(spearmanr(y_prob, y_true).statistic)\n    m.update(metrics_at_thresh(y_true, y_prob, 0.5))\n    \n    tune_t = val_true if val_true is not None else y_true\n    tune_p = val_prob if val_prob is not None else y_prob\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(tune_t, tune_p, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef _save(fig, path):\n    fig.tight_layout(); fig.savefig(path, dpi=150); plt.close(fig)\n\ndef save_all_plots(y_true, y_prob, plot_dir, prefix, best_t=0.5):\n    if len(np.unique(y_true)) < 2: return\n    y_true=np.array(y_true); y_prob=np.array(y_prob)\n    \n    # ROC\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=C_POS, lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{prefix} ROC\"); ax.legend(); _save(fig, f\"{plot_dir}/{prefix}_roc.png\")\n    \n    # PR\n    p, r, _ = precision_recall_curve(y_true, y_prob); ap = average_precision_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(r, p, color=C_POS, lw=2, label=f\"AP={ap:.3f}\")\n    ax.set(title=f\"{prefix} PR\"); ax.legend(); _save(fig, f\"{plot_dir}/{prefix}_pr.png\")\n\n# ── CELL 13: Inner-fold val split helper ─────────────────────────────────────\ndef inner_val_split(df_sub, val_frac=INNER_VAL_FRAC):\n    n_splits = max(2, int(round(1/val_frac)))\n    sgkf = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    splits = list(sgkf.split(df_sub, df_sub[\"label\"], df_sub[\"participant_id\"]))\n    tr_idx, val_idx = splits[0]\n    return df_sub.iloc[tr_idx].reset_index(drop=True), df_sub.iloc[val_idx].reset_index(drop=True)\n\n# ── CELL 14 & 15: AUDIO MODEL CV & OOF ────────────────────────────────────────\nprint(\"\\n\" + \"=\"*70)\nprint(\"AUDIO MODEL  —  HeAR + LogisticRegression\")\nprint(\"=\"*70)\n\naudio_oof_rows, fold_metrics_aud = [], []\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n── Audio Fold {fold_i+1} ──\")\n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    df_tr, df_val = inner_val_split(df_tr_full)\n\n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n\n    X_tr_emb  = get_embeddings(df_tr,  desc=f\"F{fold_i}_tr\")\n    X_val_emb = get_embeddings(df_val, desc=f\"F{fold_i}_val\")\n    X_te_emb  = get_embeddings(df_te,  desc=f\"F{fold_i}_te\")\n\n    clf_a = build_audio_clf().fit(X_tr_emb, y_tr)\n\n    if CALIBRATE and len(np.unique(y_val)) >= 2:\n        inner_lr = clf_a.named_steps[\"clf\"]\n        cal_a    = calibrate(inner_lr, clf_a.named_steps[\"sc\"].transform(X_val_emb), y_val)\n        _proba_a = lambda X: cal_a.predict_proba(clf_a.named_steps[\"sc\"].transform(X))[:,1]\n    else:\n        cal_a = None\n        _proba_a = lambda X: clf_a.predict_proba(X)[:,1]\n\n    val_prob, te_prob = _proba_a(X_val_emb), _proba_a(X_te_emb)\n    \n    fm = full_eval(y_te, te_prob, val_true=y_val, val_prob=val_prob)\n    fold_metrics_aud.append(fm)\n    print(f\"  AUC={fm['roc_auc']:.3f}  PR-AUC={fm['pr_auc']:.3f}\")\n\n    for pid, p, lbl in zip(df_te[\"participant_id\"], te_prob, y_te):\n        audio_oof_rows.append({\"participant_id\":pid,\"fold\":fold_i,\"prob\":p,\"label\":lbl})\n\naud_oof = pd.DataFrame(audio_oof_rows)\nyt_a, yp_a = aud_oof[\"label\"].values, aud_oof[\"prob\"].values\nm_aud_cough = full_eval(yt_a, yp_a)\npart_a = aud_oof.groupby(\"participant_id\").agg(prob=(\"prob\",\"max\"), label=(\"label\",\"first\")).reset_index()\nm_aud_part = full_eval(part_a[\"label\"].values, part_a[\"prob\"].values)\nsave_all_plots(yt_a, yp_a, f\"{AUDIO_OUT}/plots\", \"audio\", best_t=find_thresh_for_sens(yt_a, yp_a, 0.90))\n\n# ── CELL 16 & 17: METADATA MODEL CV & OOF ──────────────────────────────────────\nprint(\"\\n\" + \"=\"*70)\nprint(\"METADATA MODEL  —  LightGBM + missingness augmentation\")\nprint(\"=\"*70)\n\nmeta_oof_rows, fold_metrics_meta = [], []\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n── Meta Fold {fold_i+1} ──\")\n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    df_tr, df_val = inner_val_split(df_tr_full)\n\n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n\n    X_tr, X_val_m, X_te_m, prep = preprocess_meta_fold(df_tr, df_val, df_te, META_COLS_NUM, META_COLS_CAT)\n\n    clf_m = build_meta_clf(int(y_tr.sum()), int((y_tr==0).sum()))\n    if HAS_LGB and len(np.unique(y_val)) >= 2:\n        clf_m.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr),(X_val_m, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n    else:\n        clf_m.fit(X_tr, y_tr)\n\n    if CALIBRATE and len(np.unique(y_val)) >= 2:\n        cal_m    = calibrate(clf_m, X_val_m, y_val)\n        val_prob = cal_m.predict_proba(X_val_m)[:,1]\n        te_prob  = cal_m.predict_proba(X_te_m)[:,1]\n    else:\n        val_prob = clf_m.predict_proba(X_val_m)[:,1]\n        te_prob  = clf_m.predict_proba(X_te_m)[:,1]\n\n    fm = full_eval(y_te, te_prob, val_true=y_val, val_prob=val_prob)\n    fold_metrics_meta.append(fm)\n    print(f\"  AUC={fm['roc_auc']:.3f}  PR-AUC={fm['pr_auc']:.3f}\")\n\n    for pid, p, lbl in zip(df_te[\"participant_id\"], te_prob, y_te):\n        meta_oof_rows.append({\"participant_id\":pid,\"fold\":fold_i,\"prob\":p,\"label\":lbl})\n\nmeta_oof = pd.DataFrame(meta_oof_rows)\nyt_m, yp_m = meta_oof[\"label\"].values, meta_oof[\"prob\"].values\nm_meta_cough = full_eval(yt_m, yp_m)\npart_m = meta_oof.groupby(\"participant_id\").agg(prob=(\"prob\",\"max\"), label=(\"label\",\"first\")).reset_index()\nm_meta_part = full_eval(part_m[\"label\"].values, part_m[\"prob\"].values)\nsave_all_plots(yt_m, yp_m, f\"{META_OUT}/plots\", \"meta\", best_t=find_thresh_for_sens(yt_m, yp_m, 0.90))\n\n# ── CELL 18-20: Reporting & Zipping ──────────────────────────────────────────\nif m_aud_cough and m_meta_cough:\n    fig, ax = plt.subplots(figsize=(6,5))\n    fpr, tpr, _ = roc_curve(yt_a, yp_a); ax.plot(fpr, tpr, lw=2, color=C_POS, label=f\"Audio AUC={roc_auc_score(yt_a,yp_a):.3f}\")\n    fpr, tpr, _ = roc_curve(yt_m, yp_m); ax.plot(fpr, tpr, lw=2, color=C_NEG, label=f\"Meta AUC={roc_auc_score(yt_m,yp_m):.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=\"ROC Comparison\"); ax.legend(); _save(fig, f\"{OUT_ROOT}/roc_comparison.png\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"Audio (HeAR+LR)\", m_aud_cough, m_aud_part), \n                           make_row(\"Metadata (LightGBM)\", m_meta_cough, m_meta_part)])\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"REPORT-READY SUMMARY (LEAK-FREE)\")\nprint(\"=\"*100)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 2 - SCIENTIFIC BEST PRACTICE)\n# Leak-Free CV + Mean-Pooled HeAR + MNAR-Aware LightGBM + Ensemble\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve)\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V2)\nOUT_ROOT = \"/kaggle/working/outputs_v2\"\nAUDIO_OUT = os.path.join(OUT_ROOT, \"audio_model\")\nMETA_OUT  = os.path.join(OUT_ROOT, \"metadata_model\")\nENS_OUT   = os.path.join(OUT_ROOT, \"ensemble_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [AUDIO_OUT, META_OUT, ENS_OUT, CACHE_DIR, \n          f\"{AUDIO_OUT}/plots\", f\"{META_OUT}/plots\", f\"{ENS_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\n# Identify safe metadata columns (excluding post-diag leaks)\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\n# Map physical audio files\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"Total valid audio files: {len(cough_df)} | Participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD (LEAK-FREE SPLITS) ───────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\nfor i, (tr, te) in enumerate(folds):\n    tr_p = set(cough_df.loc[tr, \"participant_id\"])\n    te_p = set(cough_df.loc[te, \"participant_id\"])\n    assert len(tr_p & te_p) == 0, f\"LEAK DETECTED IN FOLD {i}!\"\n\n# ── 4. AUDIO FEATURE EXTRACTION (MEAN-POOLING) ──────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    \"\"\"Slices entire audio into 2-second chunks for mean pooling.\"\"\"\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        \n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        \n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n            \n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    \"\"\"Safe caching without Pandas index type confusion.\"\"\"\n    if os.path.exists(EMBED_CACHE):\n        try:\n            cache = pd.read_parquet(EMBED_CACHE)\n        except:\n            cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else:\n        cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    \n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio (Mean Pooled)\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)  # FORCE pyarrow string type\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    # Dictionary lookup for maximum speed and safety\n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n        \n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            \n    return embeddings\n\n# ── 5. MNAR-AWARE PREPROCESSING & MODEL BUILDERS ────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), \n            (\"sc\", StandardScaler())\n        ]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), \n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n        ]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_audio_clf():\n    return Pipeline([\n        (\"sc\", StandardScaler()),\n        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000, C=0.1, random_state=SEED))\n    ])\n\ndef build_meta_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=300, learning_rate=0.03,\n            num_leaves=15, max_depth=4,         \n            subsample=0.8, colsample_bytree=0.8,\n            min_child_samples=15,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression()\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)),\n            \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9),\n            \"precision\": tp/(tp+fp+1e-9), \"npv\": tn/(tn+fn+1e-9), \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {}\n    m[\"roc_auc\"] = float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"pr_auc\"]  = float(average_precision_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V2 CV Pipeline (Audio + Meta + Ensemble)...\")\noof_aud, oof_meta, oof_ens = np.zeros(len(cough_df)), np.zeros(len(cough_df)), np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # ── AUDIO PATH ──\n    X_tr_emb  = get_mean_embeddings(df_tr)\n    X_val_emb = get_mean_embeddings(df_val)\n    X_te_emb  = get_mean_embeddings(df_te)\n    \n    clf_a = build_audio_clf().fit(X_tr_emb, y_tr)\n    cal_a = calibrate(clf_a.named_steps[\"clf\"], clf_a.named_steps[\"sc\"].transform(X_val_emb), y_val)\n    \n    te_prob_a = cal_a.predict_proba(clf_a.named_steps[\"sc\"].transform(X_te_emb))[:,1]\n    oof_aud[te_idx] = te_prob_a\n    \n    # ── META PATH ──\n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = prep.fit_transform(df_tr)\n    X_val_m = prep.transform(df_val)\n    X_te_m  = prep.transform(df_te)\n    \n    clf_m = build_meta_clf(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    cal_m = calibrate(clf_m, X_val_m, y_val)\n    \n    te_prob_m = cal_m.predict_proba(X_te_m)[:,1]\n    oof_meta[te_idx] = te_prob_m\n\n    # ── ENSEMBLE PATH ──\n    te_prob_ens = (te_prob_a + te_prob_m) / 2.0\n    oof_ens[te_idx] = te_prob_ens\n    \n    print(f\" Fold {fold_i+1} ROC-AUC | Audio: {roc_auc_score(y_te, te_prob_a):.3f} | Meta: {roc_auc_score(y_te, te_prob_m):.3f} | Ens: {roc_auc_score(y_te, te_prob_ens):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_aud\"] = oof_aud\ncough_df[\"pred_meta\"] = oof_meta\ncough_df[\"pred_ens\"] = oof_ens\n\n# Participant-Level Aggregation\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"),\n    prob_aud=(\"pred_aud\", \"max\"),\n    prob_meta=(\"pred_meta\", \"max\"),\n    prob_ens=(\"pred_ens\", \"max\")\n).reset_index()\n\nm_aud  = full_eval(cough_df['label'], oof_aud)\nm_meta = full_eval(cough_df['label'], oof_meta)\nm_ens  = full_eval(cough_df['label'], oof_ens)\n\np_aud  = full_eval(part_df['label'], part_df['prob_aud'])\np_meta = full_eval(part_df['label'], part_df['prob_meta'])\np_ens  = full_eval(part_df['label'], part_df['prob_ens'])\n\n# Plotting\nplot_curves(cough_df['label'], oof_aud, f\"{AUDIO_OUT}/plots/audio\", \"Audio\")\nplot_curves(cough_df['label'], oof_meta, f\"{META_OUT}/plots/meta\", \"Metadata\")\nplot_curves(cough_df['label'], oof_ens, f\"{ENS_OUT}/plots/ensemble\", \"Ensemble\")\n\n# Generate Summary Table\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([\n    make_row(\"Audio (HeAR Mean-Pool)\", m_aud, p_aud), \n    make_row(\"Metadata (MNAR LightGBM)\", m_meta, p_meta),\n    make_row(\"Fusion Ensemble (V2)\", m_ens, p_ens)\n])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REPORT-READY SUMMARY (VERSION 2 - BEST PRACTICES)\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False))\n\n# Zipping Outputs\nzip_path = \"/kaggle/working/outputs_v2.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V2 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 3 - THE DeepGB-TB ADAPTATION)\n# Leak-Free CV + Early Fusion + Cross-Modal LightGBM Feature Bottlenecking\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve)\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V3)\nOUT_ROOT = \"/kaggle/working/outputs_v3\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"early_fusion_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. STRATIFIED GROUP K-FOLD (LEAK-FREE SPLITS) ───────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION ─────────────────────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── 5. MNAR PREPROCESSING & FUSION MODEL BUILDER ────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_early_fusion_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=400, learning_rate=0.02,\n            num_leaves=31, max_depth=5,         \n            subsample=0.8, \n            colsample_bytree=0.15, # <--- THE DeepGB-TB SECRET: Forces mixing of Audio & Meta\n            min_child_samples=15,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V3 CV Pipeline (Early Fusion Cross-Attention Model)...\")\noof_fusion = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # Extract Embeddings (Audio)\n    X_tr_emb  = get_mean_embeddings(df_tr)\n    X_val_emb = get_mean_embeddings(df_val)\n    X_te_emb  = get_mean_embeddings(df_te)\n    \n    # Preprocess Metadata (Tabular)\n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = prep.fit_transform(df_tr)\n    X_val_m = prep.transform(df_val)\n    X_te_m  = prep.transform(df_te)\n    \n    # --- EARLY FUSION: Combine Audio & Metadata into one massive array ---\n    X_tr_fusion  = np.hstack([X_tr_emb, X_tr_m])\n    X_val_fusion = np.hstack([X_val_emb, X_val_m])\n    X_te_fusion  = np.hstack([X_te_emb, X_te_m])\n    \n    # Train Cross-Modal LightGBM\n    clf_fusion = build_early_fusion_clf(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_fusion, y_tr)\n    cal_fusion = calibrate(clf_fusion, X_val_fusion, y_val)\n    \n    te_prob_fusion = cal_fusion.predict_proba(X_te_fusion)[:,1]\n    oof_fusion[te_idx] = te_prob_fusion\n    \n    print(f\" Fold {fold_i+1} Early Fusion ROC-AUC: {roc_auc_score(y_te, te_prob_fusion):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_fusion\"] = oof_fusion\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_fusion=(\"pred_fusion\", \"max\")\n).reset_index()\n\nm_fusion = full_eval(cough_df['label'], oof_fusion)\np_fusion = full_eval(part_df['label'], part_df['prob_fusion'])\n\nplot_curves(cough_df['label'], oof_fusion, f\"{FUSION_OUT}/plots/early_fusion\", \"Early Fusion\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"Early Fusion (V3 - DeepGB-TB Inspired)\", m_fusion, p_fusion)])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REPORT-READY SUMMARY (VERSION 3 - EARLY FUSION)\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v3.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V3 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 4 - THE ACOUSTIC BOTTLENECK)\n# Early Fusion + PCA Dimensionality Reduction + LightGBM\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\nPCA_COMPONENTS = 32  # THE BOTTLENECK\n\n# Output Directories (V4)\nOUT_ROOT = \"/kaggle/working/outputs_v4\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"pca_fusion_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION ─────────────────────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── 5. PREPROCESSING & BOTTLENECK BUILDERS ──────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# Feature Compression Pipeline\ndef build_audio_pca_preprocessor():\n    return Pipeline([\n        (\"sc\", StandardScaler()),\n        (\"pca\", PCA(n_components=PCA_COMPONENTS, random_state=SEED))\n    ])\n\ndef build_early_fusion_clf(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=500, learning_rate=0.015,  # Slowed down learning rate for stability\n            num_leaves=31, max_depth=5,         \n            subsample=0.8, \n            colsample_bytree=0.6,  # We can increase this now because dimensions are balanced!\n            min_child_samples=15,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V4 CV Pipeline (PCA Bottleneck + Early Fusion)...\")\noof_fusion = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # 1. Extract Raw Embeddings\n    X_tr_emb_raw  = get_mean_embeddings(df_tr)\n    X_val_emb_raw = get_mean_embeddings(df_val)\n    X_te_emb_raw  = get_mean_embeddings(df_te)\n    \n    # 2. Apply PCA Bottleneck (Reduces 512-dim to 32-dim)\n    pca_prep = build_audio_pca_preprocessor()\n    X_tr_emb  = pca_prep.fit_transform(X_tr_emb_raw)\n    X_val_emb = pca_prep.transform(X_val_emb_raw)\n    X_te_emb  = pca_prep.transform(X_te_emb_raw)\n    \n    # 3. Preprocess Metadata (Tabular)\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m  = meta_prep.transform(df_te)\n    \n    # 4. EARLY FUSION: Combine Compressed Audio (32) & Metadata (~15)\n    X_tr_fusion  = np.hstack([X_tr_emb, X_tr_m])\n    X_val_fusion = np.hstack([X_val_emb, X_val_m])\n    X_te_fusion  = np.hstack([X_te_emb, X_te_m])\n    \n    # 5. Train Cross-Modal LightGBM\n    clf_fusion = build_early_fusion_clf(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_fusion, y_tr)\n    cal_fusion = calibrate(clf_fusion, X_val_fusion, y_val)\n    \n    te_prob_fusion = cal_fusion.predict_proba(X_te_fusion)[:,1]\n    oof_fusion[te_idx] = te_prob_fusion\n    \n    print(f\" Fold {fold_i+1} PCA-Fusion ROC-AUC: {roc_auc_score(y_te, te_prob_fusion):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_fusion\"] = oof_fusion\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_fusion=(\"pred_fusion\", \"max\")\n).reset_index()\n\nm_fusion = full_eval(cough_df['label'], oof_fusion)\np_fusion = full_eval(part_df['label'], part_df['prob_fusion'])\n\nplot_curves(cough_df['label'], oof_fusion, f\"{FUSION_OUT}/plots/pca_fusion\", \"PCA Early Fusion\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"PCA-Bottlenecked Fusion (V4)\", m_fusion, p_fusion)])\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"REPORT-READY SUMMARY (VERSION 4 - PCA BOTTLENECK)\")\nprint(\"=\"*80)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v4.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V4 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 5 - STATE OF THE ART)\n# Asymmetric Blending (Stacking) + SMOTE + Clinical Attention Mechanism\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\ntry:\n    from imblearn.over_sampling import SMOTE\n    HAS_SMOTE = True\n    print(\"✓ imbalanced-learn (SMOTE) found.\")\nexcept ImportError:\n    HAS_SMOTE = False\n    print(\"⚠ imbalanced-learn not found. SMOTE disabled.\")\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SECS = 2.0\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V5)\nOUT_ROOT = \"/kaggle/working/outputs_v5\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"sota_stacking_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mean_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\nprint(\"Loading data...\")\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\nBuilding Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION ─────────────────────────────────────────────\nprint(\"\\nLoading HeAR Model...\")\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"✓ HeAR loaded\")\nexcept Exception as e:\n    print(f\"⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_and_chunk(path):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        win_samples = int(SR * WIN_SECS)\n        n = len(audio)\n        if n == 0: return [np.zeros(win_samples, np.float32)]\n        peak = np.max(np.abs(audio))\n        if peak > 0: audio = audio / peak\n        if n <= win_samples:\n            return [np.pad(audio, (0, win_samples - n)).astype(np.float32)]\n        chunks = []\n        for start in range(0, n, win_samples):\n            seg = audio[start : start + win_samples]\n            if len(seg) < win_samples:\n                seg = np.pad(seg, (0, win_samples - len(seg)))\n            chunks.append(seg.astype(np.float32))\n        return chunks\n    except: return []\n\ndef get_mean_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio\", leave=False):\n        chunks = load_and_chunk(row.audio_path)\n        if chunks:\n            embs = _infer_batch(chunks)\n            new_entries.append({\"key\": keys[i], \"embedding\": np.mean(embs, axis=0).tolist()})\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n    return embeddings\n\n# ── 5. PREPROCESSING & STACKING BUILDERS ────────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_audio_expert():\n    return Pipeline([\n        (\"sc\", StandardScaler()),\n        (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000, C=0.1, random_state=SEED))\n    ])\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=300, learning_rate=0.03, num_leaves=15, max_depth=4,         \n            subsample=0.8, colsample_bytree=0.8, min_child_samples=15,\n            scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\")\n\ndef build_supervisor_clf():\n    # The Supervisor must be shallow to prevent overfitting the Expert's predictions\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100, learning_rate=0.05,\n            num_leaves=7, max_depth=3,  \n            min_child_samples=10,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", C=0.1)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\nStarting V5 CV Pipeline (Asymmetric Stacking + SMOTE)...\")\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    # SPLIT: 70% for Level-1 Experts, 30% for Level-2 Supervisor\n    l1_split = int(len(df_tr_full) * 0.7)\n    df_l1 = df_tr_full.iloc[:l1_split]\n    df_l2 = df_tr_full.iloc[l1_split:]\n    \n    y_l1, y_l2, y_te = df_l1[\"label\"].values, df_l2[\"label\"].values, df_te[\"label\"].values\n    \n    # ── LEVEL 1: AUDIO EXPERT ──\n    X_l1_emb = get_mean_embeddings(df_l1)\n    X_l2_emb = get_mean_embeddings(df_l2)\n    X_te_emb = get_mean_embeddings(df_te)\n    \n    clf_aud = build_audio_expert().fit(X_l1_emb, y_l1)\n    \n    prob_l2_aud = clf_aud.predict_proba(X_l2_emb)[:,1]\n    prob_te_aud = clf_aud.predict_proba(X_te_emb)[:,1]\n    \n    # ── LEVEL 1: CLINICAL EXPERT ──\n    prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_l1_m = prep.fit_transform(df_l1)\n    X_l2_m = prep.transform(df_l2)\n    X_te_m = prep.transform(df_te)\n    \n    clf_meta = build_clinical_expert(int(y_l1.sum()), int((y_l1==0).sum())).fit(X_l1_m, y_l1)\n    \n    prob_l2_meta = clf_meta.predict_proba(X_l2_m)[:,1]\n    prob_te_meta = clf_meta.predict_proba(X_te_m)[:,1]\n    \n    # ── LEVEL 2: THE SUPERVISOR ──\n    # Concatenate the Expert probabilities WITH the raw clinical features\n    X_l2_stack = np.column_stack([prob_l2_aud, prob_l2_meta, X_l2_m])\n    X_te_stack = np.column_stack([prob_te_aud, prob_te_meta, X_te_m])\n    \n    # Apply SMOTE to the Supervisor's training data to learn the TB boundary flawlessly\n    if HAS_SMOTE:\n        smote = SMOTE(random_state=SEED)\n        X_l2_resampled, y_l2_resampled = smote.fit_resample(X_l2_stack, y_l2)\n    else:\n        X_l2_resampled, y_l2_resampled = X_l2_stack, y_l2\n        \n    # Train Supervisor & Calibrate internally via CV to ensure perfect probabilities\n    base_supervisor = build_supervisor_clf()\n    cal_supervisor = CalibratedClassifierCV(base_supervisor, cv=3, method=\"sigmoid\")\n    cal_supervisor.fit(X_l2_resampled, y_l2_resampled)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    print(f\" Fold {fold_i+1} Supervisor ROC-AUC: {roc_auc_score(y_te, te_prob_stack):.3f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_stack=(\"pred_stack\", \"max\")\n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"Supervisor Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (cough)\": f\"{cough_m.get('roc_auc', 0):.3f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.3f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.3f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.3f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"Asymmetric Supervisor + SMOTE (V5)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"REPORT-READY SUMMARY (VERSION 5 - SOTA STACKING)\")\nprint(\"=\"*90)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v5.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V5 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 6 - EXACT 2-SECOND SOTA)\n# PCA Bottleneck + Early Fusion + TRBL + Heavy Debugging\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # EXACTLY 2 SECONDS\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\nPCA_COMPONENTS = 32  \n\n# Output Directories (V6)\nOUT_ROOT = \"/kaggle/working/outputs_v6\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"debug_fusion_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_exact2s_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\nprint(f\"[*] Identified Numeric Clinical Features: {num_cols}\")\nprint(f\"[*] Identified Categorical Clinical Features: {cat_cols}\")\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION (EXACTLY 2 SECONDS) ─────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_exact_audio(path):\n    \"\"\"Loads audio and forces EXACTLY 32,000 samples (2 seconds)\"\"\"\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        if len(audio) < WIN_SAMPLES:\n            audio = np.pad(audio, (0, WIN_SAMPLES - len(audio)))\n        else:\n            audio = audio[:WIN_SAMPLES]\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef get_exact_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys = [], []\n    new_entries = []\n    \n    for i, row in tqdm(need, desc=\"Extracting Audio (2-Sec Strict)\", leave=False):\n        seg = load_exact_audio(row.audio_path)\n        buf_segs.append(seg)\n        buf_keys.append(keys[i])\n        \n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            buf_segs, buf_keys = [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            \n    return embeddings\n\n# ── 5. PREPROCESSING & RISK-BALANCED LIGHTGBM BUILDERS ──────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef build_audio_pca_preprocessor():\n    return Pipeline([(\"sc\", StandardScaler()), (\"pca\", PCA(n_components=PCA_COMPONENTS, random_state=SEED))])\n\ndef build_risk_balanced_clf(n_pos, n_neg):\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=400, learning_rate=0.02,\n            num_leaves=31, max_depth=5,         \n            subsample=0.8, colsample_bytree=0.6,\n            min_child_samples=15,\n            scale_pos_weight=trbl_scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef calibrate(clf, X_cal, y_cal):\n    cal = CalibratedClassifierCV(clf, cv=\"prefit\", method=\"sigmoid\")\n    cal.fit(X_cal, y_cal)\n    return cal\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V6 TRAINING (PCA FUSION + TRBL)\")\nprint(\"=\"*60)\noof_fusion = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    print(f\"[*] Fold Balances:\")\n    print(f\"    Train: {len(y_tr)} (TB+: {y_tr.sum()}) | Val: {len(y_val)} (TB+: {y_val.sum()}) | Test: {len(y_te)} (TB+: {y_te.sum()})\")\n    \n    # Extract Strict 2-sec Embeddings\n    X_tr_emb_raw  = get_exact_embeddings(df_tr)\n    X_val_emb_raw = get_exact_embeddings(df_val)\n    X_te_emb_raw  = get_exact_embeddings(df_te)\n    \n    # PCA Bottleneck\n    pca_prep = build_audio_pca_preprocessor()\n    X_tr_emb  = pca_prep.fit_transform(X_tr_emb_raw)\n    X_val_emb = pca_prep.transform(X_val_emb_raw)\n    X_te_emb  = pca_prep.transform(X_te_emb_raw)\n    \n    print(f\"[*] Audio Features reduced from {X_tr_emb_raw.shape[1]} to {X_tr_emb.shape[1]} via PCA\")\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m  = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m  = meta_prep.transform(df_te)\n    \n    print(f\"[*] Clinical Metadata Features generated (MNAR Aware): {X_tr_m.shape[1]}\")\n    \n    # EARLY FUSION\n    X_tr_fusion  = np.hstack([X_tr_emb, X_tr_m])\n    X_val_fusion = np.hstack([X_val_emb, X_val_m])\n    X_te_fusion  = np.hstack([X_te_emb, X_te_m])\n    \n    print(f\"[*] Final Fused Training Matrix Shape: {X_tr_fusion.shape}\")\n    \n    # Train Risk-Balanced LightGBM\n    clf_fusion = build_risk_balanced_clf(int(y_tr.sum()), int((y_tr==0).sum()))\n    clf_fusion.fit(X_tr_fusion, y_tr)\n    cal_fusion = calibrate(clf_fusion, X_val_fusion, y_val)\n    \n    te_prob_fusion = cal_fusion.predict_proba(X_te_fusion)[:,1]\n    oof_fusion[te_idx] = te_prob_fusion\n    \n    fold_auc = roc_auc_score(y_te, te_prob_fusion)\n    print(f\"[*] Fold {fold_i+1} ROC-AUC: {fold_auc:.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_fusion\"] = oof_fusion\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_fusion=(\"pred_fusion\", \"max\")\n).reset_index()\n\nm_fusion = full_eval(cough_df['label'], oof_fusion)\np_fusion = full_eval(part_df['label'], part_df['prob_fusion'])\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V6 (Exact 2s + PCA + TRBL)\", m_fusion, p_fusion)])\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"REPORT-READY SUMMARY (VERSION 6)\")\nprint(\"=\"*85)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v6.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V6 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 7 - THE SOTA LIMIT BREAKER)\n# OOF CVPEM Stacking + Unshackled 512-dim HeAR + TRBL Meta-Learner\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # EXACTLY 2 SECONDS (Based on original notebook constraints)\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V7)\nOUT_ROOT = \"/kaggle/working/outputs_v7\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"oof_stacking_model\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_exact2s_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. AUDIO FEATURE EXTRACTION (EXACTLY 2 SECONDS) ─────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_exact_audio(path):\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        if len(audio) < WIN_SAMPLES:\n            audio = np.pad(audio, (0, WIN_SAMPLES - len(audio)))\n        else:\n            audio = audio[:WIN_SAMPLES]\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef get_exact_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys = [], []\n    new_entries = []\n    for i, row in tqdm(need, desc=\"Extracting Audio (2-Sec Strict)\", leave=False):\n        buf_segs.append(load_exact_audio(row.audio_path))\n        buf_keys.append(keys[i])\n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            buf_segs, buf_keys = [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist()} for k, e in zip(buf_keys, embs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], cache[\"embedding\"]))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            val = cache_dict[k]\n            embeddings[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            \n    return embeddings\n\n# ── 5. PREPROCESSING & OOF STACKING BUILDERS ────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# LEVEL 1 EXPERTS (No PCA, Audio gets full 512 dimensions!)\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, num_leaves=31, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\n# LEVEL 2 SUPERVISOR (With TRBL Loss)\ndef build_supervisor(n_pos, n_neg):\n    # TRBL: Tuberculosis Risk-Balanced Loss (1.5x penalty for False Negatives)\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    return LogisticRegression(class_weight={0: 1.0, 1: trbl_scale}, max_iter=2000, random_state=SEED)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V7 TRAINING (OOF STACKING + UNSHACKLED HeAR)\")\nprint(\"=\"*60)\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    print(f\"[*] Fold Balances:\")\n    print(f\"    Train: {len(y_tr)} (TB+: {y_tr.sum()}) | Val: {len(y_val)} (TB+: {y_val.sum()}) | Test: {len(y_te)} (TB+: {y_te.sum()})\")\n    \n    # Extract Strict 2-sec Embeddings (All 512 dimensions)\n    X_tr_emb = get_exact_embeddings(df_tr)\n    X_val_emb = get_exact_embeddings(df_val)\n    X_te_emb = get_exact_embeddings(df_te)\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m = meta_prep.transform(df_te)\n    \n    # ── LEVEL 1: GENERATE OUT-OF-FOLD (OOF) PROBABILITIES FOR TRAIN ──\n    print(\"[*] Generating Out-Of-Fold Probabilities for Level-1 Experts (100% Data Retained)\")\n    cv_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=SEED)\n    inner_folds = list(cv_inner.split(df_tr, y_tr, df_tr[\"participant_id\"]))\n    \n    tr_oof_a = np.zeros(len(y_tr))\n    tr_oof_m = np.zeros(len(y_tr))\n    \n    for i_tr, i_val in inner_folds:\n        # Inner Audio Expert\n        clf_a_inner = build_audio_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum()))\n        clf_a_inner.fit(X_tr_emb[i_tr], y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_inner.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        # Inner Clinical Expert\n        clf_m_inner = build_clinical_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum()))\n        clf_m_inner.fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_inner.predict_proba(X_tr_m[i_val])[:, 1]\n        \n    # ── LEVEL 1: TRAIN EXPERTS ON FULL TRAIN SET & PREDICT VAL/TEST ──\n    print(\"[*] Training Level-1 Experts on Full Train Set\")\n    clf_a = build_audio_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_emb, y_tr)\n    val_prob_a = clf_a.predict_proba(X_val_emb)[:,1]\n    te_prob_a = clf_a.predict_proba(X_te_emb)[:,1]\n    \n    clf_m = build_clinical_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    val_prob_m = clf_m.predict_proba(X_val_m)[:,1]\n    te_prob_m = clf_m.predict_proba(X_te_m)[:,1]\n\n    # ── LEVEL 2: THE SUPERVISOR (TRBL META-LEARNER) ──\n    print(\"[*] Training Level-2 Supervisor on OOF Features\")\n    X_tr_stack = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_val_stack = np.column_stack([val_prob_a, val_prob_m, X_val_m])\n    X_te_stack = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n    \n    supervisor = build_supervisor(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_val_stack, y_val)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    fold_auc = roc_auc_score(y_te, te_prob_stack)\n    print(f\"[*] Fold {fold_i+1} Supervisor ROC-AUC: {fold_auc:.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_stack=(\"pred_stack\", \"max\")\n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"V7 Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V7 (OOF CVPEM Stacking + TRBL)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"REPORT-READY SUMMARY (VERSION 7 - SOTA LIMIT BREAKER)\")\nprint(\"=\"*85)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v7.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V7 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport librosa\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\n# Root directory from your screenshot\nAUDIO_ROOT = \"/kaggle/input/tb-audio/Tuberculosis/raw_data\"\n\nfolder_stats = []\n\n# Iterate through subfolders (longitudinal_data, solicited_data)\nfor folder_name in ['longitudinal_data', 'solicited_data']:\n    folder_path = os.path.join(AUDIO_ROOT, folder_name)\n    if not os.path.exists(folder_path):\n        continue\n        \n    print(f\"Processing folder: {folder_name}...\")\n    \n    # Find all unique audio files in this specific folder\n    files_in_folder = []\n    for root, _, filenames in os.walk(folder_path):\n        for fn in filenames:\n            if fn.lower().endswith((\".wav\", \".ogg\", \".flac\", \".mp3\")):\n                files_in_folder.append(os.path.join(root, fn))\n    \n    unique_files = list(set(files_in_folder))\n    \n    durations = []\n    sizes = []\n    \n    for path in tqdm(unique_files, desc=f\"Analyzing {folder_name}\", leave=False):\n        try:\n            # Fast duration check without loading the full waveform\n            duration = librosa.get_duration(path=path)\n            durations.append(duration)\n            sizes.append(os.path.getsize(path) / (1024 * 1024)) # MB\n        except Exception:\n            continue\n\n    if durations:\n        folder_stats.append({\n            \"Folder\": folder_name,\n            \"Unique Files\": len(durations),\n            \"Total Minutes\": sum(durations) / 60,\n            \"Avg Sec\": sum(durations) / len(durations),\n            \"Total Size (MB)\": sum(sizes)\n        })\n\n# Display the final breakdown\nstats_df = pd.DataFrame(folder_stats)\nprint(\"\\n\" + \"=\"*60)\nprint(\"AUDIO DATASET BREAKDOWN BY FOLDER\")\nprint(\"=\"*60)\nprint(stats_df.to_string(index=False))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 8.1 - SOTA LIMIT BREAKER)\n# Acoustic Tiling (Zero-Silence) + OOF Stacking + Non-Linear Supervisor\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # Google HeAR HARDCODED constraint (2.0s)\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V8.1)\nOUT_ROOT = \"/kaggle/working/outputs_v8_1\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"tiled_oof_stacking\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_tiled2s_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. ACOUSTIC TILING (ZERO-SILENCE) ───────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL & EXTRACTING AUDIO\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_tiled_audio(path):\n    \"\"\"If audio is < 2s, TILES (repeats) the audio instead of padding with silence.\"\"\"\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        dur = len(audio) / sr\n        \n        if len(audio) == 0:\n            return np.zeros(WIN_SAMPLES, np.float32), 0.0\n            \n        if len(audio) < WIN_SAMPLES:\n            # TILING THE SIGNAL: Repeats 0.5s audio 4x to fill 2.0s\n            repeats = int(np.ceil(WIN_SAMPLES / len(audio)))\n            audio = np.tile(audio, repeats)[:WIN_SAMPLES]\n        else:\n            # Smart Energy Peak Detection for files > 2s\n            frame_len = 400; hop = 160\n            frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n            rms = np.sqrt(np.mean(frames**2, axis=0))\n            smooth_n = max(1, int(0.2 * sr / hop))\n            rms_smooth = np.convolve(rms, np.ones(smooth_n)/smooth_n, mode=\"same\")\n            peak_fr = int(np.argmax(rms_smooth))\n            center = peak_fr * hop + frame_len // 2\n            \n            start = max(0, center - WIN_SAMPLES // 2)\n            end = start + WIN_SAMPLES\n            if end > len(audio):\n                end = len(audio); start = max(0, len(audio) - WIN_SAMPLES)\n            audio = audio[start:end]\n            \n        return audio, dur\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32), 0.0\n\ndef get_tiled_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    durations = np.zeros(N, np.float32)\n    \n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys, buf_durs = [], [], []\n    new_entries = []\n    \n    for i, row in tqdm(need, desc=\"Extracting Audio (Acoustic Tiling)\", leave=False):\n        seg, dur = load_tiled_audio(row.audio_path)\n        buf_segs.append(seg)\n        buf_keys.append(keys[i])\n        buf_durs.append(dur)\n        \n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            buf_segs, buf_keys, buf_durs = [], [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], zip(cache[\"embedding\"], cache[\"duration\"])))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            emb_val, dur_val = cache_dict[k]\n            embeddings[i] = np.array(emb_val, np.float32) if not isinstance(emb_val, np.ndarray) else emb_val\n            durations[i]  = float(dur_val)\n            \n    return embeddings, durations\n\n# ── 5. PREPROCESSING & OOF STACKING BUILDERS ────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# LEVEL 1 EXPERTS\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=300, learning_rate=0.03, num_leaves=31, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\n# LEVEL 2 SUPERVISOR (Non-Linear LightGBM with TRBL)\ndef build_supervisor(n_pos, n_neg):\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100, learning_rate=0.03,\n            num_leaves=7, max_depth=3,  # Very shallow to avoid overfitting meta-features\n            min_child_samples=10,\n            scale_pos_weight=trbl_scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight={0: 1.0, 1: trbl_scale}, max_iter=2000, random_state=SEED)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V8.1 TRAINING (ACOUSTIC TILING + STACKING)\")\nprint(\"=\"*60)\n\n# Pre-fetch all embeddings once to print duration stats\nprint(\"[*] Pre-fetching audio to audit durations & tile...\")\nall_embs, all_durs = get_tiled_embeddings(cough_df)\nprint(f\"    -> Audio Durations (seconds) | Min: {np.min(all_durs):.2f}s | Max: {np.max(all_durs):.2f}s | Mean: {np.mean(all_durs):.2f}s\")\n\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # Extract Smart Embeddings\n    X_tr_emb, _ = get_tiled_embeddings(df_tr)\n    X_val_emb, _ = get_tiled_embeddings(df_val)\n    X_te_emb, _ = get_tiled_embeddings(df_te)\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m = meta_prep.transform(df_te)\n    \n    # ── LEVEL 1: GENERATE OUT-OF-FOLD (OOF) PROBABILITIES FOR TRAIN ──\n    print(\"[*] Generating Out-Of-Fold Probabilities...\")\n    cv_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=SEED)\n    inner_folds = list(cv_inner.split(df_tr, y_tr, df_tr[\"participant_id\"]))\n    \n    tr_oof_a = np.zeros(len(y_tr))\n    tr_oof_m = np.zeros(len(y_tr))\n    \n    for i_tr, i_val in inner_folds:\n        clf_a_inner = build_audio_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_emb[i_tr], y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_inner.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        clf_m_inner = build_clinical_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_inner.predict_proba(X_tr_m[i_val])[:, 1]\n        \n    # ── LEVEL 1: TRAIN EXPERTS ON FULL TRAIN SET ──\n    clf_a = build_audio_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_emb, y_tr)\n    val_prob_a = clf_a.predict_proba(X_val_emb)[:,1]\n    te_prob_a = clf_a.predict_proba(X_te_emb)[:,1]\n    \n    clf_m = build_clinical_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    val_prob_m = clf_m.predict_proba(X_val_m)[:,1]\n    te_prob_m = clf_m.predict_proba(X_te_m)[:,1]\n\n    # ── LEVEL 2: THE NON-LINEAR SUPERVISOR ──\n    X_tr_stack = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_val_stack = np.column_stack([val_prob_a, val_prob_m, X_val_m])\n    X_te_stack = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n    \n    supervisor = build_supervisor(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_val_stack, y_val)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    print(f\"[*] Fold {fold_i+1} Supervisor ROC-AUC: {roc_auc_score(y_te, te_prob_stack):.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), prob_stack=(\"pred_stack\", \"max\")\n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"V8.1 Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V8.1 (Acoustic Tiling + LGBM Meta)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*85)\nprint(\"REPORT-READY SUMMARY (VERSION 8.1 - SOTA LIMIT BREAKER)\")\nprint(\"=\"*85)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v8_1.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V8.1 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 9 - THE DEPLOYMENT CANDIDATE)\n# Mirrored Tiling (No Artifacts) + OOF Stacking + Mean Probability Voting\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, brier_score_loss, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR = 16_000\nWIN_SAMPLES = 32_000  # Google HeAR HARDCODED constraint (2.0s)\nEMBED_DIM = 512\nN_SPLITS = 5          \nTARGET_SENS = [0.85, 0.90, 0.95]\n\n# Output Directories (V9)\nOUT_ROOT = \"/kaggle/working/outputs_v9\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"mirrored_oof_stacking\")\nCACHE_DIR = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nHEAR_VERSION = \"google/hear-v1\"\nEMBED_CACHE  = os.path.join(CACHE_DIR, \"hear_mirrored_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols], on=\"participant_id\", how=\"left\")\n\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files mapped: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\n\n# ── 3. STRATIFIED GROUP K-FOLD ──────────────────────────────────────────────\nprint(\"\\n[*] Building Custom Stratified Group K-Folds...\")\nsgkf = StratifiedGroupKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(cough_df, cough_df[\"label\"], cough_df[\"participant_id\"]))\n\n# ── 4. ACOUSTIC REFLECTION (ZERO-ARTIFACT TILING) ───────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL & EXTRACTING AUDIO\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments):\n    if HEAR_SERVING is None: return np.zeros((len(segments), EMBED_DIM), np.float32)\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\ndef load_reflected_audio(path):\n    \"\"\"Uses Mirrored Tiling to extend 0.5s audio to 2.0s without seam artifacts.\"\"\"\n    try:\n        audio, sr = librosa.load(str(path), sr=SR, mono=True)\n        dur = len(audio) / sr\n        \n        if len(audio) == 0:\n            return np.zeros(WIN_SAMPLES, np.float32), 0.0\n            \n        if len(audio) < WIN_SAMPLES:\n            # MIRRORED TILING (Acoustic Reflection):\n            # Normal tiling [1, 2, 3] -> [1, 2, 3 | 1, 2, 3] creates a harsh \"click\" at 3->1.\n            # Mirrored tiling [1, 2, 3] -> [1, 2, 3, 3, 2, 1 | 1, 2, 3] ensures perfect amplitude continuity!\n            audio_mirrored = np.concatenate((audio, audio[::-1]))\n            repeats = int(np.ceil(WIN_SAMPLES / len(audio_mirrored)))\n            audio = np.tile(audio_mirrored, repeats)[:WIN_SAMPLES]\n        else:\n            # Smart Energy Peak Detection for files > 2s\n            frame_len = 400; hop = 160\n            frames = librosa.util.frame(audio, frame_length=frame_len, hop_length=hop)\n            rms = np.sqrt(np.mean(frames**2, axis=0))\n            smooth_n = max(1, int(0.2 * sr / hop))\n            rms_smooth = np.convolve(rms, np.ones(smooth_n)/smooth_n, mode=\"same\")\n            peak_fr = int(np.argmax(rms_smooth))\n            center = peak_fr * hop + frame_len // 2\n            \n            start = max(0, center - WIN_SAMPLES // 2)\n            end = start + WIN_SAMPLES\n            if end > len(audio):\n                end = len(audio); start = max(0, len(audio) - WIN_SAMPLES)\n            audio = audio[start:end]\n            \n        return audio, dur\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32), 0.0\n\ndef get_reflected_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try: cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n    else: cache = pd.DataFrame(columns=[\"key\", \"embedding\", \"duration\"])\n\n    N = len(df_rows)\n    embeddings = np.zeros((N, EMBED_DIM), np.float32)\n    durations = np.zeros(N, np.float32)\n    \n    keys = [hashlib.md5(f\"{HEAR_VERSION}::{r.audio_path}\".encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    \n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n    \n    buf_segs, buf_keys, buf_durs = [], [], []\n    new_entries = []\n    \n    for i, row in tqdm(need, desc=\"Extracting Audio (Mirrored)\", leave=False):\n        seg, dur = load_reflected_audio(row.audio_path)\n        buf_segs.append(seg)\n        buf_keys.append(keys[i])\n        buf_durs.append(dur)\n        \n        if len(buf_segs) >= 64:\n            embs = _infer_batch(buf_segs)\n            new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            buf_segs, buf_keys, buf_durs = [], [], []\n            \n    if buf_segs:\n        embs = _infer_batch(buf_segs)\n        new_entries.extend([{\"key\": k, \"embedding\": e.tolist(), \"duration\": d} for k, e, d in zip(buf_keys, embs, buf_durs)])\n            \n    if new_entries:\n        cache = pd.concat([cache, pd.DataFrame(new_entries)], ignore_index=True)\n        cache[\"key\"] = cache[\"key\"].astype(str)\n        cache.to_parquet(EMBED_CACHE, index=False)\n        \n    cache_dict = dict(zip(cache[\"key\"], zip(cache[\"embedding\"], cache[\"duration\"])))\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            emb_val, dur_val = cache_dict[k]\n            embeddings[i] = np.array(emb_val, np.float32) if not isinstance(emb_val, np.ndarray) else emb_val\n            durations[i]  = float(dur_val)\n            \n    return embeddings, durations\n\n# ── 5. PREPROCESSING & OOF STACKING BUILDERS ────────────────────────────────\ndef build_meta_preprocessor(num_cols, cat_cols):\n    transformers = []\n    if num_cols:\n        transformers.append((\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_cols))\n    if cat_cols:\n        transformers.append((\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_cols))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\n# LEVEL 1 EXPERTS\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        # Added colsample_bytree to force LightGBM to generalize across the 512 dimensions\n        return lgb.LGBMClassifier(\n            n_estimators=300, learning_rate=0.03, num_leaves=31, \n            colsample_bytree=0.3, scale_pos_weight=scale, \n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=2000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.03, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\n# LEVEL 2 SUPERVISOR (Non-Linear LightGBM with TRBL)\ndef build_supervisor(n_pos, n_neg):\n    trbl_scale = (n_neg / max(n_pos, 1)) * 1.5 \n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100, learning_rate=0.03,\n            num_leaves=7, max_depth=3,\n            min_child_samples=10,\n            scale_pos_weight=trbl_scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight={0: 1.0, 1: trbl_scale}, max_iter=2000, random_state=SEED)\n\n# ── EVALUATION HELPERS ──────────────────────────────────────────────────────\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \"sensitivity\": tp/(tp+fn+1e-9), \"specificity\": tn/(tn+fp+1e-9)}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true))>1 else np.nan}\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob); auc = roc_auc_score(y_true, y_prob)\n    fig, ax = plt.subplots(figsize=(5,4)); ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1); ax.set(title=f\"{title_prefix} ROC\"); ax.legend()\n    fig.tight_layout(); fig.savefig(f\"{path_prefix}_roc.png\", dpi=150); plt.close(fig)\n\n# ── 6. TRAINING & EVALUATION LOOP ───────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. STARTING V9 TRAINING (MIRRORED AUDIO + MEAN POOLING)\")\nprint(\"=\"*60)\n\nprint(\"[*] Pre-fetching audio to audit durations & apply acoustic reflection...\")\nall_embs, all_durs = get_reflected_embeddings(cough_df)\n\noof_stack = np.zeros(len(cough_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    \n    df_tr_full = cough_df.iloc[tr_idx].reset_index(drop=True)\n    df_te      = cough_df.iloc[te_idx].reset_index(drop=True)\n    \n    val_split_idx = int(len(df_tr_full) * 0.8)\n    df_tr, df_val = df_tr_full.iloc[:val_split_idx], df_tr_full.iloc[val_split_idx:]\n    \n    y_tr, y_val, y_te = df_tr[\"label\"].values, df_val[\"label\"].values, df_te[\"label\"].values\n    \n    # Extract Mirrored Embeddings\n    X_tr_emb, _ = get_reflected_embeddings(df_tr)\n    X_val_emb, _ = get_reflected_embeddings(df_val)\n    X_te_emb, _ = get_reflected_embeddings(df_te)\n    \n    # Preprocess Metadata\n    meta_prep = build_meta_preprocessor(num_cols, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_val_m = meta_prep.transform(df_val)\n    X_te_m = meta_prep.transform(df_te)\n    \n    # ── LEVEL 1: GENERATE OUT-OF-FOLD (OOF) PROBABILITIES FOR TRAIN ──\n    print(\"[*] Generating Out-Of-Fold Probabilities...\")\n    cv_inner = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=SEED)\n    inner_folds = list(cv_inner.split(df_tr, y_tr, df_tr[\"participant_id\"]))\n    \n    tr_oof_a = np.zeros(len(y_tr))\n    tr_oof_m = np.zeros(len(y_tr))\n    \n    for i_tr, i_val in inner_folds:\n        clf_a_inner = build_audio_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_emb[i_tr], y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_inner.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        clf_m_inner = build_clinical_expert(int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())).fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_inner.predict_proba(X_tr_m[i_val])[:, 1]\n        \n    # ── LEVEL 1: TRAIN EXPERTS ON FULL TRAIN SET ──\n    clf_a = build_audio_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_emb, y_tr)\n    val_prob_a = clf_a.predict_proba(X_val_emb)[:,1]\n    te_prob_a = clf_a.predict_proba(X_te_emb)[:,1]\n    \n    clf_m = build_clinical_expert(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_m, y_tr)\n    val_prob_m = clf_m.predict_proba(X_val_m)[:,1]\n    te_prob_m = clf_m.predict_proba(X_te_m)[:,1]\n\n    # ── LEVEL 2: THE NON-LINEAR SUPERVISOR ──\n    X_tr_stack = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_val_stack = np.column_stack([val_prob_a, val_prob_m, X_val_m])\n    X_te_stack = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n    \n    supervisor = build_supervisor(int(y_tr.sum()), int((y_tr==0).sum())).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_val_stack, y_val)\n    \n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:,1]\n    oof_stack[te_idx] = te_prob_stack\n    \n    print(f\"[*] Fold {fold_i+1} Supervisor ROC-AUC: {roc_auc_score(y_te, te_prob_stack):.4f}\")\n\n# ── 7. FINAL SCORES & REPORTING ─────────────────────────────────────────────\ncough_df[\"pred_stack\"] = oof_stack\n\n# ----------------------------------------------------------------------------\n# THE SPECIFICITY FIX: Use 'mean' instead of 'max' to prevent noisy outliers \n# from destroying healthy patient predictions!\n# ----------------------------------------------------------------------------\npart_df = cough_df.groupby(\"participant_id\").agg(\n    label=(\"label\", \"first\"), \n    prob_stack=(\"pred_stack\", \"mean\")  \n).reset_index()\n\nm_stack = full_eval(cough_df['label'], oof_stack)\np_stack = full_eval(part_df['label'], part_df['prob_stack'])\n\nplot_curves(cough_df['label'], oof_stack, f\"{FUSION_OUT}/plots/sota_stacking\", \"V9 Stacking\")\n\ndef make_row(name, cough_m, part_m):\n    return {\n        \"Model\": name,\n        \"ROC-AUC (recording)\": f\"{cough_m.get('roc_auc', 0):.4f}\",\n        \"ROC-AUC (participant)\": f\"{part_m.get('roc_auc', 0):.4f}\",\n        \"Sens@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('sensitivity',0):.4f}\",\n        \"Spec@90%\": f\"{cough_m.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\"\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V9 (Mirrored Audio + Mean Voting)\", m_stack, p_stack)])\n\nprint(\"\\n\" + \"=\"*90)\nprint(\"REPORT-READY SUMMARY (VERSION 9 - DEPLOYMENT CANDIDATE)\")\nprint(\"=\"*90)\nprint(summary_df.to_string(index=False))\n\nzip_path = \"/kaggle/working/outputs_v9.zip\"\nwith zipfile.ZipFile(zip_path,\"w\",zipfile.ZIP_DEFLATED) as zf:\n    for root,_,files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root,fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\nprint(f\"\\n✅ All V9 Results Zipped to: {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# TB SCREENING RANKER — CODA-TB DATASET (VERSION 10 - RESEARCH CANDIDATE)\n# ============================================================================\n# KEY IMPROVEMENTS OVER V9:\n#\n#  1. MULTI-WINDOW EMBEDDING: Every cough recording is sliced into ALL possible\n#     2-second windows (with 50% overlap), each embedded by HeAR, then\n#     aggregated (mean + std + [25,50,75] percentiles = 5x512 = 2560-dim feature).\n#     This replaces the single-window \"energy peak\" approach that discards signal.\n#\n#  2. PARTICIPANT-LEVEL CLASSIFICATION (no late-fusion): All cough embeddings\n#     from a participant are aggregated BEFORE classification, not after.\n#     The model sees a 2560-dim \"acoustic fingerprint\" per participant,\n#     combined with clinical meta. This avoids noisy recording-level predictions.\n#\n#  3. PARTIAL AUC (pAUC) AS OBJECTIVE: The CODA challenge metric is specificity\n#     at ≥90% sensitivity. We add a pAUC metric (TPR in [0.85, 1.0]) and tune\n#     all thresholds against it.\n#\n#  4. COUNTRY AS EXPLICIT FEATURE: AUROC varies dramatically by country in this\n#     dataset (0.63-0.81 in the challenge). Country is now a hard-coded feature.\n#\n#  5. EMBEDDING NOISE AUGMENTATION: Gaussian noise injection on HeAR embeddings\n#     during training (σ=0.01) provides regularization without touching audio.\n#\n#  6. FIXED CALIBRATION SPLIT: Uses a proper StratifiedGroupKFold inner split\n#     for calibration data, not a naive sequential iloc slice.\n#\n#  7. COUGH COUNT AS FEATURE: Number of valid cough segments per participant\n#     is included as an explicit clinical feature.\n#\n#  8. CLEANER CACHE: Cache key now includes a content hash of the audio path\n#     only (no version string that can silently go stale).\n# ============================================================================\n\nimport os, sys, json, warnings, random, hashlib, zipfile\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom scipy import stats as sp_stats\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedGroupKFold, StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n                             f1_score, confusion_matrix, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION ────────────────────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR          = 16_000\nWIN_SAMPLES = 32_000   # 2s @ 16kHz — HeAR hard constraint\nHOP_SAMPLES = 16_000   # 50% overlap for multi-window extraction\nEMBED_DIM   = 512\nN_SPLITS    = 5\nTARGET_SENS = [0.85, 0.90, 0.95]\nPAUC_LOW    = 0.85     # pAUC window: specificity-at-sensitivity >= PAUC_LOW\n\n# Aggregation stats per participant (mean + std + 3 percentiles = 5 vectors)\nAGG_FUNCS   = [\"mean\", \"std\", \"p25\", \"p50\", \"p75\"]\nAGG_DIM     = EMBED_DIM * len(AGG_FUNCS)  # 2560\n\n# Embedding augmentation noise (Gaussian, applied only during training)\nEMB_NOISE_STD = 0.01\n\n# Output Directories\nOUT_ROOT   = \"/kaggle/working/outputs_v10\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"multiwindow_participant\")\nCACHE_DIR  = os.path.join(OUT_ROOT, \"cache\")\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\"]:\n    os.makedirs(d, exist_ok=True)\n\nEMBED_CACHE = os.path.join(CACHE_DIR, \"hear_multiwindow_embeddings.parquet\")\n\n# ── 2. DATA LOADING & MERGING ───────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio    = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\n# ── Clinical feature selection (exclude post-diagnostic leakage) ─────────────\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\n                \"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\n# Ensure country column is included as categorical (high-value feature)\ncountry_col = None\nfor hint in [\"country\", \"site\", \"country_id\", \"collection_country\"]:\n    matches = [c for c in df_clinical.columns if hint in c.lower()]\n    if matches:\n        country_col = matches[0]\n        if country_col not in cat_cols:\n            cat_cols.append(country_col)\n        break\n\nif country_col:\n    print(f\"[*] Country feature found: '{country_col}'\")\nelse:\n    print(\"[!] WARNING: No country column found. Country is a top-3 feature for TB!\")\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols],\n                          on=\"participant_id\", how=\"left\")\n\n# ── Audio file mapping ────────────────────────────────────────────────────────\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(\n    lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\nprint(f\"[*] Total valid audio files: {len(cough_df)}\")\nprint(f\"[*] Total unique participants: {cough_df['participant_id'].nunique()}\")\nprint(f\"[*] Num clinical features — numerical: {len(num_cols)}, categorical: {len(cat_cols)}\")\n\n# ── 3. HeAR MODEL LOADING ────────────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL\")\nprint(\"=\"*60)\ntry:\n    from kaggle_secrets import UserSecretsClient\n    from huggingface_hub import login, from_pretrained_keras\n    import tensorflow as tf\n    _sec = UserSecretsClient()\n    login(token=_sec.get_secret(\"HF_TOKEN\"))\n    HEAR_MODEL   = from_pretrained_keras(\"google/hear\")\n    HEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n    print(\"[*] ✓ HeAR loaded successfully\")\nexcept Exception as e:\n    print(f\"[*] ⚠ HeAR load failed: {e}\")\n    HEAR_SERVING = None\n\ndef _infer_batch(segments: list) -> np.ndarray:\n    \"\"\"Run HeAR on a batch of 2s segments. Returns (N, 512) float32.\"\"\"\n    if HEAR_SERVING is None:\n        return np.zeros((len(segments), EMBED_DIM), np.float32)\n    import tensorflow as tf\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\n# ── 4. MULTI-WINDOW AUDIO LOADING ────────────────────────────────────────────\ndef load_audio(path: str):\n    \"\"\"Load audio, return float32 waveform at SR.\"\"\"\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef extract_windows(audio: np.ndarray) -> list:\n    \"\"\"\n    Slice audio into all WIN_SAMPLES windows with HOP_SAMPLES stride.\n    Short clips: mirrored-tile to WIN_SAMPLES, yielding exactly 1 window.\n    Long clips: sliding window with 50% overlap — captures all cough events.\n    \"\"\"\n    if len(audio) == 0:\n        return [np.zeros(WIN_SAMPLES, np.float32)]\n\n    if len(audio) < WIN_SAMPLES:\n        # Mirrored tiling (no click artifact at boundaries)\n        audio_mir = np.concatenate((audio, audio[::-1]))\n        repeats   = int(np.ceil(WIN_SAMPLES / len(audio_mir)))\n        audio     = np.tile(audio_mir, repeats)[:WIN_SAMPLES]\n        return [audio.astype(np.float32)]\n\n    windows = []\n    start   = 0\n    while start + WIN_SAMPLES <= len(audio):\n        windows.append(audio[start:start + WIN_SAMPLES].astype(np.float32))\n        start += HOP_SAMPLES\n\n    # Always include the tail window to capture the end of long coughs\n    if start < len(audio):\n        tail_start = len(audio) - WIN_SAMPLES\n        windows.append(audio[tail_start:].astype(np.float32))\n\n    return windows if windows else [audio[:WIN_SAMPLES].astype(np.float32)]\n\ndef aggregate_embeddings(emb_matrix: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Aggregate (N_windows, 512) embeddings into a single (2560,) vector.\n    Stats: mean, std, 25th, 50th, 75th percentile across windows.\n    This is much richer than a single-window embedding.\n    \"\"\"\n    if emb_matrix.shape[0] == 1:\n        # Single window: pad std/percentiles with zeros (no variance info)\n        e = emb_matrix[0]\n        return np.concatenate([e, np.zeros(EMBED_DIM * 4, np.float32)])\n    m   = emb_matrix.mean(axis=0)\n    s   = emb_matrix.std(axis=0)\n    p25 = np.percentile(emb_matrix, 25, axis=0)\n    p50 = np.percentile(emb_matrix, 50, axis=0)\n    p75 = np.percentile(emb_matrix, 75, axis=0)\n    return np.concatenate([m, s, p25, p50, p75]).astype(np.float32)\n\n# ── 5. EMBEDDING EXTRACTION WITH CACHING ─────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"3. EXTRACTING MULTI-WINDOW HeAR EMBEDDINGS\")\nprint(\"=\"*60)\n\ndef get_multiwindow_embeddings(df_rows: pd.DataFrame):\n    \"\"\"\n    Returns:\n      agg_embeddings: (N_rows, AGG_DIM=2560)  — per-recording aggregated embedding\n      n_windows_arr:  (N_rows,)               — number of windows extracted per recording\n    \"\"\"\n    if os.path.exists(EMBED_CACHE):\n        try:    cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n    else:\n        cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n\n    N   = len(df_rows)\n    agg = np.zeros((N, AGG_DIM), np.float32)\n    nw  = np.zeros(N, np.int32)\n\n    # Use path-only hash (no version string that can silently go stale)\n    keys         = [hashlib.md5(str(r.audio_path).encode()).hexdigest()\n                    for _, r in df_rows.iterrows()]\n    cached_keys  = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    need         = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows())\n                    if keys[i] not in cached_keys]\n\n    # Process in batches across all windows from multiple files\n    BATCH = 64\n    buf_segs, buf_meta = [], []  # meta = (row_idx, key, window_idx_for_file, total_for_file)\n\n    new_entries = {}  # key -> {\"segments_emb\": list_of_embs, \"n_windows\": int}\n\n    for i, row in tqdm(need, desc=\"Loading audio\", leave=False):\n        audio   = load_audio(row.audio_path)\n        windows = extract_windows(audio)\n        k       = keys[i]\n        new_entries[k] = {\"n_windows\": len(windows), \"embs\": []}\n\n        for seg in windows:\n            buf_segs.append(seg)\n            buf_meta.append(k)\n\n            if len(buf_segs) >= BATCH:\n                batch_embs = _infer_batch(buf_segs)\n                for idx, (bk, be) in enumerate(zip(buf_meta, batch_embs)):\n                    new_entries[bk][\"embs\"].append(be)\n                buf_segs, buf_meta = [], []\n\n    if buf_segs:\n        batch_embs = _infer_batch(buf_segs)\n        for bk, be in zip(buf_meta, batch_embs):\n            new_entries[bk][\"embs\"].append(be)\n\n    # Aggregate and save new entries\n    new_rows = []\n    for k, v in new_entries.items():\n        emb_mat = np.stack(v[\"embs\"])           # (n_windows, 512)\n        agg_emb = aggregate_embeddings(emb_mat) # (2560,)\n        new_rows.append({\"key\": k,\n                         \"agg_embedding\": agg_emb.tolist(),\n                         \"n_windows\": v[\"n_windows\"]})\n\n    if new_rows:\n        cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n        cache.to_parquet(EMBED_CACHE, index=False)\n\n    cache_dict = {row[\"key\"]: row for _, row in cache.iterrows()}\n    for i in range(N):\n        k = keys[i]\n        if k in cache_dict:\n            r       = cache_dict[k]\n            agg_val = r[\"agg_embedding\"]\n            agg[i]  = np.array(agg_val, np.float32) if not isinstance(agg_val, np.ndarray) else agg_val\n            nw[i]   = int(r[\"n_windows\"])\n\n    return agg, nw\n\n# Extract all embeddings upfront\nprint(\"[*] Pre-fetching all audio embeddings (multi-window)...\")\nall_agg_embs, all_n_windows = get_multiwindow_embeddings(cough_df)\ncough_df[\"n_cough_windows\"] = all_n_windows\n\nprint(f\"[*] Embedding shape per recording: ({AGG_DIM},)\")\nprint(f\"[*] Avg windows per recording: {all_n_windows.mean():.1f}  \"\n      f\"(max={all_n_windows.max()}, min={all_n_windows.min()})\")\n\n# ── 6. PARTICIPANT-LEVEL AGGREGATION ─────────────────────────────────────────\n# Pool all per-recording embeddings into one fingerprint per participant.\n# This is the key architectural insight: the model should reason about\n# a participant, not individual recordings.\nprint(\"\\n[*] Aggregating to participant level...\")\n\nparticipant_ids  = cough_df[\"participant_id\"].values\nunique_pids      = cough_df[\"participant_id\"].unique()\n\n# Build a participant-level DataFrame\npid_records = []\nfor pid in unique_pids:\n    mask  = participant_ids == pid\n    p_embs = all_agg_embs[mask]   # (n_recordings, 2560)\n    label  = cough_df.loc[mask, \"label\"].values[0]\n    n_recs = mask.sum()\n\n    # Average across recordings for this participant\n    p_mean_emb = p_embs.mean(axis=0)  # (2560,)\n\n    # Get clinical features from first row (they're participant-level)\n    first_row = cough_df.loc[mask].iloc[0]\n    rec = {\"participant_id\": pid, \"label\": label, \"n_recordings\": n_recs}\n    for col in num_cols + cat_cols:\n        rec[col] = first_row.get(col, np.nan)\n    rec[\"n_cough_windows_total\"] = int(cough_df.loc[mask, \"n_cough_windows\"].sum())\n    pid_records.append((rec, p_mean_emb))\n\nparticipant_df  = pd.DataFrame([r for r, _ in pid_records]).reset_index(drop=True)\nparticipant_embs = np.stack([e for _, e in pid_records])  # (N_participants, 2560)\n\nprint(f\"[*] Participant-level dataset: {len(participant_df)} participants, \"\n      f\"{participant_df['label'].sum()} TB+, {(participant_df['label']==0).sum()} TB-\")\n\n# Add n_recordings and n_cough_windows_total to numerical features\nextra_num = [\"n_recordings\", \"n_cough_windows_total\"]\nnum_cols_p = num_cols + extra_num\n\n# ── 7. CROSS-VALIDATION SETUP ────────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. STARTING V10 TRAINING (PARTICIPANT-LEVEL + MULTI-WINDOW)\")\nprint(\"=\"*60)\n\n# Folds are at participant level (no group constraint needed — already deduplicated)\nsgkf  = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(participant_df, participant_df[\"label\"]))\n\n# ── 8. PREPROCESSING & MODEL BUILDERS ────────────────────────────────────────\ndef build_meta_preprocessor(num_c, cat_c):\n    transformers = []\n    if num_c:\n        transformers.append((\"num\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n            (\"sc\",  StandardScaler())\n        ]), num_c))\n    if cat_c:\n        transformers.append((\"cat\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Not_Available\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n        ]), cat_c))\n    return ColumnTransformer(transformers, remainder=\"drop\")\n\ndef add_embedding_noise(X: np.ndarray, noise_std: float = EMB_NOISE_STD) -> np.ndarray:\n    \"\"\"Gaussian noise injection for embedding augmentation (training only).\"\"\"\n    return X + np.random.normal(0, noise_std, X.shape).astype(np.float32)\n\ndef build_audio_expert(n_pos, n_neg):\n    \"\"\"LightGBM over aggregated multi-window HeAR embeddings (2560-dim).\"\"\"\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=400,\n            learning_rate=0.02,\n            num_leaves=31,\n            colsample_bytree=0.4,    # important: 2560 features, need regularization\n            subsample=0.8,\n            min_child_samples=5,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\", max_iter=3000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    \"\"\"LightGBM over clinical + demographic features.\"\"\"\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=200,\n            learning_rate=0.02,\n            num_leaves=15,\n            max_depth=4,\n            min_child_samples=5,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight=\"balanced\")\n\ndef build_supervisor(n_pos, n_neg):\n    \"\"\"\n    Shallow meta-learner over stacked [audio_prob, clinical_prob, clinical_features].\n    Deliberately constrained to prevent overfitting on small N.\n    \"\"\"\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB:\n        return lgb.LGBMClassifier(\n            n_estimators=100,\n            learning_rate=0.02,\n            num_leaves=7,\n            max_depth=3,\n            min_child_samples=8,\n            scale_pos_weight=scale,\n            random_state=SEED, verbose=-1, n_jobs=-1\n        )\n    return LogisticRegression(class_weight={0: 1.0, 1: scale}, max_iter=2000, random_state=SEED)\n\n# ── 9. EVALUATION HELPERS ────────────────────────────────────────────────────\ndef partial_auc(y_true, y_prob, low_tpr=PAUC_LOW):\n    \"\"\"\n    Partial AUC: area under ROC curve where TPR (sensitivity) >= low_tpr.\n    Normalized to [0, 1] by dividing by the max possible area (1 - low_tpr).\n    This is the primary metric aligned with the CODA challenge objective.\n    \"\"\"\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    # We want the region where TPR >= low_tpr\n    mask  = tpr >= low_tpr\n    if mask.sum() < 2:\n        return 0.0\n    # Interpolate the FPR at exactly tpr=low_tpr\n    sub_fpr = fpr[mask]\n    sub_tpr = tpr[mask]\n    area    = float(np.trapz(sub_tpr, sub_fpr))\n    # Area is negative because fpr is typically increasing -> abs\n    area    = abs(area)\n    max_area = (1.0 - low_tpr)\n    return area / max_area if max_area > 0 else 0.0\n\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\n        \"threshold\":   float(t),\n        \"accuracy\":    float(accuracy_score(y_true, y_pred)),\n        \"sensitivity\": tp / (tp + fn + 1e-9),\n        \"specificity\": tn / (tn + fp + 1e-9),\n        \"f1\":          float(f1_score(y_true, y_pred, zero_division=0))\n    }\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec = m[\"specificity\"]; best_t = t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    y_true = np.array(y_true); y_prob = np.array(y_prob)\n    m = {\n        \"roc_auc\":  float(roc_auc_score(y_true, y_prob)) if len(np.unique(y_true)) > 1 else np.nan,\n        \"pauc_85\":  partial_auc(y_true, y_prob, low_tpr=0.85),\n        \"pauc_90\":  partial_auc(y_true, y_prob, low_tpr=0.90),\n    }\n    m[\"tuned_thresholds\"] = {}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\n            \"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)\n        }\n    return m\n\ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    auc = roc_auc_score(y_true, y_prob)\n    fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n\n    # Full ROC\n    ax = axes[0]\n    ax.plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={auc:.3f}\")\n    ax.plot([0,1],[0,1],\"--\",color=\"gray\",lw=1)\n    ax.set(title=f\"{title_prefix} — Full ROC\", xlabel=\"FPR\", ylabel=\"TPR\")\n    ax.legend()\n\n    # Partial AUC zoom (high sensitivity region)\n    ax = axes[1]\n    mask = tpr >= 0.85\n    ax.fill_between(fpr[mask], tpr[mask], 0.85, alpha=0.25, color=\"#457b9d\",\n                    label=f\"pAUC@85%={partial_auc(y_true,y_prob,0.85):.3f}\")\n    ax.plot(fpr, tpr, color=\"#e63946\", lw=2)\n    ax.set_xlim([0, 1]); ax.set_ylim([0.8, 1.0])\n    ax.set(title=\"Partial AUC (TPR≥85%)\", xlabel=\"FPR\", ylabel=\"TPR\")\n    ax.legend(fontsize=8)\n\n    fig.tight_layout()\n    fig.savefig(f\"{path_prefix}_roc.png\", dpi=150)\n    plt.close(fig)\n\n# ── 10. MAIN TRAINING LOOP ───────────────────────────────────────────────────\noof_stack = np.zeros(len(participant_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n\n    df_tr_full   = participant_df.iloc[tr_idx].reset_index(drop=True)\n    df_te        = participant_df.iloc[te_idx].reset_index(drop=True)\n    emb_tr_full  = participant_embs[tr_idx]\n    emb_te       = participant_embs[te_idx]\n\n    y_tr_full    = df_tr_full[\"label\"].values\n    y_te         = df_te[\"label\"].values\n\n    # ── Calibration split: 20% of train, stratified ──────────────────────────\n    cal_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n    tr_sub_idx, cal_idx = next(cal_fold.split(df_tr_full, y_tr_full))\n\n    df_tr     = df_tr_full.iloc[tr_sub_idx].reset_index(drop=True)\n    df_cal    = df_tr_full.iloc[cal_idx].reset_index(drop=True)\n    emb_tr    = emb_tr_full[tr_sub_idx]\n    emb_cal   = emb_tr_full[cal_idx]\n    y_tr      = df_tr[\"label\"].values\n    y_cal     = df_cal[\"label\"].values\n\n    print(f\"[*] Train: {len(y_tr)} ({y_tr.sum()} TB+)  \"\n          f\"| Cal: {len(y_cal)} ({y_cal.sum()} TB+)  \"\n          f\"| Test: {len(y_te)} ({y_te.sum()} TB+)\")\n\n    # ── Preprocess clinical features ──────────────────────────────────────────\n    meta_prep  = build_meta_preprocessor(num_cols_p, cat_cols)\n    X_tr_m     = meta_prep.fit_transform(df_tr)\n    X_cal_m    = meta_prep.transform(df_cal)\n    X_te_m     = meta_prep.transform(df_te)\n\n    # ── Embedding noise augmentation (training only) ──────────────────────────\n    X_tr_emb   = add_embedding_noise(emb_tr)\n    X_cal_emb  = emb_cal   # no augmentation at calibration/test time\n    X_te_emb   = emb_te\n\n    # ── LEVEL 1: Inner OOF for stacking meta-features ────────────────────────\n    print(\"[*]  Generating inner OOF probabilities...\")\n    inner_cv   = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n    tr_oof_a   = np.zeros(len(y_tr))\n    tr_oof_m   = np.zeros(len(y_tr))\n\n    for i_tr, i_val in inner_cv.split(X_tr_emb, y_tr):\n        n_p  = int(y_tr[i_tr].sum()); n_n = int((y_tr[i_tr]==0).sum())\n\n        # Audio expert (inner)\n        clf_a_i = build_audio_expert(n_p, n_n)\n        X_aug   = add_embedding_noise(X_tr_emb[i_tr])   # augment inner too\n        clf_a_i.fit(X_aug, y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_i.predict_proba(X_tr_emb[i_val])[:, 1]\n\n        # Clinical expert (inner)\n        clf_m_i = build_clinical_expert(n_p, n_n)\n        clf_m_i.fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_i.predict_proba(X_tr_m[i_val])[:, 1]\n\n    # ── LEVEL 1: Full experts on all train data ───────────────────────────────\n    n_pos = int(y_tr.sum()); n_neg = int((y_tr==0).sum())\n\n    clf_a = build_audio_expert(n_pos, n_neg)\n    clf_a.fit(X_tr_emb, y_tr)\n    cal_prob_a = clf_a.predict_proba(X_cal_emb)[:, 1]\n    te_prob_a  = clf_a.predict_proba(X_te_emb)[:, 1]\n\n    clf_m = build_clinical_expert(n_pos, n_neg)\n    clf_m.fit(X_tr_m, y_tr)\n    cal_prob_m = clf_m.predict_proba(X_cal_m)[:, 1]\n    te_prob_m  = clf_m.predict_proba(X_te_m)[:, 1]\n\n    # ── LEVEL 2: Supervisor ───────────────────────────────────────────────────\n    X_tr_stack  = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_cal_stack = np.column_stack([cal_prob_a, cal_prob_m, X_cal_m])\n    X_te_stack  = np.column_stack([te_prob_a, te_prob_m, X_te_m])\n\n    supervisor  = build_supervisor(n_pos, n_neg)\n    supervisor.fit(X_tr_stack, y_tr)\n\n    # Platt scaling calibration on the held-out calibration fold\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\")\n    cal_supervisor.fit(X_cal_stack, y_cal)\n\n    te_prob_stack       = cal_supervisor.predict_proba(X_te_stack)[:, 1]\n    oof_stack[te_idx]   = te_prob_stack\n\n    fold_auc  = roc_auc_score(y_te, te_prob_stack)\n    fold_pauc = partial_auc(y_te, te_prob_stack, low_tpr=0.90)\n    print(f\"[*] Fold {fold_i+1} | AUC={fold_auc:.4f} | pAUC@90%={fold_pauc:.4f}\")\n\n# ── 11. FINAL REPORTING ──────────────────────────────────────────────────────\nprint(\"\\n\" + \"=\"*90)\nprint(\"5. FINAL EVALUATION (V10 — PARTICIPANT-LEVEL)\")\nprint(\"=\"*90)\n\nparticipant_df[\"pred_stack\"] = oof_stack\n\nm_part = full_eval(participant_df[\"label\"], participant_df[\"pred_stack\"])\n\nplot_curves(participant_df[\"label\"], participant_df[\"pred_stack\"],\n            f\"{FUSION_OUT}/plots/v10_participant\", \"V10 Participant-Level\")\n\ndef make_row(name, pm):\n    tt = pm.get(\"tuned_thresholds\", {})\n    return {\n        \"Model\":        name,\n        \"ROC-AUC\":      f\"{pm.get('roc_auc', 0):.4f}\",\n        \"pAUC@85%\":     f\"{pm.get('pauc_85', 0):.4f}\",\n        \"pAUC@90%\":     f\"{pm.get('pauc_90', 0):.4f}\",\n        \"Spec@Sens=85%\": f\"{tt.get('sens_85',{}).get('specificity',0):.4f}\",\n        \"Spec@Sens=90%\": f\"{tt.get('sens_90',{}).get('specificity',0):.4f}\",\n        \"Spec@Sens=95%\": f\"{tt.get('sens_95',{}).get('specificity',0):.4f}\",\n    }\n\nsummary_df = pd.DataFrame([make_row(\"V10 (Multi-Window + Participant-Level)\", m_part)])\n\nprint(summary_df.to_string(index=False))\nprint()\n\n# Threshold summary\nfor sk, sv in m_part.get(\"tuned_thresholds\", {}).items():\n    print(f\"  [{sk}] threshold={sv['threshold']:.4f}  \"\n          f\"sens={sv['sensitivity']:.3f}  spec={sv['specificity']:.3f}  \"\n          f\"f1={sv['f1']:.3f}\")\n\n# Save results\nsummary_df.to_csv(f\"{FUSION_OUT}/v10_summary.csv\", index=False)\nparticipant_df.to_csv(f\"{FUSION_OUT}/v10_oof_predictions.csv\", index=False)\n\nzip_path = \"/kaggle/working/outputs_v10.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUT_ROOT):\n        for fn in files:\n            fp = os.path.join(root, fn)\n            zf.write(fp, os.path.relpath(fp, \"/kaggle/working\"))\n\nprint(f\"\\n✅ All V10 Results → {zip_path}\")\nprint(\"PIPELINE COMPLETE\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n=====================================================================================\nTB SCREENING RANKER: MASTER TRAINING & EXPORT PIPELINE (VERSION 10)\n=====================================================================================\n\nDESCRIPTION:\nThis script is the final, production-ready build of the CODA-TB screening pipeline. \nIt trains decoupled models (Audio and Metadata) using an Out-of-Fold (OOF) Stacking \narchitecture. It rigorously evaluates the models using Partial AUC and automatically \nexports all weights, plots, and evaluation metrics for cloud deployment.\n\nKEY FEATURES IN V10:\n1. Robust Multi-Window Audio: Dynamically slices audio of any length into overlapping \n   2-second windows, extracting a rich 2560-dim acoustic fingerprint via Google HeAR.\n2. Decoupled Experts: Trains independent LightGBM models for Audio and Clinical Metadata.\n3. Master Supervisor: Fuses the independent probabilities to mathematically rank patients.\n4. Deployment Ready: Trains a final 100%-data master model and exports all `.pkl` \n   weights alongside a downloadable ZIP archive.\n=====================================================================================\n\"\"\"\n\nimport os, sys, json, warnings, random, hashlib, zipfile, shutil\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import FileLink, display\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, accuracy_score,\n                             f1_score, confusion_matrix, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION & DIRECTORY SETUP ─────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR          = 16_000\nWIN_SAMPLES = 32_000   # 2s @ 16kHz — HeAR hard constraint\nHOP_SAMPLES = 16_000   # 50% overlap for multi-window extraction\nEMBED_DIM   = 512\nN_SPLITS    = 5\nTARGET_SENS = [0.85, 0.90, 0.95]\nPAUC_LOW    = 0.85     \n\n# Aggregation stats per participant \nAGG_FUNCS   = [\"mean\", \"std\", \"p25\", \"p50\", \"p75\"]\nAGG_DIM     = EMBED_DIM * len(AGG_FUNCS)  # 2560\nEMB_NOISE_STD = 0.01\n\n# Output Directories\nOUT_ROOT   = \"/kaggle/working/outputs_v10\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"multiwindow_participant\")\nCACHE_DIR  = os.path.join(OUT_ROOT, \"cache\")\nMODEL_DIR  = os.path.join(OUT_ROOT, \"models\")\nEVAL_DIR   = os.path.join(OUT_ROOT, \"eval\")\n\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\", MODEL_DIR, EVAL_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nEMBED_CACHE = os.path.join(CACHE_DIR, \"hear_multiwindow_embeddings.parquet\")\n\n# ── 2. DATA LOADING & HARMONISATION ─────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio    = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\n# Clinical feature selection\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\n                \"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols = [], []\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols: continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\ncountry_col = None\nfor hint in [\"country\", \"site\", \"country_id\", \"collection_country\"]:\n    matches = [c for c in df_clinical.columns if hint in c.lower()]\n    if matches:\n        country_col = matches[0]\n        if country_col not in cat_cols: cat_cols.append(country_col)\n        break\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols],\n                          on=\"participant_id\", how=\"left\")\n\n# Audio file mapping\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(\n    lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. HeAR MODEL LOADING WITH KAGGLE AUTHENTICATION ───────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL (GATED REPO FIX)\")\nprint(\"=\"*60)\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login, from_pretrained_keras\nimport tensorflow as tf\n\ntry:\n    _sec = UserSecretsClient()\n    hf_token = _sec.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\nexcept Exception as e:\n    raise ValueError(\"Could not find HF_TOKEN in Kaggle Secrets.\") from e\n\nHEAR_MODEL   = from_pretrained_keras(\"google/hear\")\nHEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n\ndef _infer_batch(segments: list) -> np.ndarray:\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\n# ── 4. AUDIO PROCESSING & EMBEDDING ──────────────────────────────────────────\ndef load_audio(path: str):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef extract_windows(audio: np.ndarray) -> list:\n    if len(audio) == 0: return [np.zeros(WIN_SAMPLES, np.float32)]\n    if len(audio) < WIN_SAMPLES:\n        audio_mir = np.concatenate((audio, audio[::-1]))\n        repeats   = int(np.ceil(WIN_SAMPLES / len(audio_mir)))\n        return [np.tile(audio_mir, repeats)[:WIN_SAMPLES].astype(np.float32)]\n    windows = []\n    start   = 0\n    while start + WIN_SAMPLES <= len(audio):\n        windows.append(audio[start:start + WIN_SAMPLES].astype(np.float32))\n        start += HOP_SAMPLES\n    if start < len(audio):\n        windows.append(audio[len(audio) - WIN_SAMPLES:].astype(np.float32))\n    return windows if windows else [audio[:WIN_SAMPLES].astype(np.float32)]\n\ndef aggregate_embeddings(emb_matrix: np.ndarray) -> np.ndarray:\n    if emb_matrix.shape[0] == 1:\n        return np.concatenate([emb_matrix[0], np.zeros(EMBED_DIM * 4, np.float32)])\n    m, s = emb_matrix.mean(axis=0), emb_matrix.std(axis=0)\n    p25, p50, p75 = np.percentile(emb_matrix, [25, 50, 75], axis=0)\n    return np.concatenate([m, s, p25, p50, p75]).astype(np.float32)\n\nprint(\"\\n[*] Pre-fetching multi-window HeAR embeddings...\")\ndef get_multiwindow_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try:    cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n    else: cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n\n    N = len(df_rows)\n    agg = np.zeros((N, AGG_DIM), np.float32)\n    nw  = np.zeros(N, np.int32)\n    keys = [hashlib.md5(str(r.audio_path).encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n\n    BATCH = 64\n    buf_segs, buf_meta, new_entries = [], [], {}\n\n    for i, row in tqdm(need, desc=\"Extracting embeddings\", leave=False):\n        windows = extract_windows(load_audio(row.audio_path))\n        k = keys[i]\n        new_entries[k] = {\"n_windows\": len(windows), \"embs\": []}\n        for seg in windows:\n            buf_segs.append(seg); buf_meta.append(k)\n            if len(buf_segs) >= BATCH:\n                batch_embs = _infer_batch(buf_segs)\n                for bk, be in zip(buf_meta, batch_embs): new_entries[bk][\"embs\"].append(be)\n                buf_segs, buf_meta = [], []\n    if buf_segs:\n        batch_embs = _infer_batch(buf_segs)\n        for bk, be in zip(buf_meta, batch_embs): new_entries[bk][\"embs\"].append(be)\n\n    new_rows = []\n    for k, v in new_entries.items():\n        new_rows.append({\"key\": k, \"agg_embedding\": aggregate_embeddings(np.stack(v[\"embs\"])).tolist(), \"n_windows\": v[\"n_windows\"]})\n    \n    if new_rows:\n        cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n        cache.to_parquet(EMBED_CACHE, index=False)\n\n    cache_dict = {row[\"key\"]: row for _, row in cache.iterrows()}\n    for i in range(N):\n        if keys[i] in cache_dict:\n            r = cache_dict[keys[i]]\n            val = r[\"agg_embedding\"]\n            agg[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            nw[i]  = int(r[\"n_windows\"])\n    return agg, nw\n\nall_agg_embs, cough_df[\"n_cough_windows\"] = get_multiwindow_embeddings(cough_df)\n\n# Participant Aggregation\nprint(\"\\n[*] Aggregating to participant level...\")\nunique_pids = cough_df[\"participant_id\"].unique()\npid_records = []\nfor pid in unique_pids:\n    mask = cough_df[\"participant_id\"] == pid\n    first_row = cough_df.loc[mask].iloc[0]\n    rec = {\"participant_id\": pid, \"label\": cough_df.loc[mask, \"label\"].values[0], \n           \"n_recordings\": mask.sum(), \"n_cough_windows_total\": int(cough_df.loc[mask, \"n_cough_windows\"].sum())}\n    for col in num_cols + cat_cols: rec[col] = first_row.get(col, np.nan)\n    pid_records.append((rec, all_agg_embs[mask].mean(axis=0)))\n\nparticipant_df = pd.DataFrame([r for r, _ in pid_records]).reset_index(drop=True)\nparticipant_embs = np.stack([e for _, e in pid_records])\nnum_cols_p = num_cols + [\"n_recordings\", \"n_cough_windows_total\"]\n\n# ── 5. MODEL BUILDERS & HELPERS ──────────────────────────────────────────────\ndef build_meta_preprocessor(num_c, cat_c):\n    return ColumnTransformer([\n        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_c),\n        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_c)\n    ], remainder=\"drop\")\n\ndef add_embedding_noise(X): return X + np.random.normal(0, EMB_NOISE_STD, X.shape).astype(np.float32)\n\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB: return lgb.LGBMClassifier(n_estimators=400, learning_rate=0.02, num_leaves=31, colsample_bytree=0.4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\", max_iter=3000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB: return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.02, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\ndef build_supervisor(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB: return lgb.LGBMClassifier(n_estimators=100, learning_rate=0.02, num_leaves=7, max_depth=3, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight={0: 1.0, 1: scale}, max_iter=2000, random_state=SEED)\n\n# Fixed Partial AUC Formulation\ndef partial_auc(y_true, y_prob, low_tpr=PAUC_LOW):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    mask = tpr >= low_tpr\n    if mask.sum() < 2: return 0.0\n    sub_fpr, sub_tpr = fpr[mask], tpr[mask]\n    area = float(np.trapz(sub_tpr, sub_fpr))\n    # Correctly bounds area between 0.0 and 1.0 for the FPR domain measured\n    max_area = max(1e-9, 1.0 * (sub_fpr[-1] - sub_fpr[0])) \n    return area / max_area \n\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \n            \"sensitivity\": tp / (tp + fn + 1e-9), \"specificity\": tn / (tn + fp + 1e-9), \n            \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec, best_t = m[\"specificity\"], t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)), \n         \"pauc_85\": partial_auc(y_true, y_prob, 0.85), \"pauc_90\": partial_auc(y_true, y_prob, 0.90), \"tuned_thresholds\": {}}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\n# Plotting Helpers \ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n    axes[0].plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={roc_auc_score(y_true, y_prob):.3f}\")\n    axes[0].plot([0,1],[0,1],\"--\",color=\"gray\",lw=1)\n    axes[0].set(title=f\"{title_prefix} — Full ROC\", xlabel=\"FPR\", ylabel=\"TPR\")\n    axes[0].legend()\n    mask = tpr >= 0.85\n    axes[1].fill_between(fpr[mask], tpr[mask], 0.85, alpha=0.25, color=\"#457b9d\", label=f\"pAUC@85%={partial_auc(y_true,y_prob,0.85):.3f}\")\n    axes[1].plot(fpr, tpr, color=\"#e63946\", lw=2)\n    axes[1].set(xlim=[0, 1], ylim=[0.8, 1.0], title=\"Partial AUC (TPR≥85%)\", xlabel=\"FPR\", ylabel=\"TPR\")\n    axes[1].legend(fontsize=8)\n    fig.tight_layout()\n    fig.savefig(f\"{path_prefix}_roc.png\", dpi=150)\n    plt.close(fig)\n\ndef plot_confusion_matrix_sns(y_true, y_prob, path, threshold=0.5, title=\"Confusion Matrix\"):\n    y_pred = (np.array(y_prob) >= threshold).astype(int)\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n                xticklabels=['TB Negative', 'TB Positive'], yticklabels=['TB Negative', 'TB Positive'])\n    ax.set_title(title, fontweight='bold')\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\n    fig.tight_layout()\n    fig.savefig(path, dpi=150)\n    plt.close(fig)\n\n# ── 6. MAIN TRAINING & FOLD EXPORT LOOP ──────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. STARTING V10 TRAINING & WEIGHT EXPORT\")\nprint(\"=\"*60)\n\nsgkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(participant_df, participant_df[\"label\"]))\noof_stack = np.zeros(len(participant_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    df_tr_full, df_te = participant_df.iloc[tr_idx].reset_index(drop=True), participant_df.iloc[te_idx].reset_index(drop=True)\n    emb_tr_full, emb_te = participant_embs[tr_idx], participant_embs[te_idx]\n    y_tr_full, y_te = df_tr_full[\"label\"].values, df_te[\"label\"].values\n\n    cal_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n    tr_sub_idx, cal_idx = next(cal_fold.split(df_tr_full, y_tr_full))\n\n    df_tr, df_cal = df_tr_full.iloc[tr_sub_idx].reset_index(drop=True), df_tr_full.iloc[cal_idx].reset_index(drop=True)\n    emb_tr, emb_cal = emb_tr_full[tr_sub_idx], emb_tr_full[cal_idx]\n    y_tr, y_cal = df_tr[\"label\"].values, df_cal[\"label\"].values\n\n    meta_prep = build_meta_preprocessor(num_cols_p, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_cal_m, X_te_m = meta_prep.transform(df_cal), meta_prep.transform(df_te)\n    X_tr_emb, X_cal_emb, X_te_emb = add_embedding_noise(emb_tr), emb_cal, emb_te\n\n    inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n    tr_oof_a, tr_oof_m = np.zeros(len(y_tr)), np.zeros(len(y_tr))\n\n    for i_tr, i_val in inner_cv.split(X_tr_emb, y_tr):\n        n_p, n_n = int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())\n        clf_a_i = build_audio_expert(n_p, n_n)\n        clf_a_i.fit(add_embedding_noise(X_tr_emb[i_tr]), y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_i.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        clf_m_i = build_clinical_expert(n_p, n_n)\n        clf_m_i.fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_i.predict_proba(X_tr_m[i_val])[:, 1]\n\n    n_pos, n_neg = int(y_tr.sum()), int((y_tr==0).sum())\n    clf_a = build_audio_expert(n_pos, n_neg).fit(X_tr_emb, y_tr)\n    clf_m = build_clinical_expert(n_pos, n_neg).fit(X_tr_m, y_tr)\n\n    X_tr_stack  = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_cal_stack = np.column_stack([clf_a.predict_proba(X_cal_emb)[:,1], clf_m.predict_proba(X_cal_m)[:,1], X_cal_m])\n    X_te_stack  = np.column_stack([clf_a.predict_proba(X_te_emb)[:,1], clf_m.predict_proba(X_te_m)[:,1], X_te_m])\n\n    supervisor = build_supervisor(n_pos, n_neg).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\").fit(X_cal_stack, y_cal)\n\n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:, 1]\n    oof_stack[te_idx] = te_prob_stack\n    print(f\"[*] Fold {fold_i+1} | AUC={roc_auc_score(y_te, te_prob_stack):.4f} | pAUC@90%={partial_auc(y_te, te_prob_stack, 0.90):.4f}\")\n\n    # EXPORTING PER-FOLD WEIGHTS\n    fold_tag = f\"fold{fold_i+1}\"\n    joblib.dump(clf_a, os.path.join(MODEL_DIR, f\"{fold_tag}_audio_expert.pkl\"))\n    joblib.dump(clf_m, os.path.join(MODEL_DIR, f\"{fold_tag}_clinical_expert.pkl\"))\n    joblib.dump(supervisor, os.path.join(MODEL_DIR, f\"{fold_tag}_supervisor.pkl\"))\n    joblib.dump(cal_supervisor, os.path.join(MODEL_DIR, f\"{fold_tag}_calibrated_supervisor.pkl\"))\n    joblib.dump(meta_prep, os.path.join(MODEL_DIR, f\"{fold_tag}_meta_preprocessor.pkl\"))\n\n# ── 7. FINAL FULL-DATA MODEL DEPLOYMENT & EVALUATION ─────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"5. SAVING FINAL DEPLOYMENT MODEL & ARTIFACTS\")\nprint(\"=\"*60)\n\nparticipant_df[\"pred_stack\"] = oof_stack\nm_part = full_eval(participant_df[\"label\"], participant_df[\"pred_stack\"])\n\n# Save evaluation metrics as JSON / text\nwith open(os.path.join(EVAL_DIR, \"v10_metrics.json\"), \"w\") as f: json.dump(m_part, f, indent=2)\n\nsummary_rows = [{\"Model\": \"V10 (Multi-Window + Participant-Level)\", \"ROC-AUC\": f\"{m_part.get('roc_auc', 0):.4f}\", \n                 \"pAUC@85%\": f\"{m_part.get('pauc_85', 0):.4f}\", \"pAUC@90%\": f\"{m_part.get('pauc_90', 0):.4f}\",\n                 \"Spec@Sens=85%\": f\"{m_part.get('tuned_thresholds',{}).get('sens_85',{}).get('specificity',0):.4f}\",\n                 \"Spec@Sens=90%\": f\"{m_part.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\",\n                 \"Spec@Sens=95%\": f\"{m_part.get('tuned_thresholds',{}).get('sens_95',{}).get('specificity',0):.4f}\"}]\nsummary_df = pd.DataFrame(summary_rows)\n\nwith open(os.path.join(EVAL_DIR, \"v10_metrics_readable.txt\"), \"w\") as f:\n    f.write(\"V10 FINAL EVALUATION\\n\" + \"=\"*50 + \"\\n\" + summary_df.to_string(index=False) + \"\\n\\n\")\n    for sk, sv in m_part.get(\"tuned_thresholds\", {}).items():\n        f.write(f\"[{sk}] threshold={sv['threshold']:.4f}  sens={sv['sensitivity']:.3f}  spec={sv['specificity']:.3f}  f1={sv['f1']:.3f}\\n\")\n\n# Save Visual Plots\nplot_curves(participant_df[\"label\"], participant_df[\"pred_stack\"], f\"{FUSION_OUT}/plots/v10_participant\", \"V10 Participant-Level\")\nbest_t_90 = m_part[\"tuned_thresholds\"][\"sens_90\"][\"threshold\"]\nplot_confusion_matrix_sns(participant_df[\"label\"], participant_df[\"pred_stack\"], \n                          f\"{FUSION_OUT}/plots/v10_participant_cm.png\", threshold=best_t_90, title=f\"V10 CM (Sens ≥ 90%)\")\n\n# Train ONE final deployable model on 100% of data\nprint(\"[*] Training final full-data models for deployment...\")\ndf_all, y_all, X_all_emb = participant_df.copy().reset_index(drop=True), participant_df[\"label\"].values, participant_embs.copy()\n\ncal_fold_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\ntr_sub_idx_final, cal_idx_final = next(cal_fold_final.split(df_all, y_all))\ndf_tr_final, df_cal_final = df_all.iloc[tr_sub_idx_final].reset_index(drop=True), df_all.iloc[cal_idx_final].reset_index(drop=True)\nX_tr_emb_final, X_cal_emb_final = X_all_emb[tr_sub_idx_final], X_all_emb[cal_idx_final]\ny_tr_final, y_cal_final = df_tr_final[\"label\"].values, df_cal_final[\"label\"].values\n\nmeta_prep_final = build_meta_preprocessor(num_cols_p, cat_cols)\nX_tr_m_final, X_cal_m_final, X_all_m_final = meta_prep_final.fit_transform(df_tr_final), meta_prep_final.transform(df_cal_final), meta_prep_final.transform(df_all)\n\nX_tr_emb_final_noisy, X_cal_emb_final_clean, X_all_emb_clean = add_embedding_noise(X_tr_emb_final), X_cal_emb_final, X_all_emb\ninner_cv_final = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\ntr_oof_a_final, tr_oof_m_final = np.zeros(len(y_tr_final)), np.zeros(len(y_tr_final))\n\nfor i_tr, i_val in inner_cv_final.split(X_tr_emb_final_noisy, y_tr_final):\n    n_p_i, n_n_i = int(y_tr_final[i_tr].sum()), int((y_tr_final[i_tr] == 0).sum())\n    clf_a_i = build_audio_expert(n_p_i, n_n_i).fit(add_embedding_noise(X_tr_emb_final_noisy[i_tr]), y_tr_final[i_tr])\n    tr_oof_a_final[i_val] = clf_a_i.predict_proba(X_tr_emb_final_noisy[i_val])[:, 1]\n    clf_m_i = build_clinical_expert(n_p_i, n_n_i).fit(X_tr_m_final[i_tr], y_tr_final[i_tr])\n    tr_oof_m_final[i_val] = clf_m_i.predict_proba(X_tr_m_final[i_val])[:, 1]\n\nn_pos_all, n_neg_all = int(y_tr_final.sum()), int((y_tr_final == 0).sum())\nclf_a_final = build_audio_expert(n_pos_all, n_neg_all).fit(X_tr_emb_final_noisy, y_tr_final)\nclf_m_final = build_clinical_expert(n_pos_all, n_neg_all).fit(X_tr_m_final, y_tr_final)\n\nX_tr_stack_final  = np.column_stack([tr_oof_a_final, tr_oof_m_final, X_tr_m_final])\nX_cal_stack_final = np.column_stack([clf_a_final.predict_proba(X_cal_emb_final_clean)[:, 1], clf_m_final.predict_proba(X_cal_m_final)[:, 1], X_cal_m_final])\n\nsupervisor_final = build_supervisor(n_pos_all, n_neg_all).fit(X_tr_stack_final, y_tr_final)\ncal_supervisor_final = CalibratedClassifierCV(supervisor_final, cv=\"prefit\", method=\"sigmoid\").fit(X_cal_stack_final, y_cal_final)\n\n# EXPORTING MASTER DEPLOYMENT WEIGHTS\njoblib.dump(meta_prep_final,        os.path.join(MODEL_DIR, \"final_meta_preprocessor.pkl\"))\njoblib.dump(clf_a_final,            os.path.join(MODEL_DIR, \"final_audio_expert.pkl\"))\njoblib.dump(clf_m_final,            os.path.join(MODEL_DIR, \"final_clinical_expert.pkl\"))\njoblib.dump(supervisor_final,       os.path.join(MODEL_DIR, \"final_supervisor.pkl\"))\njoblib.dump(cal_supervisor_final,   os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\"))\n\ndeploy_info = {\n    \"model_name\": \"TB SCREENING RANKER V10\",\n    \"seed\": SEED, \"sr\": SR, \"win_samples\": WIN_SAMPLES, \"hop_samples\": HOP_SAMPLES,\n    \"embed_dim\": EMBED_DIM, \"agg_funcs\": AGG_FUNCS, \"agg_dim\": AGG_DIM,\n    \"num_cols_p\": num_cols_p, \"cat_cols\": cat_cols,\n    \"target_sens_thresholds\": {k: float(v[\"threshold\"]) for k, v in m_part.get(\"tuned_thresholds\", {}).items()}\n}\nwith open(os.path.join(MODEL_DIR, \"final_inference_config.json\"), \"w\") as f: json.dump(deploy_info, f, indent=2)\n\nsummary_df.to_csv(f\"{FUSION_OUT}/v10_summary.csv\", index=False)\nparticipant_df.to_csv(f\"{FUSION_OUT}/v10_oof_predictions.csv\", index=False)\n\n# ZIP EVERYTHING FOR DOWNLOAD\nzip_path = \"/kaggle/working/outputs_v10.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUT_ROOT):\n        for fn in files: zf.write(os.path.join(root, fn), os.path.relpath(os.path.join(root, fn), \"/kaggle/working\"))\n\nprint(\"\\n✅ V10 PIPELINE COMPLETE & ZIPPED!\")\nprint(\"\\n--- DOWNLOAD LINKS ---\")\ndisplay(FileLink(zip_path))\ndisplay(FileLink(f\"{FUSION_OUT}/plots/v10_participant_roc.png\"))\ndisplay(FileLink(f\"{FUSION_OUT}/plots/v10_participant_cm.png\"))\ndisplay(FileLink(os.path.join(EVAL_DIR, \"v10_metrics_readable.txt\")))\ndisplay(FileLink(os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\")))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n# Must be set to JAX BEFORE importing keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Cap JAX memory so it doesn't crash Pandas/LightGBM\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.85\" \n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport keras\nimport keras_hub\n\n# 1. CRITICAL: Force Keras to use half-precision. \n# This shrinks MedGemma 4B from 16GB to 8GB so it fits on a Kaggle T4!\nkeras.config.set_floatx(\"float16\")\n\n# ============================================================================\n# 2. SETUP: LOAD THE V10 ML MODELS\n# ============================================================================\nprint(\"Loading V10 ML Models...\")\nMODEL_DIR = \"/kaggle/input/datasets/jroot888/medgemma-trained-weights/outputs_v10/models\"\n\nmeta_prep      = joblib.load(os.path.join(MODEL_DIR, \"final_meta_preprocessor.pkl\"))\nclf_a          = joblib.load(os.path.join(MODEL_DIR, \"final_audio_expert.pkl\"))\nclf_m          = joblib.load(os.path.join(MODEL_DIR, \"final_clinical_expert.pkl\"))\ncal_supervisor = joblib.load(os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\"))\n\n# ============================================================================\n# 3. SIMULATE A WAITING ROOM (Standalone Version)\n# ============================================================================\nprint(\"Simulating Clinic Waiting Room...\")\n# Load actual clinical data from your V10 output CSV\ncsv_path = \"/kaggle/input/datasets/jroot888/medgemma-trained-weights/outputs_v10/multiwindow_participant/v10_oof_predictions.csv\"\ndf_all = pd.read_csv(csv_path)\ndf_waiting_room = df_all.sample(10, random_state=99).reset_index(drop=True)\n\n# Generate dummy HeAR embeddings (2560-dim) just for this UI/Agent test\n# (In your real deployment later, this will come from the live microphone audio)\nX_audio_waiting_room = np.random.randn(10, 2560).astype(np.float32)\n\n# ============================================================================\n# 4. THE RANKING ENGINE\n# ============================================================================\ndef rank_patients(df_meta, X_audio):\n    X_m_processed = meta_prep.transform(df_meta)\n    \n    prob_a = clf_a.predict_proba(X_audio)[:, 1]\n    prob_m = clf_m.predict_proba(X_m_processed)[:, 1]\n    \n    X_stack = np.column_stack([prob_a, prob_m, X_m_processed])\n    final_scores = cal_supervisor.predict_proba(X_stack)[:, 1]\n    \n    desired_cols = ['participant_id', 'age', 'sex', 'weight_loss', 'night_sweats']\n    display_cols = [c for c in desired_cols if c in df_meta.columns]\n    \n    df_queue = df_meta[display_cols].copy()\n    df_queue['audio_risk']  = np.round(prob_a, 3)\n    df_queue['clinic_risk'] = np.round(prob_m, 3)\n    df_queue['final_triage_score'] = np.round(final_scores, 3)\n    \n    df_queue = df_queue.sort_values(by='final_triage_score', ascending=False).reset_index(drop=True)\n    df_queue.index = df_queue.index + 1 \n    return df_queue\n\nranked_queue = rank_patients(df_waiting_room, X_audio_waiting_room)\nprint(\"\\n--- SORTED TRIAGE QUEUE ---\")\nprint(ranked_queue[['participant_id', 'audio_risk', 'clinic_risk', 'final_triage_score']])\n\n# ============================================================================\n# 5. LOAD MEDGEMMA 4B (Memory-Optimized)\n# ============================================================================\nprint(\"\\nLoading MedGemma 4B into GPU... (This takes a minute)\")\nMEDGEMMA_PATH = \"/kaggle/input/models/keras/medgemma/keras/medgemma_4b/1\"\n\ntry:\n    # dtype=\"float16\" combined with set_floatx prevents the OOM crash\n    medgemma = keras_hub.models.CausalLM.from_preset(MEDGEMMA_PATH, dtype=\"float16\")\n    medgemma.compile(sampler=\"greedy\") \n    print(\"✓ MedGemma 4B Loaded Successfully!\")\nexcept Exception as e:\n    print(f\"⚠ Warning: MedGemma failed to load.\\nError: {e}\")\n    medgemma = None \n\n# ============================================================================\n# 6. EXECUTE THE AGENTIC WORKFLOW\n# ============================================================================\ndef generate_clinical_justification(patient_row):\n    age = patient_row.get('age', 'Unknown')\n    sex = patient_row.get('sex', 'Unknown')\n    weight_loss = patient_row.get('weight_loss', 'Unknown')\n    night_sweats = patient_row.get('night_sweats', 'Unknown')\n    \n    prompt = (\n        f\"You are an expert AI Triage Assistant in a tuberculosis clinic. \"\n        f\"You have limited GeneXpert tests available today.\\n\\n\"\n        f\"PATIENT DATA:\\n\"\n        f\"- Demographics: {age} year old {sex}.\\n\"\n        f\"- Symptoms: Weight loss is {weight_loss}, Night Sweats is {night_sweats}.\\n\"\n        f\"- AI Assessment: Acoustic Cough Risk Score is {patient_row['audio_risk']} out of 1.0. \"\n        f\"Clinical Risk Score is {patient_row['clinic_risk']} out of 1.0.\\n\\n\"\n        f\"TASK: Write a concise, professional 2-sentence clinical justification explaining why this patient \"\n        f\"has been assigned a high priority triage rank for a GeneXpert test today.\\n\\n\"\n        f\"JUSTIFICATION:\\n\"\n    )\n    \n    if medgemma is None: return \"LLM Not Loaded.\"\n    \n    response = medgemma.generate(prompt, max_length=256)\n    justification = response.replace(prompt, \"\").strip()\n    return justification\n\nprint(\"\\n--- MEDGEMMA CLINICAL EXPLANATIONS FOR TOP 3 PATIENTS ---\")\nfor rank, patient in ranked_queue.head(3).iterrows():\n    print(f\"\\nEvaluating Rank #{rank} (ID: {patient.get('participant_id', 'Unknown')})\")\n    print(\"Generating LLM Justification...\")\n    explanation = generate_clinical_justification(patient)\n    print(f\"MEDGEMMA OUTPUT:\\n{explanation}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n=====================================================================================\nTB SCREENING RANKER: MASTER TRAINING & EXPORT PIPELINE (VERSION 10)\n=====================================================================================\n\nDESCRIPTION:\nThis script is the final, production-ready build of the CODA-TB screening pipeline. \nIt trains decoupled models (Audio and Metadata) using an Out-of-Fold (OOF) Stacking \narchitecture. It rigorously evaluates the models using Partial AUC and automatically \nexports all weights, plots, and evaluation metrics for cloud deployment.\n\nKEY FEATURES IN V10:\n1. Robust Multi-Window Audio: Dynamically slices audio of any length into overlapping \n   2-second windows, extracting a rich 2560-dim acoustic fingerprint via Google HeAR.\n2. Decoupled Experts: Trains independent LightGBM models for Audio and Clinical Metadata.\n3. Master Supervisor: Fuses the independent probabilities to mathematically rank patients.\n4. Deployment Ready: Trains a final 100%-data master model and exports all `.pkl` \n   weights alongside a downloadable ZIP archive.\n=====================================================================================\n\"\"\"\n\nimport os, sys, json, warnings, random, hashlib, zipfile, shutil\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport matplotlib; matplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import FileLink, display\n\nwarnings.filterwarnings(\"ignore\")\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\n\nimport sklearn, librosa, joblib\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import (roc_auc_score, accuracy_score,\n                             f1_score, confusion_matrix, roc_curve)\n\ntry:\n    import lightgbm as lgb; HAS_LGB = True\nexcept ImportError:\n    HAS_LGB = False\n\n# ── 1. CONFIGURATION & DIRECTORY SETUP ─────────────────────────────────────────\nBASE       = \"/kaggle/input/tb-audio/Tuberculosis\"\nMETA       = f\"{BASE}/metadata\"\nAUDIO_BASE = f\"{BASE}/raw_data/solicited_data\"\n\nCLINICAL_CSV  = f\"{META}/CODA_TB_Clinical_Meta_Info.csv\"\nSOLICITED_CSV = f\"{META}/CODA_TB_Solicited_Meta_Info.csv\"\n\nSR          = 16_000\nWIN_SAMPLES = 32_000   # 2s @ 16kHz — HeAR hard constraint\nHOP_SAMPLES = 16_000   # 50% overlap for multi-window extraction\nEMBED_DIM   = 512\nN_SPLITS    = 5\nTARGET_SENS = [0.85, 0.90, 0.95]\nPAUC_LOW    = 0.85     \n\n# Aggregation stats per participant \nAGG_FUNCS   = [\"mean\", \"std\", \"p25\", \"p50\", \"p75\"]\nAGG_DIM     = EMBED_DIM * len(AGG_FUNCS)  # 2560\nEMB_NOISE_STD = 0.01\n\n# Output Directories\nOUT_ROOT   = \"/kaggle/working/outputs_v10\"\nFUSION_OUT = os.path.join(OUT_ROOT, \"multiwindow_participant\")\nCACHE_DIR  = os.path.join(OUT_ROOT, \"cache\")\nMODEL_DIR  = os.path.join(OUT_ROOT, \"models\")\nEVAL_DIR   = os.path.join(OUT_ROOT, \"eval\")\n\nfor d in [FUSION_OUT, CACHE_DIR, f\"{FUSION_OUT}/plots\", MODEL_DIR, EVAL_DIR]:\n    os.makedirs(d, exist_ok=True)\n\nEMBED_CACHE = os.path.join(CACHE_DIR, \"hear_multiwindow_embeddings.parquet\")\n\n# ── 2. DATA LOADING & HARMONISATION ─────────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"1. LOADING & HARMONISING DATA\")\nprint(\"=\"*60)\n\ndef harmonise_cols(df):\n    rename = {}\n    cols_lc = {c.lower(): c for c in df.columns}\n    for hint in [\"participant_id\",\"participant\",\"subject_id\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"participant_id\"; break\n    for hint in [\"filename\",\"file_name\",\"audio_file\",\"wav_file\",\"cough_file\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"filename\"; break\n    for hint in [\"tb_status\",\"tb\",\"label\",\"target\",\"tb_result\"]:\n        if hint in cols_lc: rename[cols_lc[hint]] = \"label_raw\"; break\n    return df.rename(columns=rename)\n\ndef binarise_label(series):\n    def _b(v):\n        if pd.isna(v): return np.nan\n        s = str(v).strip().lower()\n        if s in (\"1\",\"yes\",\"positive\",\"tb+\",\"tb_positive\",\"true\",\"pos\"): return 1\n        if s in (\"0\",\"no\",\"negative\",\"tb-\",\"tb_negative\",\"false\",\"neg\"): return 0\n        try: return int(float(s))\n        except: return np.nan\n    return series.apply(_b)\n\ndf_audio    = harmonise_cols(pd.read_csv(SOLICITED_CSV))\ndf_clinical = harmonise_cols(pd.read_csv(CLINICAL_CSV))\n\nif \"label_raw\" not in df_audio.columns and \"label_raw\" in df_clinical.columns:\n    df_audio = df_audio.merge(df_clinical[[\"participant_id\", \"label_raw\"]], on=\"participant_id\", how=\"left\")\n\ndf_audio[\"label\"] = binarise_label(df_audio[\"label_raw\"])\ndf_audio = df_audio.dropna(subset=[\"label\"]).reset_index(drop=True)\ndf_audio[\"label\"] = df_audio[\"label\"].astype(int)\n\n# Clinical feature selection & Auditing\nPOST_DIAG_KW = [\"sputum\",\"culture\",\"smear\",\"xpert\",\"dst\",\"microscopy\",\"molecular\",\n                \"confirmatory\",\"tb_status\",\"label\"]\nskip_cols = set(POST_DIAG_KW) | {\"participant_id\"}\nnum_cols, cat_cols, dropped_cols = [], [], []\n\nprint(\"\\n\" + \"-\"*50)\nprint(\"🔍 FEATURE AUDIT: METADATA & CLINICAL VARIABLES\")\nprint(\"-\"*50)\nprint(f\"[*] Total Available Features in Metadata ({len(df_clinical.columns)}):\")\nprint(list(df_clinical.columns))\n\nfor c in df_clinical.columns:\n    if any(kw in c.lower() for kw in POST_DIAG_KW) or c in skip_cols:\n        dropped_cols.append(c)\n        continue\n    if df_clinical[c].dtype in (np.float64, np.float32, np.int64, np.int32): num_cols.append(c)\n    else: cat_cols.append(c)\n\nprint(f\"\\n[*] Excluded Features (Data Leakage/IDs) ({len(dropped_cols)}):\")\nprint(dropped_cols)\n\ncountry_col = None\nfor hint in [\"country\", \"site\", \"country_id\", \"collection_country\"]:\n    matches = [c for c in df_clinical.columns if hint in c.lower()]\n    if matches:\n        country_col = matches[0]\n        if country_col not in cat_cols: cat_cols.append(country_col)\n        break\n\ncough_df = df_audio.merge(df_clinical[[\"participant_id\"] + num_cols + cat_cols],\n                          on=\"participant_id\", how=\"left\")\n\n# Audio file mapping\nlookup = {}\nfor dirpath, _, fns in os.walk(AUDIO_BASE):\n    for fn in fns:\n        if fn.lower().endswith((\".wav\",\".ogg\",\".flac\",\".mp3\")):\n            lookup[fn] = os.path.join(dirpath, fn)\n            lookup[os.path.splitext(fn)[0]] = os.path.join(dirpath, fn)\n\ncough_df[\"audio_path\"] = cough_df[\"filename\"].apply(\n    lambda x: lookup.get(str(x), lookup.get(os.path.splitext(str(x))[0], np.nan)))\ncough_df = cough_df.dropna(subset=[\"audio_path\"]).reset_index(drop=True)\n\n# ── 3. HeAR MODEL LOADING WITH KAGGLE AUTHENTICATION ───────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"2. LOADING GOOGLE HeAR MODEL (GATED REPO FIX)\")\nprint(\"=\"*60)\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login, from_pretrained_keras\nimport tensorflow as tf\n\ntry:\n    _sec = UserSecretsClient()\n    hf_token = _sec.get_secret(\"HF_TOKEN\")\n    login(token=hf_token)\nexcept Exception as e:\n    raise ValueError(\"Could not find HF_TOKEN in Kaggle Secrets.\") from e\n\nHEAR_MODEL   = from_pretrained_keras(\"google/hear\")\nHEAR_SERVING = HEAR_MODEL.signatures[\"serving_default\"]\n\ndef _infer_batch(segments: list) -> np.ndarray:\n    x = tf.constant(np.stack(segments), dtype=tf.float32)\n    return list(HEAR_SERVING(x=x).values())[0].numpy().astype(np.float32)\n\n# ── 4. AUDIO PROCESSING & EMBEDDING ──────────────────────────────────────────\ndef load_audio(path: str):\n    try:\n        audio, _ = librosa.load(str(path), sr=SR, mono=True)\n        return audio\n    except:\n        return np.zeros(WIN_SAMPLES, np.float32)\n\ndef extract_windows(audio: np.ndarray) -> list:\n    if len(audio) == 0: return [np.zeros(WIN_SAMPLES, np.float32)]\n    if len(audio) < WIN_SAMPLES:\n        audio_mir = np.concatenate((audio, audio[::-1]))\n        repeats   = int(np.ceil(WIN_SAMPLES / len(audio_mir)))\n        return [np.tile(audio_mir, repeats)[:WIN_SAMPLES].astype(np.float32)]\n    windows = []\n    start   = 0\n    while start + WIN_SAMPLES <= len(audio):\n        windows.append(audio[start:start + WIN_SAMPLES].astype(np.float32))\n        start += HOP_SAMPLES\n    if start < len(audio):\n        windows.append(audio[len(audio) - WIN_SAMPLES:].astype(np.float32))\n    return windows if windows else [audio[:WIN_SAMPLES].astype(np.float32)]\n\ndef aggregate_embeddings(emb_matrix: np.ndarray) -> np.ndarray:\n    if emb_matrix.shape[0] == 1:\n        return np.concatenate([emb_matrix[0], np.zeros(EMBED_DIM * 4, np.float32)])\n    m, s = emb_matrix.mean(axis=0), emb_matrix.std(axis=0)\n    p25, p50, p75 = np.percentile(emb_matrix, [25, 50, 75], axis=0)\n    return np.concatenate([m, s, p25, p50, p75]).astype(np.float32)\n\nprint(\"\\n[*] Pre-fetching multi-window HeAR embeddings...\")\ndef get_multiwindow_embeddings(df_rows):\n    if os.path.exists(EMBED_CACHE):\n        try:    cache = pd.read_parquet(EMBED_CACHE)\n        except: cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n    else: cache = pd.DataFrame(columns=[\"key\",\"agg_embedding\",\"n_windows\"])\n\n    N = len(df_rows)\n    agg = np.zeros((N, AGG_DIM), np.float32)\n    nw  = np.zeros(N, np.int32)\n    keys = [hashlib.md5(str(r.audio_path).encode()).hexdigest() for _, r in df_rows.iterrows()]\n    cached_keys = set(cache[\"key\"].tolist()) if not cache.empty else set()\n    need = [(i, row) for i, (_, row) in enumerate(df_rows.iterrows()) if keys[i] not in cached_keys]\n\n    BATCH = 64\n    buf_segs, buf_meta, new_entries = [], [], {}\n\n    for i, row in tqdm(need, desc=\"Extracting embeddings\", leave=False):\n        windows = extract_windows(load_audio(row.audio_path))\n        k = keys[i]\n        new_entries[k] = {\"n_windows\": len(windows), \"embs\": []}\n        for seg in windows:\n            buf_segs.append(seg); buf_meta.append(k)\n            if len(buf_segs) >= BATCH:\n                batch_embs = _infer_batch(buf_segs)\n                for bk, be in zip(buf_meta, batch_embs): new_entries[bk][\"embs\"].append(be)\n                buf_segs, buf_meta = [], []\n    if buf_segs:\n        batch_embs = _infer_batch(buf_segs)\n        for bk, be in zip(buf_meta, batch_embs): new_entries[bk][\"embs\"].append(be)\n\n    new_rows = []\n    for k, v in new_entries.items():\n        new_rows.append({\"key\": k, \"agg_embedding\": aggregate_embeddings(np.stack(v[\"embs\"])).tolist(), \"n_windows\": v[\"n_windows\"]})\n    \n    if new_rows:\n        cache = pd.concat([cache, pd.DataFrame(new_rows)], ignore_index=True)\n        cache.to_parquet(EMBED_CACHE, index=False)\n\n    cache_dict = {row[\"key\"]: row for _, row in cache.iterrows()}\n    for i in range(N):\n        if keys[i] in cache_dict:\n            r = cache_dict[keys[i]]\n            val = r[\"agg_embedding\"]\n            agg[i] = np.array(val, np.float32) if not isinstance(val, np.ndarray) else val\n            nw[i]  = int(r[\"n_windows\"])\n    return agg, nw\n\nall_agg_embs, cough_df[\"n_cough_windows\"] = get_multiwindow_embeddings(cough_df)\n\n# Participant Aggregation\nprint(\"\\n[*] Aggregating to participant level...\")\nunique_pids = cough_df[\"participant_id\"].unique()\npid_records = []\nfor pid in unique_pids:\n    mask = cough_df[\"participant_id\"] == pid\n    first_row = cough_df.loc[mask].iloc[0]\n    rec = {\"participant_id\": pid, \"label\": cough_df.loc[mask, \"label\"].values[0], \n           \"n_recordings\": mask.sum(), \"n_cough_windows_total\": int(cough_df.loc[mask, \"n_cough_windows\"].sum())}\n    for col in num_cols + cat_cols: rec[col] = first_row.get(col, np.nan)\n    pid_records.append((rec, all_agg_embs[mask].mean(axis=0)))\n\nparticipant_df = pd.DataFrame([r for r, _ in pid_records]).reset_index(drop=True)\nparticipant_embs = np.stack([e for _, e in pid_records])\nnum_cols_p = num_cols + [\"n_recordings\", \"n_cough_windows_total\"]\n\nprint(f\"\\n[*] Final USED Numerical Features ({len(num_cols_p)}):\")\nprint(num_cols_p)\nprint(f\"\\n[*] Final USED Categorical Features ({len(cat_cols)}):\")\nprint(cat_cols)\nprint(\"-\" * 50)\n\n# ── 5. MODEL BUILDERS & HELPERS ──────────────────────────────────────────────\ndef build_meta_preprocessor(num_c, cat_c):\n    return ColumnTransformer([\n        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)), (\"sc\", StandardScaler())]), num_c),\n        (\"cat\", Pipeline([(\"imp\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))]), cat_c)\n    ], remainder=\"drop\")\n\ndef add_embedding_noise(X): return X + np.random.normal(0, EMB_NOISE_STD, X.shape).astype(np.float32)\n\ndef build_audio_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB: return lgb.LGBMClassifier(n_estimators=400, learning_rate=0.02, num_leaves=31, colsample_bytree=0.4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\", max_iter=3000)\n\ndef build_clinical_expert(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB: return lgb.LGBMClassifier(n_estimators=200, learning_rate=0.02, num_leaves=15, max_depth=4, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight=\"balanced\")\n\ndef build_supervisor(n_pos, n_neg):\n    scale = n_neg / max(n_pos, 1)\n    if HAS_LGB: return lgb.LGBMClassifier(n_estimators=100, learning_rate=0.02, num_leaves=7, max_depth=3, scale_pos_weight=scale, random_state=SEED, verbose=-1, n_jobs=-1)\n    return LogisticRegression(class_weight={0: 1.0, 1: scale}, max_iter=2000, random_state=SEED)\n\n# Fixed Partial AUC Formulation\ndef partial_auc(y_true, y_prob, low_tpr=PAUC_LOW):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    mask = tpr >= low_tpr\n    if mask.sum() < 2: return 0.0\n    sub_fpr, sub_tpr = fpr[mask], tpr[mask]\n    area = float(np.trapz(sub_tpr, sub_fpr))\n    # Correctly bounds area between 0.0 and 1.0 for the FPR domain measured\n    max_area = max(1e-9, 1.0 * (sub_fpr[-1] - sub_fpr[0])) \n    return area / max_area \n\ndef metrics_at_thresh(y_true, y_prob, t=0.5):\n    y_pred = (np.array(y_prob) >= t).astype(int)\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n    return {\"threshold\": float(t), \"accuracy\": float(accuracy_score(y_true, y_pred)), \n            \"sensitivity\": tp / (tp + fn + 1e-9), \"specificity\": tn / (tn + fp + 1e-9), \n            \"f1\": float(f1_score(y_true, y_pred, zero_division=0))}\n\ndef find_thresh_for_sens(y_true, y_prob, target):\n    thresholds = np.sort(np.unique(np.round(y_prob, 4)))[::-1]\n    best_t, best_spec = 0.0, 0.0\n    for t in thresholds:\n        m = metrics_at_thresh(y_true, y_prob, t)\n        if m[\"sensitivity\"] >= target and m[\"specificity\"] >= best_spec:\n            best_spec, best_t = m[\"specificity\"], t\n    return float(best_t)\n\ndef full_eval(y_true, y_prob):\n    m = {\"roc_auc\": float(roc_auc_score(y_true, y_prob)), \n         \"pauc_85\": partial_auc(y_true, y_prob, 0.85), \"pauc_90\": partial_auc(y_true, y_prob, 0.90), \"tuned_thresholds\": {}}\n    for ts in TARGET_SENS:\n        t = find_thresh_for_sens(y_true, y_prob, ts)\n        m[\"tuned_thresholds\"][f\"sens_{int(ts*100)}\"] = {\"threshold\": t, **metrics_at_thresh(y_true, y_prob, t)}\n    return m\n\n# Plotting Helpers \ndef plot_curves(y_true, y_prob, path_prefix, title_prefix):\n    fpr, tpr, _ = roc_curve(y_true, y_prob)\n    fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n    axes[0].plot(fpr, tpr, color=\"#e63946\", lw=2, label=f\"AUC={roc_auc_score(y_true, y_prob):.3f}\")\n    axes[0].plot([0,1],[0,1],\"--\",color=\"gray\",lw=1)\n    axes[0].set(title=f\"{title_prefix} — Full ROC\", xlabel=\"FPR\", ylabel=\"TPR\")\n    axes[0].legend()\n    mask = tpr >= 0.85\n    axes[1].fill_between(fpr[mask], tpr[mask], 0.85, alpha=0.25, color=\"#457b9d\", label=f\"pAUC@85%={partial_auc(y_true,y_prob,0.85):.3f}\")\n    axes[1].plot(fpr, tpr, color=\"#e63946\", lw=2)\n    axes[1].set(xlim=[0, 1], ylim=[0.8, 1.0], title=\"Partial AUC (TPR≥85%)\", xlabel=\"FPR\", ylabel=\"TPR\")\n    axes[1].legend(fontsize=8)\n    fig.tight_layout()\n    fig.savefig(f\"{path_prefix}_roc.png\", dpi=150)\n    plt.close(fig)\n\ndef plot_confusion_matrix_sns(y_true, y_prob, path, threshold=0.5, title=\"Confusion Matrix\"):\n    y_pred = (np.array(y_prob) >= threshold).astype(int)\n    cm = confusion_matrix(y_true, y_pred)\n    fig, ax = plt.subplots(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n                xticklabels=['TB Negative', 'TB Positive'], yticklabels=['TB Negative', 'TB Positive'])\n    ax.set_title(title, fontweight='bold')\n    ax.set_ylabel('True Label')\n    ax.set_xlabel('Predicted Label')\n    fig.tight_layout()\n    fig.savefig(path, dpi=150)\n    plt.close(fig)\n\n# ── 6. MAIN TRAINING & FOLD EXPORT LOOP ──────────────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"4. STARTING V10 TRAINING & WEIGHT EXPORT\")\nprint(\"=\"*60)\n\nsgkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfolds = list(sgkf.split(participant_df, participant_df[\"label\"]))\noof_stack = np.zeros(len(participant_df))\n\nfor fold_i, (tr_idx, te_idx) in enumerate(folds):\n    print(f\"\\n--- FOLD {fold_i+1}/{N_SPLITS} ---\")\n    df_tr_full, df_te = participant_df.iloc[tr_idx].reset_index(drop=True), participant_df.iloc[te_idx].reset_index(drop=True)\n    emb_tr_full, emb_te = participant_embs[tr_idx], participant_embs[te_idx]\n    y_tr_full, y_te = df_tr_full[\"label\"].values, df_te[\"label\"].values\n\n    cal_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n    tr_sub_idx, cal_idx = next(cal_fold.split(df_tr_full, y_tr_full))\n\n    df_tr, df_cal = df_tr_full.iloc[tr_sub_idx].reset_index(drop=True), df_tr_full.iloc[cal_idx].reset_index(drop=True)\n    emb_tr, emb_cal = emb_tr_full[tr_sub_idx], emb_tr_full[cal_idx]\n    y_tr, y_cal = df_tr[\"label\"].values, df_cal[\"label\"].values\n\n    meta_prep = build_meta_preprocessor(num_cols_p, cat_cols)\n    X_tr_m = meta_prep.fit_transform(df_tr)\n    X_cal_m, X_te_m = meta_prep.transform(df_cal), meta_prep.transform(df_te)\n    X_tr_emb, X_cal_emb, X_te_emb = add_embedding_noise(emb_tr), emb_cal, emb_te\n\n    inner_cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\n    tr_oof_a, tr_oof_m = np.zeros(len(y_tr)), np.zeros(len(y_tr))\n\n    for i_tr, i_val in inner_cv.split(X_tr_emb, y_tr):\n        n_p, n_n = int(y_tr[i_tr].sum()), int((y_tr[i_tr]==0).sum())\n        clf_a_i = build_audio_expert(n_p, n_n)\n        clf_a_i.fit(add_embedding_noise(X_tr_emb[i_tr]), y_tr[i_tr])\n        tr_oof_a[i_val] = clf_a_i.predict_proba(X_tr_emb[i_val])[:, 1]\n        \n        clf_m_i = build_clinical_expert(n_p, n_n)\n        clf_m_i.fit(X_tr_m[i_tr], y_tr[i_tr])\n        tr_oof_m[i_val] = clf_m_i.predict_proba(X_tr_m[i_val])[:, 1]\n\n    n_pos, n_neg = int(y_tr.sum()), int((y_tr==0).sum())\n    clf_a = build_audio_expert(n_pos, n_neg).fit(X_tr_emb, y_tr)\n    clf_m = build_clinical_expert(n_pos, n_neg).fit(X_tr_m, y_tr)\n\n    X_tr_stack  = np.column_stack([tr_oof_a, tr_oof_m, X_tr_m])\n    X_cal_stack = np.column_stack([clf_a.predict_proba(X_cal_emb)[:,1], clf_m.predict_proba(X_cal_m)[:,1], X_cal_m])\n    X_te_stack  = np.column_stack([clf_a.predict_proba(X_te_emb)[:,1], clf_m.predict_proba(X_te_m)[:,1], X_te_m])\n\n    supervisor = build_supervisor(n_pos, n_neg).fit(X_tr_stack, y_tr)\n    cal_supervisor = CalibratedClassifierCV(supervisor, cv=\"prefit\", method=\"sigmoid\").fit(X_cal_stack, y_cal)\n\n    te_prob_stack = cal_supervisor.predict_proba(X_te_stack)[:, 1]\n    oof_stack[te_idx] = te_prob_stack\n    print(f\"[*] Fold {fold_i+1} | AUC={roc_auc_score(y_te, te_prob_stack):.4f} | pAUC@90%={partial_auc(y_te, te_prob_stack, 0.90):.4f}\")\n\n    # EXPORTING PER-FOLD WEIGHTS\n    fold_tag = f\"fold{fold_i+1}\"\n    joblib.dump(clf_a, os.path.join(MODEL_DIR, f\"{fold_tag}_audio_expert.pkl\"))\n    joblib.dump(clf_m, os.path.join(MODEL_DIR, f\"{fold_tag}_clinical_expert.pkl\"))\n    joblib.dump(supervisor, os.path.join(MODEL_DIR, f\"{fold_tag}_supervisor.pkl\"))\n    joblib.dump(cal_supervisor, os.path.join(MODEL_DIR, f\"{fold_tag}_calibrated_supervisor.pkl\"))\n    joblib.dump(meta_prep, os.path.join(MODEL_DIR, f\"{fold_tag}_meta_preprocessor.pkl\"))\n\n# ── 7. FINAL FULL-DATA MODEL DEPLOYMENT & EVALUATION ─────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"5. SAVING FINAL DEPLOYMENT MODEL & ARTIFACTS\")\nprint(\"=\"*60)\n\nparticipant_df[\"pred_stack\"] = oof_stack\nm_part = full_eval(participant_df[\"label\"], participant_df[\"pred_stack\"])\n\n# Save evaluation metrics as JSON / text\nwith open(os.path.join(EVAL_DIR, \"v10_metrics.json\"), \"w\") as f: json.dump(m_part, f, indent=2)\n\nsummary_rows = [{\"Model\": \"V10 (Multi-Window + Participant-Level)\", \"ROC-AUC\": f\"{m_part.get('roc_auc', 0):.4f}\", \n                 \"pAUC@85%\": f\"{m_part.get('pauc_85', 0):.4f}\", \"pAUC@90%\": f\"{m_part.get('pauc_90', 0):.4f}\",\n                 \"Spec@Sens=85%\": f\"{m_part.get('tuned_thresholds',{}).get('sens_85',{}).get('specificity',0):.4f}\",\n                 \"Spec@Sens=90%\": f\"{m_part.get('tuned_thresholds',{}).get('sens_90',{}).get('specificity',0):.4f}\",\n                 \"Spec@Sens=95%\": f\"{m_part.get('tuned_thresholds',{}).get('sens_95',{}).get('specificity',0):.4f}\"}]\nsummary_df = pd.DataFrame(summary_rows)\n\nwith open(os.path.join(EVAL_DIR, \"v10_metrics_readable.txt\"), \"w\") as f:\n    f.write(\"V10 FINAL EVALUATION\\n\" + \"=\"*50 + \"\\n\" + summary_df.to_string(index=False) + \"\\n\\n\")\n    for sk, sv in m_part.get(\"tuned_thresholds\", {}).items():\n        f.write(f\"[{sk}] threshold={sv['threshold']:.4f}  sens={sv['sensitivity']:.3f}  spec={sv['specificity']:.3f}  f1={sv['f1']:.3f}\\n\")\n\n# Save Visual Plots\nplot_curves(participant_df[\"label\"], participant_df[\"pred_stack\"], f\"{FUSION_OUT}/plots/v10_participant\", \"V10 Participant-Level\")\nbest_t_90 = m_part[\"tuned_thresholds\"][\"sens_90\"][\"threshold\"]\nplot_confusion_matrix_sns(participant_df[\"label\"], participant_df[\"pred_stack\"], \n                          f\"{FUSION_OUT}/plots/v10_participant_cm.png\", threshold=best_t_90, title=f\"V10 CM (Sens ≥ 90%)\")\n\n# Train ONE final deployable model on 100% of data\nprint(\"[*] Training final full-data models for deployment...\")\ndf_all, y_all, X_all_emb = participant_df.copy().reset_index(drop=True), participant_df[\"label\"].values, participant_embs.copy()\n\ncal_fold_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\ntr_sub_idx_final, cal_idx_final = next(cal_fold_final.split(df_all, y_all))\ndf_tr_final, df_cal_final = df_all.iloc[tr_sub_idx_final].reset_index(drop=True), df_all.iloc[cal_idx_final].reset_index(drop=True)\nX_tr_emb_final, X_cal_emb_final = X_all_emb[tr_sub_idx_final], X_all_emb[cal_idx_final]\ny_tr_final, y_cal_final = df_tr_final[\"label\"].values, df_cal_final[\"label\"].values\n\nmeta_prep_final = build_meta_preprocessor(num_cols_p, cat_cols)\nX_tr_m_final, X_cal_m_final, X_all_m_final = meta_prep_final.fit_transform(df_tr_final), meta_prep_final.transform(df_cal_final), meta_prep_final.transform(df_all)\n\nX_tr_emb_final_noisy, X_cal_emb_final_clean, X_all_emb_clean = add_embedding_noise(X_tr_emb_final), X_cal_emb_final, X_all_emb\ninner_cv_final = StratifiedKFold(n_splits=4, shuffle=True, random_state=SEED)\ntr_oof_a_final, tr_oof_m_final = np.zeros(len(y_tr_final)), np.zeros(len(y_tr_final))\n\nfor i_tr, i_val in inner_cv_final.split(X_tr_emb_final_noisy, y_tr_final):\n    n_p_i, n_n_i = int(y_tr_final[i_tr].sum()), int((y_tr_final[i_tr] == 0).sum())\n    clf_a_i = build_audio_expert(n_p_i, n_n_i).fit(add_embedding_noise(X_tr_emb_final_noisy[i_tr]), y_tr_final[i_tr])\n    tr_oof_a_final[i_val] = clf_a_i.predict_proba(X_tr_emb_final_noisy[i_val])[:, 1]\n    clf_m_i = build_clinical_expert(n_p_i, n_n_i).fit(X_tr_m_final[i_tr], y_tr_final[i_tr])\n    tr_oof_m_final[i_val] = clf_m_i.predict_proba(X_tr_m_final[i_val])[:, 1]\n\nn_pos_all, n_neg_all = int(y_tr_final.sum()), int((y_tr_final == 0).sum())\nclf_a_final = build_audio_expert(n_pos_all, n_neg_all).fit(X_tr_emb_final_noisy, y_tr_final)\nclf_m_final = build_clinical_expert(n_pos_all, n_neg_all).fit(X_tr_m_final, y_tr_final)\n\nX_tr_stack_final  = np.column_stack([tr_oof_a_final, tr_oof_m_final, X_tr_m_final])\nX_cal_stack_final = np.column_stack([clf_a_final.predict_proba(X_cal_emb_final_clean)[:, 1], clf_m_final.predict_proba(X_cal_m_final)[:, 1], X_cal_m_final])\n\nsupervisor_final = build_supervisor(n_pos_all, n_neg_all).fit(X_tr_stack_final, y_tr_final)\ncal_supervisor_final = CalibratedClassifierCV(supervisor_final, cv=\"prefit\", method=\"sigmoid\").fit(X_cal_stack_final, y_cal_final)\n\n# EXPORTING MASTER DEPLOYMENT WEIGHTS\njoblib.dump(meta_prep_final,        os.path.join(MODEL_DIR, \"final_meta_preprocessor.pkl\"))\njoblib.dump(clf_a_final,            os.path.join(MODEL_DIR, \"final_audio_expert.pkl\"))\njoblib.dump(clf_m_final,            os.path.join(MODEL_DIR, \"final_clinical_expert.pkl\"))\njoblib.dump(supervisor_final,       os.path.join(MODEL_DIR, \"final_supervisor.pkl\"))\njoblib.dump(cal_supervisor_final,   os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\"))\n\ndeploy_info = {\n    \"model_name\": \"TB SCREENING RANKER V10\",\n    \"seed\": SEED, \"sr\": SR, \"win_samples\": WIN_SAMPLES, \"hop_samples\": HOP_SAMPLES,\n    \"embed_dim\": EMBED_DIM, \"agg_funcs\": AGG_FUNCS, \"agg_dim\": AGG_DIM,\n    \"num_cols_p\": num_cols_p, \"cat_cols\": cat_cols,\n    \"target_sens_thresholds\": {k: float(v[\"threshold\"]) for k, v in m_part.get(\"tuned_thresholds\", {}).items()}\n}\nwith open(os.path.join(MODEL_DIR, \"final_inference_config.json\"), \"w\") as f: json.dump(deploy_info, f, indent=2)\n\nsummary_df.to_csv(f\"{FUSION_OUT}/v10_summary.csv\", index=False)\nparticipant_df.to_csv(f\"{FUSION_OUT}/v10_oof_predictions.csv\", index=False)\n\n# ZIP EVERYTHING FOR DOWNLOAD\nzip_path = \"/kaggle/working/outputs_v10.zip\"\nwith zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for root, _, files in os.walk(OUT_ROOT):\n        for fn in files: zf.write(os.path.join(root, fn), os.path.relpath(os.path.join(root, fn), \"/kaggle/working\"))\n\nprint(\"\\n✅ V10 PIPELINE COMPLETE & ZIPPED!\")\nprint(\"\\n--- DOWNLOAD LINKS ---\")\ndisplay(FileLink(zip_path))\ndisplay(FileLink(f\"{FUSION_OUT}/plots/v10_participant_roc.png\"))\ndisplay(FileLink(f\"{FUSION_OUT}/plots/v10_participant_cm.png\"))\ndisplay(FileLink(os.path.join(EVAL_DIR, \"v10_metrics_readable.txt\")))\ndisplay(FileLink(os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\")))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:25:03.325731Z","iopub.execute_input":"2026-02-22T15:25:03.326336Z","iopub.status.idle":"2026-02-22T15:34:27.532827Z","shell.execute_reply.started":"2026-02-22T15:25:03.326307Z","shell.execute_reply":"2026-02-22T15:34:27.532103Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\n1. LOADING & HARMONISING DATA\n============================================================\n\n--------------------------------------------------\n🔍 FEATURE AUDIT: METADATA & CLINICAL VARIABLES\n--------------------------------------------------\n[*] Total Available Features in Metadata (18):\n['participant_id', 'sex', 'age', 'height', 'weight', 'reported_cough_dur', 'tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul', 'tb_prior_Unknown', 'hemoptysis', 'heart_rate', 'temperature', 'weight_loss', 'smoke_lweek', 'fever', 'night_sweats', 'label_raw']\n\n[*] Excluded Features (Data Leakage/IDs) (2):\n['participant_id', 'label_raw']\n\n============================================================\n2. LOADING GOOGLE HeAR MODEL (GATED REPO FIX)\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"2026-02-22 15:25:33.671475: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771773933.847503      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771773933.897366      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771773934.336809      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771773934.336861      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771773934.336864      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771773934.336866      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 24 files:   0%|          | 0/24 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5440a9c84fe94be5843d684e8d328ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".DS_Store:   0%|          | 0.00/6.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d2da82fc784bd5aa3662c59b82562b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/fing(…):   0%|          | 0.00/79.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"818e5f1946844aae8ff287bb082c8ba5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab284b42cf964a27b01b625e5761e338"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/save(…):   0%|          | 0.00/4.89M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13d4a25e95e543888136b8af8c3e92bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17687130a7da477296600e06fd9852eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/vari(…):   0%|          | 0.00/12.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1fe32a448044fa39608c564e206b95d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1504b90e36646bc9cffcb3f3d2443fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_large/kera(…):   0%|          | 0.00/760k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d5d1e6478944279908d37b2d51d5970"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/5.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cace9313a614e0faeb527da09a311de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/kera(…):   0%|          | 0.00/644k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d931cd825afe4990a34f0bbc885796e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/save(…):   0%|          | 0.00/4.01M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5277a3556ab94ea782b258c25b22e0da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/fing(…):   0%|          | 0.00/76.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d91b782aee240babf8ea82f072cb6b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/event_detector_small/vari(…):   0%|          | 0.00/3.95M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caee21a193be49f0ad8b844c31629b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/4.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9a4cccde0e3487e99d5b87b8b76ee81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/spectrogram_frontend/kera(…):   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"433cdf451db045f89f145217636ced39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/spectrogram_frontend/fing(…):   0%|          | 0.00/55.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9a56c41456a49d68c346ca443042d70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"event_detector/spectrogram_frontend/save(…):   0%|          | 0.00/340k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58b038b6764a41ca80fbe4c17a4f7840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"fingerprint.pb:   0%|          | 0.00/78.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5700127127144c10a5b908bb3833d8d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"saved_model.pb:   0%|          | 0.00/3.98M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9300c29e0d2e427f8c0dec6ef56230a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/286 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb97e6bdcc9b4f6889dc8cd72a3f8ef9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.data-00000-of-00001:   0%|          | 0.00/24.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d57049315144a6a7e6091486408402"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gitattributes:   0%|          | 0.00/1.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b730ceafca0345b281b6b2d6aa2bbf0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables/variables.data-00000-of-00001:   0%|          | 0.00/1.21G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84a0f8c1051f46d485cb511ce8b04b9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"variables.index:   0%|          | 0.00/6.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05baa1ac35294d4a9628507ddcc1fc2b"}},"metadata":{}},{"name":"stdout","text":"WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1771773952.298320      55 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15511 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nWARNING:absl:Importing a function (__inference_internal_grad_fn_21425) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\nWARNING:absl:Importing a function (__inference_internal_grad_fn_17891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.\n","output_type":"stream"},{"name":"stdout","text":"\n[*] Pre-fetching multi-window HeAR embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting embeddings:   0%|          | 0/9772 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1771773974.183892     164 service.cc:152] XLA service 0x7d04c98ab8f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1771773974.183933     164 service.cc:160]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1771773974.517430     164 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1771773976.758527     164 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\n[*] Aggregating to participant level...\n\n[*] Final USED Numerical Features (8):\n['age', 'height', 'weight', 'reported_cough_dur', 'heart_rate', 'temperature', 'n_recordings', 'n_cough_windows_total']\n\n[*] Final USED Categorical Features (10):\n['sex', 'tb_prior', 'tb_prior_Pul', 'tb_prior_Extrapul', 'tb_prior_Unknown', 'hemoptysis', 'weight_loss', 'smoke_lweek', 'fever', 'night_sweats']\n--------------------------------------------------\n\n============================================================\n4. STARTING V10 TRAINING & WEIGHT EXPORT\n============================================================\n\n--- FOLD 1/5 ---\n[*] Fold 1 | AUC=0.8364 | pAUC@90%=0.9681\n\n--- FOLD 2/5 ---\n[*] Fold 2 | AUC=0.8297 | pAUC@90%=0.9682\n\n--- FOLD 3/5 ---\n[*] Fold 3 | AUC=0.7926 | pAUC@90%=0.9815\n\n--- FOLD 4/5 ---\n[*] Fold 4 | AUC=0.7877 | pAUC@90%=0.9619\n\n--- FOLD 5/5 ---\n[*] Fold 5 | AUC=0.7427 | pAUC@90%=0.9480\n\n============================================================\n5. SAVING FINAL DEPLOYMENT MODEL & ARTIFACTS\n============================================================\n[*] Training final full-data models for deployment...\n\n✅ V10 PIPELINE COMPLETE & ZIPPED!\n\n--- DOWNLOAD LINKS ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/outputs_v10.zip","text/html":"<a href='/kaggle/working/outputs_v10.zip' target='_blank'>/kaggle/working/outputs_v10.zip</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/outputs_v10/multiwindow_participant/plots/v10_participant_roc.png","text/html":"<a href='/kaggle/working/outputs_v10/multiwindow_participant/plots/v10_participant_roc.png' target='_blank'>/kaggle/working/outputs_v10/multiwindow_participant/plots/v10_participant_roc.png</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/outputs_v10/multiwindow_participant/plots/v10_participant_cm.png","text/html":"<a href='/kaggle/working/outputs_v10/multiwindow_participant/plots/v10_participant_cm.png' target='_blank'>/kaggle/working/outputs_v10/multiwindow_participant/plots/v10_participant_cm.png</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/outputs_v10/eval/v10_metrics_readable.txt","text/html":"<a href='/kaggle/working/outputs_v10/eval/v10_metrics_readable.txt' target='_blank'>/kaggle/working/outputs_v10/eval/v10_metrics_readable.txt</a><br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/outputs_v10/models/final_calibrated_supervisor.pkl","text/html":"<a href='/kaggle/working/outputs_v10/models/final_calibrated_supervisor.pkl' target='_blank'>/kaggle/working/outputs_v10/models/final_calibrated_supervisor.pkl</a><br>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ── EXPORTING HeAR MODEL FOR LOCAL DEPLOYMENT ──────────────────────────────\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXPORTING GOOGLE HeAR MODEL (RAW FILES)\")\nprint(\"=\"*60)\n\nimport os, shutil\nfrom huggingface_hub import snapshot_download\nfrom IPython.display import FileLink, display\n\nhear_save_dir = \"/kaggle/working/hear_saved_model\"\nos.makedirs(hear_save_dir, exist_ok=True)\n\n# 1. Download the raw TF SavedModel directory directly from Hugging Face\nprint(\"[*] Downloading raw model files directly to disk...\")\nsnapshot_download(\n    repo_id=\"google/hear\", \n    local_dir=hear_save_dir,\n    # We only need the actual model files, ignoring git or markdown files\n    ignore_patterns=[\"*.md\", \".gitattributes\"] \n)\n\n# 2. Zip the folder so it can be downloaded easily\nprint(\"[*] Zipping the model files for download...\")\nhear_zip_path = \"/kaggle/working/hear_model_offline.zip\"\nshutil.make_archive(hear_zip_path.replace('.zip', ''), 'zip', hear_save_dir)\n\n# 3. Generate the clickable download link\nprint(\"\\n✅ HeAR Model Downloaded and Zipped for Offline Use!\")\ndisplay(FileLink(\"hear_model_offline.zip\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T15:46:48.605929Z","iopub.execute_input":"2026-02-22T15:46:48.606679Z","iopub.status.idle":"2026-02-22T15:47:50.143232Z","shell.execute_reply.started":"2026-02-22T15:46:48.606650Z","shell.execute_reply":"2026-02-22T15:47:50.142604Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nEXPORTING GOOGLE HeAR MODEL (RAW FILES)\n============================================================\n[*] Downloading raw model files directly to disk...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29452a2294af4d81ba326abe982bc267"}},"metadata":{}},{"name":"stdout","text":"[*] Zipping the model files for download...\n\n✅ HeAR Model Downloaded and Zipped for Offline Use!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"/kaggle/working/hear_model_offline.zip","text/html":"<a href='hear_model_offline.zip' target='_blank'>hear_model_offline.zip</a><br>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Run this cell, then immediately go to Run -> Restart Session in the top menu\n!pip install --upgrade -q keras keras-hub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Must be set to JAX BEFORE importing keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Cap JAX memory so it doesn't crash Pandas/LightGBM (Adjusted to 0.75 for safety)\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.75\" \n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport keras\nimport keras_hub\n\n# 1. CRITICAL: Force Keras to use half-precision. \n# This shrinks MedGemma 4B from 16GB to 8GB so it fits on a Kaggle T4!\nkeras.config.set_floatx(\"float16\")\n\n# ============================================================================\n# 2. SETUP: LOAD THE V10 ML MODELS\n# ============================================================================\nprint(\"Loading V10 ML Models...\")\nMODEL_DIR = \"/kaggle/input/datasets/jroot888/medgemma-trained-weights/outputs_v10/models\"\n\nmeta_prep      = joblib.load(os.path.join(MODEL_DIR, \"final_meta_preprocessor.pkl\"))\nclf_a          = joblib.load(os.path.join(MODEL_DIR, \"final_audio_expert.pkl\"))\nclf_m          = joblib.load(os.path.join(MODEL_DIR, \"final_clinical_expert.pkl\"))\ncal_supervisor = joblib.load(os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\"))\n\n# ============================================================================\n# 3. SIMULATE A WAITING ROOM (Standalone Version)\n# ============================================================================\nprint(\"Simulating Clinic Waiting Room...\")\n# Load actual clinical data from your V10 output CSV\ncsv_path = \"/kaggle/input/datasets/jroot888/medgemma-trained-weights/outputs_v10/multiwindow_participant/v10_oof_predictions.csv\"\ndf_all = pd.read_csv(csv_path)\ndf_waiting_room = df_all.sample(10, random_state=99).reset_index(drop=True)\n\n# Generate dummy HeAR embeddings (2560-dim) just for this UI/Agent test\nX_audio_waiting_room = np.random.randn(10, 2560).astype(np.float32)\n\n# ============================================================================\n# 4. THE RANKING ENGINE\n# ============================================================================\ndef rank_patients(df_meta, X_audio):\n    X_m_processed = meta_prep.transform(df_meta)\n    \n    prob_a = clf_a.predict_proba(X_audio)[:, 1]\n    prob_m = clf_m.predict_proba(X_m_processed)[:, 1]\n    \n    X_stack = np.column_stack([prob_a, prob_m, X_m_processed])\n    final_scores = cal_supervisor.predict_proba(X_stack)[:, 1]\n    \n    desired_cols = ['participant_id', 'age', 'sex', 'weight_loss', 'night_sweats']\n    display_cols = [c for c in desired_cols if c in df_meta.columns]\n    \n    df_queue = df_meta[display_cols].copy()\n    df_queue['audio_risk']  = np.round(prob_a, 3)\n    df_queue['clinic_risk'] = np.round(prob_m, 3)\n    df_queue['final_triage_score'] = np.round(final_scores, 3)\n    \n    df_queue = df_queue.sort_values(by='final_triage_score', ascending=False).reset_index(drop=True)\n    df_queue.index = df_queue.index + 1 \n    return df_queue\n\nranked_queue = rank_patients(df_waiting_room, X_audio_waiting_room)\nprint(\"\\n--- SORTED TRIAGE QUEUE ---\")\nprint(ranked_queue[['participant_id', 'audio_risk', 'clinic_risk', 'final_triage_score']])\n\n# ============================================================================\n# 5. LOAD MEDGEMMA 4B (Memory-Optimized)\n# ============================================================================\nprint(\"\\nLoading MedGemma 4B into GPU... (This takes a minute)\")\nMEDGEMMA_PATH = \"/kaggle/input/models/keras/medgemma/keras/medgemma_4b/1\"\n\ntry:\n    # dtype=\"float16\" combined with set_floatx prevents the OOM crash\n    medgemma = keras_hub.models.CausalLM.from_preset(MEDGEMMA_PATH, dtype=\"float16\")\n    medgemma.compile(sampler=\"greedy\") \n    print(\"✓ MedGemma 4B Loaded Successfully!\")\nexcept Exception as e:\n    print(f\"⚠ Warning: MedGemma failed to load.\\nError: {e}\")\n    medgemma = None \n\n# ============================================================================\n# 6. EXECUTE THE AGENTIC WORKFLOW\n# ============================================================================\ndef generate_clinical_justification(patient_row):\n    age = patient_row.get('age', 'Unknown')\n    sex = patient_row.get('sex', 'Unknown')\n    weight_loss = patient_row.get('weight_loss', 'Unknown')\n    night_sweats = patient_row.get('night_sweats', 'Unknown')\n    \n    prompt = (\n        f\"You are an expert AI Triage Assistant in a tuberculosis clinic. \"\n        f\"You have limited GeneXpert tests available today.\\n\\n\"\n        f\"PATIENT DATA:\\n\"\n        f\"- Demographics: {age} year old {sex}.\\n\"\n        f\"- Symptoms: Weight loss is {weight_loss}, Night Sweats is {night_sweats}.\\n\"\n        f\"- AI Assessment: Acoustic Cough Risk Score is {patient_row['audio_risk']} out of 1.0. \"\n        f\"Clinical Risk Score is {patient_row['clinic_risk']} out of 1.0.\\n\\n\"\n        f\"TASK: Write a concise, professional 2-sentence clinical justification explaining why this patient \"\n        f\"has been assigned a high priority triage rank for a GeneXpert test today.\\n\\n\"\n        f\"JUSTIFICATION:\\n\"\n    )\n    \n    if medgemma is None: return \"LLM Not Loaded.\"\n    \n    response = medgemma.generate(prompt, max_length=256)\n    # Clean up the output so it only prints the new text\n    justification = response.replace(prompt, \"\").strip()\n    return justification\n\nprint(\"\\n--- MEDGEMMA CLINICAL EXPLANATIONS FOR TOP 3 PATIENTS ---\")\nfor rank, patient in ranked_queue.head(3).iterrows():\n    print(f\"\\nEvaluating Rank #{rank} (ID: {patient.get('participant_id', 'Unknown')})\")\n    print(\"Generating LLM Justification...\")\n    explanation = generate_clinical_justification(patient)\n    print(f\"MEDGEMMA OUTPUT:\\n{explanation}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Forcefully rip out the old pre-installed versions\n!pip uninstall -y -q keras-hub keras-nlp\n\n# 2. Install the absolute latest versions cleanly\n!pip install -q -U keras-hub\n!pip install -q -U keras","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import keras_hub\nprint(keras_hub.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# 1. ENVIRONMENT SETUP\n# Must be set BEFORE importing keras\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n# Cap JAX memory so it doesn't crash LightGBM on the P100\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.75\" \n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport keras\nimport keras_hub\n\n# Force Keras to use half-precision to fit MedGemma 4B into VRAM\nkeras.config.set_floatx(\"float16\")\n\n# ============================================================================\n# 2. LOAD THE V10 ML MODELS\n# ============================================================================\nprint(\"Loading V10 ML Models...\")\nMODEL_DIR = \"/kaggle/input/datasets/jroot888/medgemma-trained-weights/outputs_v10/models\"\n\nmeta_prep      = joblib.load(os.path.join(MODEL_DIR, \"final_meta_preprocessor.pkl\"))\nclf_a          = joblib.load(os.path.join(MODEL_DIR, \"final_audio_expert.pkl\"))\nclf_m          = joblib.load(os.path.join(MODEL_DIR, \"final_clinical_expert.pkl\"))\ncal_supervisor = joblib.load(os.path.join(MODEL_DIR, \"final_calibrated_supervisor.pkl\"))\n\n# ============================================================================\n# 3. SIMULATE A WAITING ROOM\n# ============================================================================\nprint(\"Simulating Clinic Waiting Room...\")\ncsv_path = \"/kaggle/input/datasets/jroot888/medgemma-trained-weights/outputs_v10/multiwindow_participant/v10_oof_predictions.csv\"\ndf_all = pd.read_csv(csv_path)\ndf_waiting_room = df_all.sample(10, random_state=99).reset_index(drop=True)\n\n# Dummy HeAR embeddings (2560-dim)\nX_audio_waiting_room = np.random.randn(10, 2560).astype(np.float32)\n\n# ============================================================================\n# 4. THE RANKING ENGINE\n# ============================================================================\ndef rank_patients(df_meta, X_audio):\n    X_m_processed = meta_prep.transform(df_meta)\n    \n    prob_a = clf_a.predict_proba(X_audio)[:, 1]\n    prob_m = clf_m.predict_proba(X_m_processed)[:, 1]\n    \n    X_stack = np.column_stack([prob_a, prob_m, X_m_processed])\n    final_scores = cal_supervisor.predict_proba(X_stack)[:, 1]\n    \n    desired_cols = ['participant_id', 'age', 'sex', 'weight_loss', 'night_sweats']\n    display_cols = [c for c in desired_cols if c in df_meta.columns]\n    \n    df_queue = df_meta[display_cols].copy()\n    df_queue['audio_risk']  = np.round(prob_a, 3)\n    df_queue['clinic_risk'] = np.round(prob_m, 3)\n    df_queue['final_triage_score'] = np.round(final_scores, 3)\n    \n    df_queue = df_queue.sort_values(by='final_triage_score', ascending=False).reset_index(drop=True)\n    df_queue.index = df_queue.index + 1 \n    return df_queue\n\nranked_queue = rank_patients(df_waiting_room, X_audio_waiting_room)\nprint(\"\\n--- SORTED TRIAGE QUEUE ---\")\nprint(ranked_queue[['participant_id', 'audio_risk', 'clinic_risk', 'final_triage_score']])\n\n# ============================================================================\n# 5. LOAD MEDGEMMA 4B (Optimized Sampler)\n# ============================================================================\nprint(\"\\nLoading MedGemma 4B into GPU... (This takes a minute)\")\nMEDGEMMA_PATH = \"/kaggle/input/models/keras/medgemma/keras/medgemma_4b/1\"\n\ntry:\n    medgemma = keras_hub.models.CausalLM.from_preset(MEDGEMMA_PATH, dtype=\"float16\")\n    \n    # BEST ACCURACY SETUP: Top-P sampler stops the repeating loops\n    sampler = keras_hub.samplers.TopPSampler(p=0.9, temperature=0.2)\n    medgemma.compile(sampler=sampler) \n    print(\"✓ MedGemma 4B Loaded Successfully!\")\nexcept Exception as e:\n    print(f\"⚠ Warning: MedGemma failed to load.\\nError: {e}\")\n    medgemma = None \n\n# ============================================================================\n# 6. EXECUTE BILINGUAL AGENTIC WORKFLOW (P100 Safe - Sequential)\n# ============================================================================\ndef build_bilingual_prompt(patient_row):\n    prompt = f\"\"\"You are an expert AI Triage Assistant in a tuberculosis clinic. Write a strict, 2-sentence clinical justification for GeneXpert testing in English, followed immediately by its Hindi translation.\n\nEXAMPLE INPUT:\n- Demographics: 45 year old Male.\n- Symptoms: Weight loss is Yes, Night Sweats is No.\n- AI Assessment: Acoustic Cough Risk Score is 0.850. Clinical Risk Score is 0.720.\n\nEXAMPLE OUTPUT:\nEnglish: This 45-year-old male presents with weight loss and highly elevated risk scores indicating probable tuberculosis. Immediate GeneXpert testing is prioritized to confirm active pulmonary infection.\nHindi: यह 45 वर्षीय पुरुष वजन कम होने और अत्यधिक जोखिम स्कोर के साथ प्रस्तुत होता है जो संभावित तपेदिक का संकेत देता है। सक्रिय फुफ्फुसीय संक्रमण की पुष्टि करने के लिए तत्काल जीनएक्सपर्ट परीक्षण को प्राथमिकता दी जाती है।\n\nREAL INPUT:\n- Demographics: {patient_row.get('age', 'Unknown')} year old {patient_row.get('sex', 'Unknown')}.\n- Symptoms: Weight loss is {patient_row.get('weight_loss', 'Unknown')}, Night Sweats is {patient_row.get('night_sweats', 'Unknown')}.\n- AI Assessment: Acoustic Cough Risk Score is {patient_row['audio_risk']}. Clinical Risk Score is {patient_row['clinic_risk']}.\n\nREAL OUTPUT:\n\"\"\"\n    return prompt\n\nif medgemma is not None:\n    print(\"\\n--- GENERATING BILINGUAL EXPLANATIONS (SEQUENTIAL) ---\")\n    \n    top_patients = ranked_queue.head(3)\n    \n    for idx, patient in top_patients.iterrows():\n        print(f\"\\nEvaluating Rank #{idx} (ID: {patient.get('participant_id', 'Unknown')})\")\n        print(\"Generating LLM Justification...\")\n        \n        # 1. Build the prompt for just this ONE patient\n        prompt = build_bilingual_prompt(patient)\n        \n        # 2. Generate sequentially (Safe for P100 architecture)\n        response = medgemma.generate(prompt, max_length=512)\n        \n        # 3. Clean and print\n        clean_output = response.replace(prompt, \"\").strip()\n        print(f\"MEDGEMMA OUTPUT:\\n{clean_output}\")\nelse:\n    print(\"LLM Not Loaded. Cannot execute agent workflow.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-21T17:55:07.262549Z","iopub.execute_input":"2026-02-21T17:55:07.262799Z","iopub.status.idle":"2026-02-21T17:57:57.719344Z","shell.execute_reply.started":"2026-02-21T17:55:07.262777Z","shell.execute_reply":"2026-02-21T17:57:57.718067Z"}},"outputs":[{"name":"stderr","text":"2026-02-21 17:55:09.319068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771696509.340156     725 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771696509.346909     725 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771696509.363514     725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771696509.363535     725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771696509.363537     725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771696509.363539     725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Loading V10 ML Models...\nSimulating Clinic Waiting Room...\n\n--- SORTED TRIAGE QUEUE ---\n   participant_id  audio_risk  clinic_risk  final_triage_score\n1    CODA_TB_0575       0.091        0.847               0.590\n2    CODA_TB_0287       0.103        0.756               0.547\n3    CODA_TB_0492       0.096        0.622               0.381\n4    CODA_TB_1056       0.057        0.191               0.084\n5    CODA_TB_0590       0.063        0.172               0.069\n6    CODA_TB_0807       0.052        0.206               0.067\n7    CODA_TB_0156       0.053        0.079               0.062\n8    CODA_TB_0889       0.080        0.097               0.058\n9    CODA_TB_0133       0.116        0.232               0.057\n10   CODA_TB_1107       0.086        0.069               0.052\n\nLoading MedGemma 4B into GPU... (This takes a minute)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\nI0000 00:00:1771696521.281371     725 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3297 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nnormalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"},{"name":"stdout","text":"✓ MedGemma 4B Loaded Successfully!\n\n--- GENERATING BILINGUAL EXPLANATIONS (SEQUENTIAL) ---\n\nEvaluating Rank #1 (ID: CODA_TB_0575)\nGenerating LLM Justification...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_725/47995226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# 2. Generate sequentially (Safe for P100 architecture)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmedgemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# 3. Clean and print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_hub/src/models/gemma3/gemma3_causal_lm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, stop_token_ids, strip_prompt)\u001b[0m\n\u001b[1;32m    371\u001b[0m             ]\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         return super().generate(\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_hub/src/models/causal_lm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, stop_token_ids, strip_prompt)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstrip_prompt_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_hub/src/models/causal_lm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_token_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstrip_prompt_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras_hub/src/models/causal_lm.py\u001b[0m in \u001b[0;36mwrapped_generate_function\u001b[0;34m(inputs, stop_token_ids)\u001b[0m\n\u001b[1;32m    204\u001b[0m                 )\n\u001b[1;32m    205\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 outputs, sampler_variables = compiled_generate_function(\n\u001b[0m\u001b[1;32m    207\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                     \u001b[0mstop_token_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jax/_src/compiler.py\u001b[0m in \u001b[0;36mbackend_compile_and_load\u001b[0;34m(backend, module, executable_devices, options, host_callbacks)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0;31m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m       \u001b[0;31m# to take in `host_callbacks`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m       return backend.compile_and_load(\n\u001b[0m\u001b[1;32m    379\u001b[0m           \u001b[0mbuilt_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m           \u001b[0mexecutable_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecutable_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mXlaRuntimeError\u001b[0m: UNIMPLEMENTED: Unsupported algorithm on the current device(s): ALG_DOT_F16_F16_F32"],"ename":"XlaRuntimeError","evalue":"UNIMPLEMENTED: Unsupported algorithm on the current device(s): ALG_DOT_F16_F16_F32","output_type":"error"}],"execution_count":1}]}